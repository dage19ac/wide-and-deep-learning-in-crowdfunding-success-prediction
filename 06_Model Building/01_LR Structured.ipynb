{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_LR Structured.ipynb","provenance":[],"collapsed_sections":["HyW82680sOSI","lhBR5qnes-TN","QlcazcrXKT5e","6250os8sMSuZ","APCM6IayNmOx","ToMIjd2_QuUK","EmzaXt6qRVbs","qx0cm4AxVzO5","LHMJDfUWYdmN","b31IIEU2alm_","9lD8O0Fud4FF","WrJZ92GieE7O","5qVnuB1FeNh8","HbFa0bV7sEjg","a3YY_dnKv8Rj","dFLoD2ymDL0v"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HyW82680sOSI"},"source":["#### Setup"]},{"cell_type":"code","metadata":{"id":"tEe4_nY39Z8C"},"source":["!pip install -U tensorflow keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2t35Z4Ax9T7v"},"source":["!pip install -U talos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJgbrWKm43PC"},"source":["# Import general Python libraries\n","import pandas as pd\n","import numpy as np\n","import random\n","import sklearn\n","import seaborn as sns\n","import os\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeA52DCkXF5J"},"source":["# Specify seeds for random-operations\n","seed_value = 0\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","np.random.seed(seed_value)\n","random.seed(seed_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCQ21bbY43PO"},"source":["# Import sklearn-specific modules\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import PCA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6CQ9UoKFnuP","executionInfo":{"status":"ok","timestamp":1615719720927,"user_tz":-60,"elapsed":2219,"user":{"displayName":"David George","photoUrl":"","userId":"01025812402211939868"}},"outputId":"6d1c8136-2308-45ab-9c7d-15ad22a9ba1a"},"source":["# Import tensorflow-specific modules\n","import tensorflow as tf\n","tf.random.set_seed(seed_value)\n","print(\"Tensorflow Version: {}\".format(tf.__version__))\n","print(\"Keras Version: {}\".format(tf.keras.__version__))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 2.4.1\n","Keras Version: 2.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_XVE26gMK-r1"},"source":["# Import keras-specific modules\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adadelta, RMSprop, Adam, Adamax, Nadam\n","from tensorflow.keras.regularizers import L1, L2\n","from tensorflow.keras.initializers import GlorotNormal, GlorotUniform, HeNormal, HeUniform, LecunNormal, LecunUniform\n","from tensorflow.keras.metrics import AUC, Precision, Recall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atLk7L7PgjwI"},"source":["# Import talos-specific modules\n","import talos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3NkYJuv43PP"},"source":["# Set pandas options\n","pd.set_option(\"display.max_columns\", None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WI9_Amw06BBV","executionInfo":{"status":"ok","timestamp":1615719743352,"user_tz":-60,"elapsed":18322,"user":{"displayName":"David George","photoUrl":"","userId":"01025812402211939868"}},"outputId":"2e45e1d9-31e1-4119-e9fd-493a350d44cc"},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lhBR5qnes-TN"},"source":["#### a) Prepare Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":151},"id":"v_sYpgrFsMqx","executionInfo":{"status":"ok","timestamp":1615719967851,"user_tz":-60,"elapsed":2446,"user":{"displayName":"David George","photoUrl":"","userId":"01025812402211939868"}},"outputId":"d6e764d3-c483-4467-f944-6684689f7f25"},"source":["# Import Dataset\n","kickstarter_df = pd.read_csv(\"04_Final Datasets/Kickstarter_Structured.csv\", index_col=0)\n","print(kickstarter_df.shape)\n","print(len(kickstarter_df.index.unique()))\n","kickstarter_df.head(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(246891, 35)\n","246891\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>campaign_successful</th>\n","      <th>goal</th>\n","      <th>number_of_collaborators</th>\n","      <th>funding_period</th>\n","      <th>days_between_created_and_launched</th>\n","      <th>launch_quartal</th>\n","      <th>staff_pick</th>\n","      <th>campaign_has_demo_video</th>\n","      <th>campaign_has_environmental_commitments</th>\n","      <th>number_of_images</th>\n","      <th>number_of_videos</th>\n","      <th>number_of_audios</th>\n","      <th>number_of_interactives</th>\n","      <th>number_of_words</th>\n","      <th>number_of_links</th>\n","      <th>creator_verified_identity</th>\n","      <th>creator_fb_auth</th>\n","      <th>creator_has_image</th>\n","      <th>creator_allows_follows</th>\n","      <th>number_of_creator_backings</th>\n","      <th>number_of_creator_projects</th>\n","      <th>facebook_linked</th>\n","      <th>twitter_linked</th>\n","      <th>instagram_linked</th>\n","      <th>linkedin_linked</th>\n","      <th>number_of_rewards</th>\n","      <th>number_of_words_per_reward</th>\n","      <th>lowest_pledge_level</th>\n","      <th>highest_pledge_level</th>\n","      <th>has_limited_rewards</th>\n","      <th>has_shipped_rewards</th>\n","      <th>has_restricted_shipping_rewards</th>\n","      <th>avg_months_until_reward</th>\n","      <th>location</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>22821161</th>\n","      <td>0</td>\n","      <td>50000.0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>102</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1378</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>36.75</td>\n","      <td>5.0</td>\n","      <td>1417.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5.5</td>\n","      <td>United States</td>\n","      <td>Design_Product Design</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          campaign_successful     goal  number_of_collaborators  \\\n","22821161                    0  50000.0                        0   \n","\n","          funding_period  days_between_created_and_launched  launch_quartal  \\\n","22821161              45                                102               3   \n","\n","          staff_pick  campaign_has_demo_video  \\\n","22821161           0                        1   \n","\n","          campaign_has_environmental_commitments  number_of_images  \\\n","22821161                                       0                13   \n","\n","          number_of_videos  number_of_audios  number_of_interactives  \\\n","22821161                 0                 0                       0   \n","\n","          number_of_words  number_of_links  creator_verified_identity  \\\n","22821161             1378                2                          1   \n","\n","          creator_fb_auth  creator_has_image  creator_allows_follows  \\\n","22821161                0                  1                       1   \n","\n","          number_of_creator_backings  number_of_creator_projects  \\\n","22821161                           0                           1   \n","\n","          facebook_linked  twitter_linked  instagram_linked  linkedin_linked  \\\n","22821161                0               0                 0                0   \n","\n","          number_of_rewards  number_of_words_per_reward  lowest_pledge_level  \\\n","22821161                  8                       36.75                  5.0   \n","\n","          highest_pledge_level  has_limited_rewards  has_shipped_rewards  \\\n","22821161                1417.0                    0                    1   \n","\n","          has_restricted_shipping_rewards  avg_months_until_reward  \\\n","22821161                                1                      5.5   \n","\n","               location               category  \n","22821161  United States  Design_Product Design  "]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309},"id":"5burqu6XEJ_C","executionInfo":{"status":"ok","timestamp":1615720691198,"user_tz":-60,"elapsed":1331,"user":{"displayName":"David George","photoUrl":"","userId":"01025812402211939868"}},"outputId":"d06ea230-8fd8-4f03-9f75-6cb588b5c809"},"source":["# Convert categorical features into dummy-variables\n","print(\"Before: {}\".format(kickstarter_df.shape))\n","kickstarter_df[\"launch_quartal\"] = kickstarter_df.launch_quartal.apply(str)\n","kickstarter_df = pd.get_dummies(kickstarter_df, prefix=[\"launch_quartal\", \"location\", \"category\"], columns=[\"launch_quartal\", \"location\", \"category\"], drop_first=False)\n","print(\"After: {}\".format(kickstarter_df.shape))\n","kickstarter_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before: (246891, 35)\n","After: (246891, 234)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>campaign_successful</th>\n","      <th>goal</th>\n","      <th>number_of_collaborators</th>\n","      <th>funding_period</th>\n","      <th>days_between_created_and_launched</th>\n","      <th>staff_pick</th>\n","      <th>campaign_has_demo_video</th>\n","      <th>campaign_has_environmental_commitments</th>\n","      <th>number_of_images</th>\n","      <th>number_of_videos</th>\n","      <th>number_of_audios</th>\n","      <th>number_of_interactives</th>\n","      <th>number_of_words</th>\n","      <th>number_of_links</th>\n","      <th>creator_verified_identity</th>\n","      <th>creator_fb_auth</th>\n","      <th>creator_has_image</th>\n","      <th>creator_allows_follows</th>\n","      <th>number_of_creator_backings</th>\n","      <th>number_of_creator_projects</th>\n","      <th>facebook_linked</th>\n","      <th>twitter_linked</th>\n","      <th>instagram_linked</th>\n","      <th>linkedin_linked</th>\n","      <th>number_of_rewards</th>\n","      <th>number_of_words_per_reward</th>\n","      <th>lowest_pledge_level</th>\n","      <th>highest_pledge_level</th>\n","      <th>has_limited_rewards</th>\n","      <th>has_shipped_rewards</th>\n","      <th>has_restricted_shipping_rewards</th>\n","      <th>avg_months_until_reward</th>\n","      <th>launch_quartal_1</th>\n","      <th>launch_quartal_2</th>\n","      <th>launch_quartal_3</th>\n","      <th>launch_quartal_4</th>\n","      <th>location_Africa</th>\n","      <th>location_Australia</th>\n","      <th>location_Belgium</th>\n","      <th>location_Canada</th>\n","      <th>location_China</th>\n","      <th>location_Denmark</th>\n","      <th>location_France</th>\n","      <th>location_Germany</th>\n","      <th>location_Hong Kong</th>\n","      <th>location_Ireland</th>\n","      <th>location_Italy</th>\n","      <th>location_Japan</th>\n","      <th>location_Latin and South America</th>\n","      <th>location_Mexico</th>\n","      <th>location_Netherlands</th>\n","      <th>location_New Zealand</th>\n","      <th>location_No Location</th>\n","      <th>location_Norway</th>\n","      <th>location_Oceania and Antarctica</th>\n","      <th>location_Rest of Asia</th>\n","      <th>location_Rest of Europe</th>\n","      <th>location_Singapore</th>\n","      <th>location_Spain</th>\n","      <th>location_Sweden</th>\n","      <th>location_Switzerland</th>\n","      <th>location_United Kingdom</th>\n","      <th>location_United States</th>\n","      <th>category_Art_Ceramics</th>\n","      <th>category_Art_Conceptual Art</th>\n","      <th>category_Art_Digital Art</th>\n","      <th>category_Art_Illustration</th>\n","      <th>category_Art_Installations</th>\n","      <th>category_Art_Mixed Media</th>\n","      <th>category_Art_No Subcategory</th>\n","      <th>category_Art_Painting</th>\n","      <th>category_Art_Performance Art</th>\n","      <th>category_Art_Public Art</th>\n","      <th>category_Art_Sculpture</th>\n","      <th>category_Art_Social Practice</th>\n","      <th>category_Art_Textiles</th>\n","      <th>category_Art_Video Art</th>\n","      <th>category_Comics_Anthologies</th>\n","      <th>category_Comics_Comic Books</th>\n","      <th>category_Comics_Events</th>\n","      <th>category_Comics_Graphic Novels</th>\n","      <th>category_Comics_No Subcategory</th>\n","      <th>category_Comics_Webcomics</th>\n","      <th>category_Crafts_Candles</th>\n","      <th>category_Crafts_Crochet</th>\n","      <th>category_Crafts_DIY</th>\n","      <th>category_Crafts_Embroidery</th>\n","      <th>category_Crafts_Glass</th>\n","      <th>category_Crafts_Knitting</th>\n","      <th>category_Crafts_No Subcategory</th>\n","      <th>category_Crafts_Pottery</th>\n","      <th>category_Crafts_Printing</th>\n","      <th>category_Crafts_Quilts</th>\n","      <th>category_Crafts_Stationery</th>\n","      <th>category_Crafts_Taxidermy</th>\n","      <th>category_Crafts_Weaving</th>\n","      <th>category_Crafts_Woodworking</th>\n","      <th>category_Dance_No Subcategory</th>\n","      <th>category_Dance_Performances</th>\n","      <th>category_Dance_Residencies</th>\n","      <th>category_Dance_Spaces</th>\n","      <th>category_Dance_Workshops</th>\n","      <th>category_Design_Architecture</th>\n","      <th>category_Design_Civic Design</th>\n","      <th>category_Design_Graphic Design</th>\n","      <th>category_Design_Interactive Design</th>\n","      <th>category_Design_No Subcategory</th>\n","      <th>category_Design_Product Design</th>\n","      <th>category_Design_Toys</th>\n","      <th>category_Design_Typography</th>\n","      <th>category_Fashion_Accessories</th>\n","      <th>category_Fashion_Apparel</th>\n","      <th>category_Fashion_Childrenswear</th>\n","      <th>category_Fashion_Couture</th>\n","      <th>category_Fashion_Footwear</th>\n","      <th>category_Fashion_Jewelry</th>\n","      <th>category_Fashion_No Subcategory</th>\n","      <th>category_Fashion_Pet Fashion</th>\n","      <th>category_Fashion_Ready-to-wear</th>\n","      <th>category_Film &amp; Video_Action</th>\n","      <th>category_Film &amp; Video_Animation</th>\n","      <th>category_Film &amp; Video_Comedy</th>\n","      <th>category_Film &amp; Video_Documentary</th>\n","      <th>category_Film &amp; Video_Drama</th>\n","      <th>category_Film &amp; Video_Experimental</th>\n","      <th>category_Film &amp; Video_Family</th>\n","      <th>category_Film &amp; Video_Fantasy</th>\n","      <th>category_Film &amp; Video_Festivals</th>\n","      <th>category_Film &amp; Video_Horror</th>\n","      <th>category_Film &amp; Video_Movie Theaters</th>\n","      <th>category_Film &amp; Video_Music Videos</th>\n","      <th>category_Film &amp; Video_Narrative Film</th>\n","      <th>category_Film &amp; Video_No Subcategory</th>\n","      <th>category_Film &amp; Video_Romance</th>\n","      <th>category_Film &amp; Video_Science Fiction</th>\n","      <th>category_Film &amp; Video_Shorts</th>\n","      <th>category_Film &amp; Video_Television</th>\n","      <th>category_Film &amp; Video_Thrillers</th>\n","      <th>category_Film &amp; Video_Webseries</th>\n","      <th>category_Food_Bacon</th>\n","      <th>category_Food_Community Gardens</th>\n","      <th>category_Food_Cookbooks</th>\n","      <th>category_Food_Drinks</th>\n","      <th>category_Food_Events</th>\n","      <th>category_Food_Farmer's Markets</th>\n","      <th>category_Food_Farms</th>\n","      <th>category_Food_Food Trucks</th>\n","      <th>category_Food_No Subcategory</th>\n","      <th>category_Food_Restaurants</th>\n","      <th>category_Food_Small Batch</th>\n","      <th>category_Food_Spaces</th>\n","      <th>category_Food_Vegan</th>\n","      <th>category_Games_Gaming Hardware</th>\n","      <th>category_Games_Live Games</th>\n","      <th>category_Games_Mobile Games</th>\n","      <th>category_Games_No Subcategory</th>\n","      <th>category_Games_Playing Cards</th>\n","      <th>category_Games_Puzzles</th>\n","      <th>category_Games_Tabletop Games</th>\n","      <th>category_Games_Video Games</th>\n","      <th>category_Journalism_Audio</th>\n","      <th>category_Journalism_No Subcategory</th>\n","      <th>category_Journalism_Photo</th>\n","      <th>category_Journalism_Print</th>\n","      <th>category_Journalism_Video</th>\n","      <th>category_Journalism_Web</th>\n","      <th>category_Music_Blues</th>\n","      <th>category_Music_Chiptune</th>\n","      <th>category_Music_Classical Music</th>\n","      <th>category_Music_Comedy</th>\n","      <th>category_Music_Country &amp; Folk</th>\n","      <th>category_Music_Electronic Music</th>\n","      <th>category_Music_Faith</th>\n","      <th>category_Music_Hip-Hop</th>\n","      <th>category_Music_Indie Rock</th>\n","      <th>category_Music_Jazz</th>\n","      <th>category_Music_Kids</th>\n","      <th>category_Music_Latin</th>\n","      <th>category_Music_Metal</th>\n","      <th>category_Music_No Subcategory</th>\n","      <th>category_Music_Pop</th>\n","      <th>category_Music_Punk</th>\n","      <th>category_Music_R&amp;B</th>\n","      <th>category_Music_Rock</th>\n","      <th>category_Music_World Music</th>\n","      <th>category_Photography_Animals</th>\n","      <th>category_Photography_Fine Art</th>\n","      <th>category_Photography_Nature</th>\n","      <th>category_Photography_No Subcategory</th>\n","      <th>category_Photography_People</th>\n","      <th>category_Photography_Photobooks</th>\n","      <th>category_Photography_Places</th>\n","      <th>category_Publishing_Academic</th>\n","      <th>category_Publishing_Anthologies</th>\n","      <th>category_Publishing_Art Books</th>\n","      <th>category_Publishing_Calendars</th>\n","      <th>category_Publishing_Children's Books</th>\n","      <th>category_Publishing_Comedy</th>\n","      <th>category_Publishing_Fiction</th>\n","      <th>category_Publishing_Letterpress</th>\n","      <th>category_Publishing_Literary Journals</th>\n","      <th>category_Publishing_Literary Spaces</th>\n","      <th>category_Publishing_No Subcategory</th>\n","      <th>category_Publishing_Nonfiction</th>\n","      <th>category_Publishing_Periodicals</th>\n","      <th>category_Publishing_Poetry</th>\n","      <th>category_Publishing_Radio &amp; Podcasts</th>\n","      <th>category_Publishing_Translations</th>\n","      <th>category_Publishing_Young Adult</th>\n","      <th>category_Publishing_Zines</th>\n","      <th>category_Technology_3D Printing</th>\n","      <th>category_Technology_Apps</th>\n","      <th>category_Technology_Camera Equipment</th>\n","      <th>category_Technology_DIY Electronics</th>\n","      <th>category_Technology_Fabrication Tools</th>\n","      <th>category_Technology_Flight</th>\n","      <th>category_Technology_Gadgets</th>\n","      <th>category_Technology_Hardware</th>\n","      <th>category_Technology_Makerspaces</th>\n","      <th>category_Technology_No Subcategory</th>\n","      <th>category_Technology_Robots</th>\n","      <th>category_Technology_Software</th>\n","      <th>category_Technology_Sound</th>\n","      <th>category_Technology_Space Exploration</th>\n","      <th>category_Technology_Wearables</th>\n","      <th>category_Technology_Web</th>\n","      <th>category_Theater_Comedy</th>\n","      <th>category_Theater_Experimental</th>\n","      <th>category_Theater_Festivals</th>\n","      <th>category_Theater_Immersive</th>\n","      <th>category_Theater_Musical</th>\n","      <th>category_Theater_No Subcategory</th>\n","      <th>category_Theater_Plays</th>\n","      <th>category_Theater_Spaces</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>22821161</th>\n","      <td>0</td>\n","      <td>50000.0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>102</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1378</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>36.750000</td>\n","      <td>5.0</td>\n","      <td>1417.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22823613</th>\n","      <td>0</td>\n","      <td>750.0</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>256</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>20.500000</td>\n","      <td>15.0</td>\n","      <td>30.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22835897</th>\n","      <td>1</td>\n","      <td>6000.0</td>\n","      <td>0</td>\n","      <td>28</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>712</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>44.000000</td>\n","      <td>1.0</td>\n","      <td>6000.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22845619</th>\n","      <td>1</td>\n","      <td>8000.0</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>25</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>676</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>64.545455</td>\n","      <td>5.0</td>\n","      <td>5000.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22848517</th>\n","      <td>1</td>\n","      <td>2500.0</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>93</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>497</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>45.875000</td>\n","      <td>1.0</td>\n","      <td>250.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          campaign_successful     goal  number_of_collaborators  \\\n","22821161                    0  50000.0                        0   \n","22823613                    0    750.0                        0   \n","22835897                    1   6000.0                        0   \n","22845619                    1   8000.0                        0   \n","22848517                    1   2500.0                        0   \n","\n","          funding_period  days_between_created_and_launched  staff_pick  \\\n","22821161              45                                102           0   \n","22823613              30                                 13           0   \n","22835897              28                                 10           0   \n","22845619              30                                 25           0   \n","22848517              30                                 93           0   \n","\n","          campaign_has_demo_video  campaign_has_environmental_commitments  \\\n","22821161                        1                                       0   \n","22823613                        0                                       0   \n","22835897                        1                                       0   \n","22845619                        1                                       0   \n","22848517                        1                                       0   \n","\n","          number_of_images  number_of_videos  number_of_audios  \\\n","22821161                13                 0                 0   \n","22823613                 0                 0                 0   \n","22835897                 6                 0                 0   \n","22845619                 1                 0                 0   \n","22848517                 4                 0                 0   \n","\n","          number_of_interactives  number_of_words  number_of_links  \\\n","22821161                       0             1378                2   \n","22823613                       0              256                0   \n","22835897                       0              712                4   \n","22845619                       0              676                3   \n","22848517                       0              497                3   \n","\n","          creator_verified_identity  creator_fb_auth  creator_has_image  \\\n","22821161                          1                0                  1   \n","22823613                          1                0                  1   \n","22835897                          1                0                  1   \n","22845619                          0                0                  1   \n","22848517                          0                1                  1   \n","\n","          creator_allows_follows  number_of_creator_backings  \\\n","22821161                       1                           0   \n","22823613                       1                           0   \n","22835897                       1                           4   \n","22845619                       1                           1   \n","22848517                       1                           0   \n","\n","          number_of_creator_projects  facebook_linked  twitter_linked  \\\n","22821161                           1                0               0   \n","22823613                           1                0               0   \n","22835897                           1                1               0   \n","22845619                           1                1               0   \n","22848517                           1                0               0   \n","\n","          instagram_linked  linkedin_linked  number_of_rewards  \\\n","22821161                 0                0                  8   \n","22823613                 0                0                  2   \n","22835897                 0                0                 12   \n","22845619                 0                0                 11   \n","22848517                 0                0                  8   \n","\n","          number_of_words_per_reward  lowest_pledge_level  \\\n","22821161                   36.750000                  5.0   \n","22823613                   20.500000                 15.0   \n","22835897                   44.000000                  1.0   \n","22845619                   64.545455                  5.0   \n","22848517                   45.875000                  1.0   \n","\n","          highest_pledge_level  has_limited_rewards  has_shipped_rewards  \\\n","22821161                1417.0                    0                    1   \n","22823613                  30.0                    1                    0   \n","22835897                6000.0                    1                    1   \n","22845619                5000.0                    0                    0   \n","22848517                 250.0                    1                    0   \n","\n","          has_restricted_shipping_rewards  avg_months_until_reward  \\\n","22821161                                1                      5.5   \n","22823613                                0                      4.0   \n","22835897                                0                      5.5   \n","22845619                                0                      0.0   \n","22848517                                0                      3.0   \n","\n","          launch_quartal_1  launch_quartal_2  launch_quartal_3  \\\n","22821161                 0                 0                 1   \n","22823613                 1                 0                 0   \n","22835897                 0                 0                 1   \n","22845619                 0                 0                 1   \n","22848517                 0                 1                 0   \n","\n","          launch_quartal_4  location_Africa  location_Australia  \\\n","22821161                 0                0                   0   \n","22823613                 0                0                   0   \n","22835897                 0                0                   0   \n","22845619                 0                0                   0   \n","22848517                 0                0                   0   \n","\n","          location_Belgium  location_Canada  location_China  location_Denmark  \\\n","22821161                 0                0               0                 0   \n","22823613                 0                0               0                 0   \n","22835897                 0                0               0                 0   \n","22845619                 0                0               0                 0   \n","22848517                 0                0               0                 0   \n","\n","          location_France  location_Germany  location_Hong Kong  \\\n","22821161                0                 0                   0   \n","22823613                0                 0                   0   \n","22835897                0                 0                   0   \n","22845619                0                 0                   0   \n","22848517                0                 0                   0   \n","\n","          location_Ireland  location_Italy  location_Japan  \\\n","22821161                 0               0               0   \n","22823613                 0               0               0   \n","22835897                 0               0               0   \n","22845619                 0               0               0   \n","22848517                 0               0               0   \n","\n","          location_Latin and South America  location_Mexico  \\\n","22821161                                 0                0   \n","22823613                                 0                0   \n","22835897                                 0                0   \n","22845619                                 0                0   \n","22848517                                 0                0   \n","\n","          location_Netherlands  location_New Zealand  location_No Location  \\\n","22821161                     0                     0                     0   \n","22823613                     0                     0                     0   \n","22835897                     0                     0                     0   \n","22845619                     0                     0                     0   \n","22848517                     0                     0                     0   \n","\n","          location_Norway  location_Oceania and Antarctica  \\\n","22821161                0                                0   \n","22823613                0                                0   \n","22835897                0                                0   \n","22845619                0                                0   \n","22848517                0                                0   \n","\n","          location_Rest of Asia  location_Rest of Europe  location_Singapore  \\\n","22821161                      0                        0                   0   \n","22823613                      0                        0                   0   \n","22835897                      0                        0                   0   \n","22845619                      0                        0                   0   \n","22848517                      0                        0                   0   \n","\n","          location_Spain  location_Sweden  location_Switzerland  \\\n","22821161               0                0                     0   \n","22823613               0                0                     0   \n","22835897               0                0                     0   \n","22845619               0                0                     0   \n","22848517               0                0                     0   \n","\n","          location_United Kingdom  location_United States  \\\n","22821161                        0                       1   \n","22823613                        0                       1   \n","22835897                        0                       1   \n","22845619                        0                       1   \n","22848517                        0                       1   \n","\n","          category_Art_Ceramics  category_Art_Conceptual Art  \\\n","22821161                      0                            0   \n","22823613                      0                            0   \n","22835897                      0                            0   \n","22845619                      0                            0   \n","22848517                      0                            0   \n","\n","          category_Art_Digital Art  category_Art_Illustration  \\\n","22821161                         0                          0   \n","22823613                         0                          0   \n","22835897                         0                          0   \n","22845619                         0                          0   \n","22848517                         0                          0   \n","\n","          category_Art_Installations  category_Art_Mixed Media  \\\n","22821161                           0                         0   \n","22823613                           0                         0   \n","22835897                           0                         0   \n","22845619                           0                         0   \n","22848517                           0                         0   \n","\n","          category_Art_No Subcategory  category_Art_Painting  \\\n","22821161                            0                      0   \n","22823613                            0                      0   \n","22835897                            0                      0   \n","22845619                            0                      0   \n","22848517                            0                      0   \n","\n","          category_Art_Performance Art  category_Art_Public Art  \\\n","22821161                             0                        0   \n","22823613                             0                        0   \n","22835897                             0                        0   \n","22845619                             0                        0   \n","22848517                             0                        0   \n","\n","          category_Art_Sculpture  category_Art_Social Practice  \\\n","22821161                       0                             0   \n","22823613                       0                             0   \n","22835897                       0                             0   \n","22845619                       0                             0   \n","22848517                       0                             0   \n","\n","          category_Art_Textiles  category_Art_Video Art  \\\n","22821161                      0                       0   \n","22823613                      0                       0   \n","22835897                      0                       0   \n","22845619                      0                       0   \n","22848517                      0                       0   \n","\n","          category_Comics_Anthologies  category_Comics_Comic Books  \\\n","22821161                            0                            0   \n","22823613                            0                            0   \n","22835897                            0                            0   \n","22845619                            0                            0   \n","22848517                            0                            0   \n","\n","          category_Comics_Events  category_Comics_Graphic Novels  \\\n","22821161                       0                               0   \n","22823613                       0                               0   \n","22835897                       0                               0   \n","22845619                       0                               0   \n","22848517                       0                               0   \n","\n","          category_Comics_No Subcategory  category_Comics_Webcomics  \\\n","22821161                               0                          0   \n","22823613                               0                          0   \n","22835897                               0                          0   \n","22845619                               0                          0   \n","22848517                               0                          0   \n","\n","          category_Crafts_Candles  category_Crafts_Crochet  \\\n","22821161                        0                        0   \n","22823613                        0                        0   \n","22835897                        0                        0   \n","22845619                        0                        0   \n","22848517                        0                        0   \n","\n","          category_Crafts_DIY  category_Crafts_Embroidery  \\\n","22821161                    0                           0   \n","22823613                    0                           0   \n","22835897                    0                           0   \n","22845619                    0                           0   \n","22848517                    0                           0   \n","\n","          category_Crafts_Glass  category_Crafts_Knitting  \\\n","22821161                      0                         0   \n","22823613                      0                         0   \n","22835897                      0                         0   \n","22845619                      0                         0   \n","22848517                      0                         0   \n","\n","          category_Crafts_No Subcategory  category_Crafts_Pottery  \\\n","22821161                               0                        0   \n","22823613                               0                        0   \n","22835897                               0                        0   \n","22845619                               0                        0   \n","22848517                               0                        0   \n","\n","          category_Crafts_Printing  category_Crafts_Quilts  \\\n","22821161                         0                       0   \n","22823613                         0                       0   \n","22835897                         0                       0   \n","22845619                         0                       0   \n","22848517                         0                       0   \n","\n","          category_Crafts_Stationery  category_Crafts_Taxidermy  \\\n","22821161                           0                          0   \n","22823613                           0                          0   \n","22835897                           0                          0   \n","22845619                           0                          0   \n","22848517                           0                          0   \n","\n","          category_Crafts_Weaving  category_Crafts_Woodworking  \\\n","22821161                        0                            0   \n","22823613                        0                            0   \n","22835897                        0                            0   \n","22845619                        0                            0   \n","22848517                        0                            0   \n","\n","          category_Dance_No Subcategory  category_Dance_Performances  \\\n","22821161                              0                            0   \n","22823613                              0                            0   \n","22835897                              0                            0   \n","22845619                              0                            0   \n","22848517                              0                            0   \n","\n","          category_Dance_Residencies  category_Dance_Spaces  \\\n","22821161                           0                      0   \n","22823613                           0                      0   \n","22835897                           0                      0   \n","22845619                           0                      0   \n","22848517                           0                      0   \n","\n","          category_Dance_Workshops  category_Design_Architecture  \\\n","22821161                         0                             0   \n","22823613                         0                             0   \n","22835897                         0                             0   \n","22845619                         0                             0   \n","22848517                         0                             0   \n","\n","          category_Design_Civic Design  category_Design_Graphic Design  \\\n","22821161                             0                               0   \n","22823613                             0                               0   \n","22835897                             0                               0   \n","22845619                             0                               0   \n","22848517                             0                               0   \n","\n","          category_Design_Interactive Design  category_Design_No Subcategory  \\\n","22821161                                   0                               0   \n","22823613                                   0                               0   \n","22835897                                   0                               0   \n","22845619                                   0                               0   \n","22848517                                   0                               0   \n","\n","          category_Design_Product Design  category_Design_Toys  \\\n","22821161                               1                     0   \n","22823613                               0                     0   \n","22835897                               0                     0   \n","22845619                               0                     0   \n","22848517                               0                     0   \n","\n","          category_Design_Typography  category_Fashion_Accessories  \\\n","22821161                           0                             0   \n","22823613                           0                             0   \n","22835897                           0                             0   \n","22845619                           0                             0   \n","22848517                           0                             0   \n","\n","          category_Fashion_Apparel  category_Fashion_Childrenswear  \\\n","22821161                         0                               0   \n","22823613                         0                               0   \n","22835897                         0                               0   \n","22845619                         0                               0   \n","22848517                         0                               0   \n","\n","          category_Fashion_Couture  category_Fashion_Footwear  \\\n","22821161                         0                          0   \n","22823613                         0                          0   \n","22835897                         0                          0   \n","22845619                         0                          0   \n","22848517                         0                          0   \n","\n","          category_Fashion_Jewelry  category_Fashion_No Subcategory  \\\n","22821161                         0                                0   \n","22823613                         0                                0   \n","22835897                         0                                0   \n","22845619                         0                                0   \n","22848517                         0                                0   \n","\n","          category_Fashion_Pet Fashion  category_Fashion_Ready-to-wear  \\\n","22821161                             0                               0   \n","22823613                             0                               0   \n","22835897                             0                               0   \n","22845619                             0                               0   \n","22848517                             0                               0   \n","\n","          category_Film & Video_Action  category_Film & Video_Animation  \\\n","22821161                             0                                0   \n","22823613                             0                                0   \n","22835897                             0                                0   \n","22845619                             0                                0   \n","22848517                             0                                0   \n","\n","          category_Film & Video_Comedy  category_Film & Video_Documentary  \\\n","22821161                             0                                  0   \n","22823613                             0                                  0   \n","22835897                             0                                  0   \n","22845619                             0                                  0   \n","22848517                             0                                  0   \n","\n","          category_Film & Video_Drama  category_Film & Video_Experimental  \\\n","22821161                            0                                   0   \n","22823613                            0                                   0   \n","22835897                            0                                   0   \n","22845619                            0                                   0   \n","22848517                            0                                   0   \n","\n","          category_Film & Video_Family  category_Film & Video_Fantasy  \\\n","22821161                             0                              0   \n","22823613                             0                              0   \n","22835897                             0                              0   \n","22845619                             0                              0   \n","22848517                             0                              0   \n","\n","          category_Film & Video_Festivals  category_Film & Video_Horror  \\\n","22821161                                0                             0   \n","22823613                                0                             0   \n","22835897                                0                             0   \n","22845619                                0                             0   \n","22848517                                0                             0   \n","\n","          category_Film & Video_Movie Theaters  \\\n","22821161                                     0   \n","22823613                                     0   \n","22835897                                     0   \n","22845619                                     0   \n","22848517                                     0   \n","\n","          category_Film & Video_Music Videos  \\\n","22821161                                   0   \n","22823613                                   0   \n","22835897                                   0   \n","22845619                                   0   \n","22848517                                   0   \n","\n","          category_Film & Video_Narrative Film  \\\n","22821161                                     0   \n","22823613                                     0   \n","22835897                                     0   \n","22845619                                     0   \n","22848517                                     0   \n","\n","          category_Film & Video_No Subcategory  category_Film & Video_Romance  \\\n","22821161                                     0                              0   \n","22823613                                     0                              0   \n","22835897                                     0                              0   \n","22845619                                     0                              0   \n","22848517                                     0                              0   \n","\n","          category_Film & Video_Science Fiction  category_Film & Video_Shorts  \\\n","22821161                                      0                             0   \n","22823613                                      0                             0   \n","22835897                                      0                             0   \n","22845619                                      0                             0   \n","22848517                                      0                             0   \n","\n","          category_Film & Video_Television  category_Film & Video_Thrillers  \\\n","22821161                                 0                                0   \n","22823613                                 0                                0   \n","22835897                                 0                                0   \n","22845619                                 0                                0   \n","22848517                                 0                                0   \n","\n","          category_Film & Video_Webseries  category_Food_Bacon  \\\n","22821161                                0                    0   \n","22823613                                0                    0   \n","22835897                                0                    0   \n","22845619                                0                    0   \n","22848517                                0                    0   \n","\n","          category_Food_Community Gardens  category_Food_Cookbooks  \\\n","22821161                                0                        0   \n","22823613                                0                        0   \n","22835897                                0                        0   \n","22845619                                0                        0   \n","22848517                                0                        0   \n","\n","          category_Food_Drinks  category_Food_Events  \\\n","22821161                     0                     0   \n","22823613                     0                     0   \n","22835897                     0                     0   \n","22845619                     0                     0   \n","22848517                     0                     0   \n","\n","          category_Food_Farmer's Markets  category_Food_Farms  \\\n","22821161                               0                    0   \n","22823613                               0                    0   \n","22835897                               0                    0   \n","22845619                               0                    0   \n","22848517                               0                    0   \n","\n","          category_Food_Food Trucks  category_Food_No Subcategory  \\\n","22821161                          0                             0   \n","22823613                          0                             0   \n","22835897                          0                             0   \n","22845619                          0                             0   \n","22848517                          0                             0   \n","\n","          category_Food_Restaurants  category_Food_Small Batch  \\\n","22821161                          0                          0   \n","22823613                          0                          0   \n","22835897                          0                          0   \n","22845619                          0                          0   \n","22848517                          0                          0   \n","\n","          category_Food_Spaces  category_Food_Vegan  \\\n","22821161                     0                    0   \n","22823613                     0                    0   \n","22835897                     0                    0   \n","22845619                     0                    0   \n","22848517                     0                    0   \n","\n","          category_Games_Gaming Hardware  category_Games_Live Games  \\\n","22821161                               0                          0   \n","22823613                               0                          0   \n","22835897                               0                          0   \n","22845619                               0                          0   \n","22848517                               0                          0   \n","\n","          category_Games_Mobile Games  category_Games_No Subcategory  \\\n","22821161                            0                              0   \n","22823613                            0                              0   \n","22835897                            0                              0   \n","22845619                            0                              0   \n","22848517                            0                              0   \n","\n","          category_Games_Playing Cards  category_Games_Puzzles  \\\n","22821161                             0                       0   \n","22823613                             0                       0   \n","22835897                             0                       0   \n","22845619                             0                       0   \n","22848517                             0                       0   \n","\n","          category_Games_Tabletop Games  category_Games_Video Games  \\\n","22821161                              0                           0   \n","22823613                              0                           0   \n","22835897                              0                           0   \n","22845619                              0                           0   \n","22848517                              1                           0   \n","\n","          category_Journalism_Audio  category_Journalism_No Subcategory  \\\n","22821161                          0                                   0   \n","22823613                          0                                   0   \n","22835897                          0                                   0   \n","22845619                          0                                   0   \n","22848517                          0                                   0   \n","\n","          category_Journalism_Photo  category_Journalism_Print  \\\n","22821161                          0                          0   \n","22823613                          0                          0   \n","22835897                          0                          0   \n","22845619                          0                          0   \n","22848517                          0                          0   \n","\n","          category_Journalism_Video  category_Journalism_Web  \\\n","22821161                          0                        0   \n","22823613                          0                        0   \n","22835897                          0                        0   \n","22845619                          0                        0   \n","22848517                          0                        0   \n","\n","          category_Music_Blues  category_Music_Chiptune  \\\n","22821161                     0                        0   \n","22823613                     0                        0   \n","22835897                     0                        0   \n","22845619                     0                        0   \n","22848517                     0                        0   \n","\n","          category_Music_Classical Music  category_Music_Comedy  \\\n","22821161                               0                      0   \n","22823613                               0                      0   \n","22835897                               0                      0   \n","22845619                               0                      0   \n","22848517                               0                      0   \n","\n","          category_Music_Country & Folk  category_Music_Electronic Music  \\\n","22821161                              0                                0   \n","22823613                              0                                0   \n","22835897                              0                                0   \n","22845619                              0                                0   \n","22848517                              0                                0   \n","\n","          category_Music_Faith  category_Music_Hip-Hop  \\\n","22821161                     0                       0   \n","22823613                     0                       0   \n","22835897                     0                       0   \n","22845619                     0                       0   \n","22848517                     0                       0   \n","\n","          category_Music_Indie Rock  category_Music_Jazz  category_Music_Kids  \\\n","22821161                          0                    0                    0   \n","22823613                          0                    0                    0   \n","22835897                          0                    0                    0   \n","22845619                          1                    0                    0   \n","22848517                          0                    0                    0   \n","\n","          category_Music_Latin  category_Music_Metal  \\\n","22821161                     0                     0   \n","22823613                     0                     0   \n","22835897                     0                     0   \n","22845619                     0                     0   \n","22848517                     0                     0   \n","\n","          category_Music_No Subcategory  category_Music_Pop  \\\n","22821161                              0                   0   \n","22823613                              0                   0   \n","22835897                              0                   0   \n","22845619                              0                   0   \n","22848517                              0                   0   \n","\n","          category_Music_Punk  category_Music_R&B  category_Music_Rock  \\\n","22821161                    0                   0                    0   \n","22823613                    0                   0                    0   \n","22835897                    0                   0                    0   \n","22845619                    0                   0                    0   \n","22848517                    0                   0                    0   \n","\n","          category_Music_World Music  category_Photography_Animals  \\\n","22821161                           0                             0   \n","22823613                           0                             0   \n","22835897                           0                             0   \n","22845619                           0                             0   \n","22848517                           0                             0   \n","\n","          category_Photography_Fine Art  category_Photography_Nature  \\\n","22821161                              0                            0   \n","22823613                              0                            0   \n","22835897                              0                            0   \n","22845619                              0                            0   \n","22848517                              0                            0   \n","\n","          category_Photography_No Subcategory  category_Photography_People  \\\n","22821161                                    0                            0   \n","22823613                                    0                            0   \n","22835897                                    0                            0   \n","22845619                                    0                            0   \n","22848517                                    0                            0   \n","\n","          category_Photography_Photobooks  category_Photography_Places  \\\n","22821161                                0                            0   \n","22823613                                0                            0   \n","22835897                                0                            0   \n","22845619                                0                            0   \n","22848517                                0                            0   \n","\n","          category_Publishing_Academic  category_Publishing_Anthologies  \\\n","22821161                             0                                0   \n","22823613                             0                                0   \n","22835897                             0                                0   \n","22845619                             0                                0   \n","22848517                             0                                0   \n","\n","          category_Publishing_Art Books  category_Publishing_Calendars  \\\n","22821161                              0                              0   \n","22823613                              0                              0   \n","22835897                              0                              0   \n","22845619                              0                              0   \n","22848517                              0                              0   \n","\n","          category_Publishing_Children's Books  category_Publishing_Comedy  \\\n","22821161                                     0                           0   \n","22823613                                     0                           0   \n","22835897                                     1                           0   \n","22845619                                     0                           0   \n","22848517                                     0                           0   \n","\n","          category_Publishing_Fiction  category_Publishing_Letterpress  \\\n","22821161                            0                                0   \n","22823613                            0                                0   \n","22835897                            0                                0   \n","22845619                            0                                0   \n","22848517                            0                                0   \n","\n","          category_Publishing_Literary Journals  \\\n","22821161                                      0   \n","22823613                                      0   \n","22835897                                      0   \n","22845619                                      0   \n","22848517                                      0   \n","\n","          category_Publishing_Literary Spaces  \\\n","22821161                                    0   \n","22823613                                    0   \n","22835897                                    0   \n","22845619                                    0   \n","22848517                                    0   \n","\n","          category_Publishing_No Subcategory  category_Publishing_Nonfiction  \\\n","22821161                                   0                               0   \n","22823613                                   0                               0   \n","22835897                                   0                               0   \n","22845619                                   0                               0   \n","22848517                                   0                               0   \n","\n","          category_Publishing_Periodicals  category_Publishing_Poetry  \\\n","22821161                                0                           0   \n","22823613                                0                           0   \n","22835897                                0                           0   \n","22845619                                0                           0   \n","22848517                                0                           0   \n","\n","          category_Publishing_Radio & Podcasts  \\\n","22821161                                     0   \n","22823613                                     0   \n","22835897                                     0   \n","22845619                                     0   \n","22848517                                     0   \n","\n","          category_Publishing_Translations  category_Publishing_Young Adult  \\\n","22821161                                 0                                0   \n","22823613                                 0                                0   \n","22835897                                 0                                0   \n","22845619                                 0                                0   \n","22848517                                 0                                0   \n","\n","          category_Publishing_Zines  category_Technology_3D Printing  \\\n","22821161                          0                                0   \n","22823613                          0                                0   \n","22835897                          0                                0   \n","22845619                          0                                0   \n","22848517                          0                                0   \n","\n","          category_Technology_Apps  category_Technology_Camera Equipment  \\\n","22821161                         0                                     0   \n","22823613                         0                                     0   \n","22835897                         0                                     0   \n","22845619                         0                                     0   \n","22848517                         0                                     0   \n","\n","          category_Technology_DIY Electronics  \\\n","22821161                                    0   \n","22823613                                    0   \n","22835897                                    0   \n","22845619                                    0   \n","22848517                                    0   \n","\n","          category_Technology_Fabrication Tools  category_Technology_Flight  \\\n","22821161                                      0                           0   \n","22823613                                      0                           0   \n","22835897                                      0                           0   \n","22845619                                      0                           0   \n","22848517                                      0                           0   \n","\n","          category_Technology_Gadgets  category_Technology_Hardware  \\\n","22821161                            0                             0   \n","22823613                            0                             0   \n","22835897                            0                             0   \n","22845619                            0                             0   \n","22848517                            0                             0   \n","\n","          category_Technology_Makerspaces  category_Technology_No Subcategory  \\\n","22821161                                0                                   0   \n","22823613                                0                                   0   \n","22835897                                0                                   0   \n","22845619                                0                                   0   \n","22848517                                0                                   0   \n","\n","          category_Technology_Robots  category_Technology_Software  \\\n","22821161                           0                             0   \n","22823613                           0                             0   \n","22835897                           0                             0   \n","22845619                           0                             0   \n","22848517                           0                             0   \n","\n","          category_Technology_Sound  category_Technology_Space Exploration  \\\n","22821161                          0                                      0   \n","22823613                          0                                      0   \n","22835897                          0                                      0   \n","22845619                          0                                      0   \n","22848517                          0                                      0   \n","\n","          category_Technology_Wearables  category_Technology_Web  \\\n","22821161                              0                        0   \n","22823613                              0                        1   \n","22835897                              0                        0   \n","22845619                              0                        0   \n","22848517                              0                        0   \n","\n","          category_Theater_Comedy  category_Theater_Experimental  \\\n","22821161                        0                              0   \n","22823613                        0                              0   \n","22835897                        0                              0   \n","22845619                        0                              0   \n","22848517                        0                              0   \n","\n","          category_Theater_Festivals  category_Theater_Immersive  \\\n","22821161                           0                           0   \n","22823613                           0                           0   \n","22835897                           0                           0   \n","22845619                           0                           0   \n","22848517                           0                           0   \n","\n","          category_Theater_Musical  category_Theater_No Subcategory  \\\n","22821161                         0                                0   \n","22823613                         0                                0   \n","22835897                         0                                0   \n","22845619                         0                                0   \n","22848517                         0                                0   \n","\n","          category_Theater_Plays  category_Theater_Spaces  \n","22821161                       0                        0  \n","22823613                       0                        0  \n","22835897                       0                        0  \n","22845619                       0                        0  \n","22848517                       0                        0  "]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdk7Ma-dsMlb","executionInfo":{"status":"ok","timestamp":1615720779888,"user_tz":-60,"elapsed":935,"user":{"displayName":"David George","photoUrl":"","userId":"01025812402211939868"}},"outputId":"79aeba66-8b5e-4471-b2c8-cbb17079e0b5"},"source":["# Convert dataset and target variable to Numpy Arrays\n","y = kickstarter_df[\"campaign_successful\"].to_numpy()\n","kickstarter_notarget_df = kickstarter_df.drop(columns=[\"campaign_successful\"])\n","X = kickstarter_notarget_df.astype(\"float32\").to_numpy()\n","\n","print(type(y))\n","print(y.shape)\n","print(type(X))\n","print(X.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","(246891,)\n","<class 'numpy.ndarray'>\n","(246891, 233)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3_B_DfqOOLp0"},"source":["# Retrieve column names which will be used for later pre-processing and evaluation\n","feature_names = kickstarter_notarget_df.columns.values\n","numeric_feature_names = [\"goal\", \"number_of_collaborators\", \"funding_period\", \"days_between_created_and_launched\", \"number_of_images\", \"number_of_videos\", \"number_of_audios\", \"number_of_interactives\", \"number_of_words\", \"number_of_links\", \"number_of_creator_backings\", \"number_of_creator_projects\", \"number_of_rewards\", \"number_of_words_per_reward\", \"lowest_pledge_level\", \"highest_pledge_level\", \"avg_months_until_reward\"]\n","binary_feature_names = [x for x in feature_names if x not in numeric_feature_names]\n","numeric_features = [kickstarter_notarget_df.columns.get_loc(x) for x in numeric_feature_names]\n","binary_features = [kickstarter_notarget_df.columns.get_loc(x) for x in binary_feature_names]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26YyHOP-J_o0","executionInfo":{"status":"ok","timestamp":1615720784574,"user_tz":-60,"elapsed":1505,"user":{"displayName":"David George","photoUrl":"","userId":"01025812402211939868"}},"outputId":"dcc9aa05-c25a-417c-ad69-fea8a71e58d9"},"source":["# Split dataset into training, subtraining, validation, and test set\n","train_size = round(kickstarter_df.shape[0]*0.7*1)\n","val_size = round(kickstarter_df.shape[0]*0.15*1)\n","test_size = round(kickstarter_df.shape[0]*1) - val_size - train_size\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=(train_size+val_size),test_size=test_size, shuffle=True, stratify=y, random_state=seed_value)\n","X_subtrain, X_val, y_subtrain, y_val = train_test_split(X_train, y_train, train_size=train_size, test_size=val_size, shuffle=True, stratify=y_train, random_state=seed_value)\n","\n","print(\"Shape of X_train: {}\".format(X_train.shape))\n","print(\"Shape of y_train: {}\".format(y_train.shape))\n","print(\"Shape of X_subtrain: {}\".format(X_subtrain.shape))\n","print(\"Shape of y_subtrain: {}\".format(y_subtrain.shape))\n","print(\"Shape of X_val: {}\".format(X_val.shape))\n","print(\"Shape of y_val: {}\".format(y_val.shape))\n","print(\"Shape of X_test: {}\".format(X_test.shape))\n","print(\"Shape of y_test: {}\".format(y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of X_train: (209858, 233)\n","Shape of y_train: (209858,)\n","Shape of X_subtrain: (172824, 233)\n","Shape of y_subtrain: (172824,)\n","Shape of X_val: (37034, 233)\n","Shape of y_val: (37034,)\n","Shape of X_test: (37033, 233)\n","Shape of y_test: (37033,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QlcazcrXKT5e"},"source":["#### b) Baseline"]},{"cell_type":"markdown","metadata":{"id":"hu2HEVe5Kf5S"},"source":["This model provides the baseline, which will be used as starting point for the hyperparameter search. The goal is to gradually improve the model performance.\n","\n","**Hyperparamaters:**\n","- Untransformed Dataset\n","- Regularizer = None\n","- Optimizer = RMSprop\n","- Weight Initialization = GlorotNormal\n","- Batch Size = 256\n","- Epochs = 20"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVUi49YRKDXi","executionInfo":{"status":"ok","timestamp":1611299245508,"user_tz":-60,"elapsed":14003,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"983298d7-cfa5-4993-bbc5-387fd2dceddf"},"source":["model = Sequential()\n","model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=L1(l1=0.0000), kernel_initializer=GlorotNormal(seed=seed_value)))\n","model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","history = model.fit(X_subtrain, y_subtrain, validation_data=(X_val, y_val), batch_size=256, epochs=100, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","print(\"Training Accuracy: {:.3f}\".format(model.evaluate(X_subtrain, y_subtrain, batch_size=256, verbose=0)[1]))\n","print(\"Validation Accuracy: {:.3f}\".format(model.evaluate(X_val, y_val, batch_size=256, verbose=0)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","676/676 [==============================] - 2s 2ms/step - loss: 113.0285 - binary_accuracy: 0.6713 - val_loss: 2.1117 - val_binary_accuracy: 0.7044\n","Epoch 2/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.1456 - binary_accuracy: 0.6860 - val_loss: 6.4972 - val_binary_accuracy: 0.3995\n","Epoch 3/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.2455 - binary_accuracy: 0.6893 - val_loss: 0.8982 - val_binary_accuracy: 0.7402\n","Epoch 4/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.2477 - binary_accuracy: 0.6916 - val_loss: 1.5085 - val_binary_accuracy: 0.7364\n","Epoch 5/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.1612 - binary_accuracy: 0.6998 - val_loss: 0.7620 - val_binary_accuracy: 0.7607\n","Epoch 6/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.1460 - binary_accuracy: 0.7027 - val_loss: 3.9274 - val_binary_accuracy: 0.6998\n","Epoch 7/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.0862 - binary_accuracy: 0.7073 - val_loss: 3.6664 - val_binary_accuracy: 0.6874\n","Epoch 8/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.1466 - binary_accuracy: 0.7075 - val_loss: 0.7218 - val_binary_accuracy: 0.7557\n","Epoch 9/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.1023 - binary_accuracy: 0.7079 - val_loss: 1.3417 - val_binary_accuracy: 0.7285\n","Epoch 10/100\n","676/676 [==============================] - 1s 2ms/step - loss: 2.1330 - binary_accuracy: 0.7078 - val_loss: 3.7521 - val_binary_accuracy: 0.7051\n","Restoring model weights from the end of the best epoch.\n","Epoch 00010: early stopping\n","Training Accuracy: 0.763\n","Validation Accuracy: 0.761\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6250os8sMSuZ"},"source":["#### c) Test Different Preprocessing Strategies"]},{"cell_type":"markdown","metadata":{"id":"APCM6IayNmOx"},"source":["##### 1. Standardization: Numerical + Binary Attributes"]},{"cell_type":"code","metadata":{"id":"2V-o3yWELd0C"},"source":["# Specify pre-processing pipeline for the different types of attributes\n","preprocessing = ColumnTransformer(\n","    [\n","      (\"numeric\", StandardScaler(), numeric_features),\n","      (\"binary\", StandardScaler(), binary_features)\n","    ],\n","    remainder=\"drop\", verbose=True, n_jobs=-1\n",").fit(X_subtrain)\n","X_subtrain_transformed = preprocessing.transform(X_subtrain)\n","X_val_transformed = preprocessing.transform(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"YtZlLoT7Ld7l","executionInfo":{"status":"ok","timestamp":1611299653732,"user_tz":-60,"elapsed":677,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"b84de675-fb88-4c5e-be61-31a57439464e"},"source":["# Validation\n","tmp = pd.DataFrame(X_subtrain_transformed)\n","print(tmp.shape)\n","tmp.head(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(172824, 233)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","      <th>110</th>\n","      <th>111</th>\n","      <th>112</th>\n","      <th>113</th>\n","      <th>114</th>\n","      <th>115</th>\n","      <th>116</th>\n","      <th>117</th>\n","      <th>118</th>\n","      <th>119</th>\n","      <th>120</th>\n","      <th>121</th>\n","      <th>122</th>\n","      <th>123</th>\n","      <th>124</th>\n","      <th>125</th>\n","      <th>126</th>\n","      <th>127</th>\n","      <th>128</th>\n","      <th>129</th>\n","      <th>130</th>\n","      <th>131</th>\n","      <th>132</th>\n","      <th>133</th>\n","      <th>134</th>\n","      <th>135</th>\n","      <th>136</th>\n","      <th>137</th>\n","      <th>138</th>\n","      <th>139</th>\n","      <th>140</th>\n","      <th>141</th>\n","      <th>142</th>\n","      <th>143</th>\n","      <th>144</th>\n","      <th>145</th>\n","      <th>146</th>\n","      <th>147</th>\n","      <th>148</th>\n","      <th>149</th>\n","      <th>150</th>\n","      <th>151</th>\n","      <th>152</th>\n","      <th>153</th>\n","      <th>154</th>\n","      <th>155</th>\n","      <th>156</th>\n","      <th>157</th>\n","      <th>158</th>\n","      <th>159</th>\n","      <th>160</th>\n","      <th>161</th>\n","      <th>162</th>\n","      <th>163</th>\n","      <th>164</th>\n","      <th>165</th>\n","      <th>166</th>\n","      <th>167</th>\n","      <th>168</th>\n","      <th>169</th>\n","      <th>170</th>\n","      <th>171</th>\n","      <th>172</th>\n","      <th>173</th>\n","      <th>174</th>\n","      <th>175</th>\n","      <th>176</th>\n","      <th>177</th>\n","      <th>178</th>\n","      <th>179</th>\n","      <th>180</th>\n","      <th>181</th>\n","      <th>182</th>\n","      <th>183</th>\n","      <th>184</th>\n","      <th>185</th>\n","      <th>186</th>\n","      <th>187</th>\n","      <th>188</th>\n","      <th>189</th>\n","      <th>190</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","      <th>200</th>\n","      <th>201</th>\n","      <th>202</th>\n","      <th>203</th>\n","      <th>204</th>\n","      <th>205</th>\n","      <th>206</th>\n","      <th>207</th>\n","      <th>208</th>\n","      <th>209</th>\n","      <th>210</th>\n","      <th>211</th>\n","      <th>212</th>\n","      <th>213</th>\n","      <th>214</th>\n","      <th>215</th>\n","      <th>216</th>\n","      <th>217</th>\n","      <th>218</th>\n","      <th>219</th>\n","      <th>220</th>\n","      <th>221</th>\n","      <th>222</th>\n","      <th>223</th>\n","      <th>224</th>\n","      <th>225</th>\n","      <th>226</th>\n","      <th>227</th>\n","      <th>228</th>\n","      <th>229</th>\n","      <th>230</th>\n","      <th>231</th>\n","      <th>232</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.298547</td>\n","      <td>-0.198447</td>\n","      <td>-0.941790</td>\n","      <td>-0.301747</td>\n","      <td>-0.542916</td>\n","      <td>-0.264041</td>\n","      <td>-0.08432</td>\n","      <td>-0.01557</td>\n","      <td>-1.130397</td>\n","      <td>-0.746823</td>\n","      <td>-0.215401</td>\n","      <td>-0.224846</td>\n","      <td>-0.265058</td>\n","      <td>-0.639590</td>\n","      <td>-0.094083</td>\n","      <td>-0.563342</td>\n","      <td>0.613136</td>\n","      <td>-0.287578</td>\n","      <td>0.675031</td>\n","      <td>-0.08672</td>\n","      <td>-1.081998</td>\n","      <td>-0.947352</td>\n","      <td>0.147337</td>\n","      <td>0.078146</td>\n","      <td>-0.654237</td>\n","      <td>-0.432702</td>\n","      <td>-0.33426</td>\n","      <td>-0.159431</td>\n","      <td>-1.120286</td>\n","      <td>-1.49029</td>\n","      <td>-0.673015</td>\n","      <td>1.766646</td>\n","      <td>-0.594752</td>\n","      <td>-0.607508</td>\n","      <td>-0.541083</td>\n","      <td>-0.033</td>\n","      <td>-0.145087</td>\n","      <td>-0.04121</td>\n","      <td>-0.199623</td>\n","      <td>-0.036584</td>\n","      <td>-0.052331</td>\n","      <td>-0.070387</td>\n","      <td>-0.097759</td>\n","      <td>-0.057777</td>\n","      <td>-0.052387</td>\n","      <td>-0.094291</td>\n","      <td>-0.048765</td>\n","      <td>-0.053432</td>\n","      <td>-0.048346</td>\n","      <td>-0.084526</td>\n","      <td>-0.063451</td>\n","      <td>-0.059955</td>\n","      <td>-0.040139</td>\n","      <td>-0.009622</td>\n","      <td>-0.064318</td>\n","      <td>-0.085869</td>\n","      <td>-0.051829</td>\n","      <td>-0.072474</td>\n","      <td>-0.066504</td>\n","      <td>-0.041839</td>\n","      <td>-0.324079</td>\n","      <td>0.561241</td>\n","      <td>-0.033436</td>\n","      <td>-0.054723</td>\n","      <td>-0.069468</td>\n","      <td>-0.101491</td>\n","      <td>-0.038213</td>\n","      <td>-0.092748</td>\n","      <td>-0.155436</td>\n","      <td>-0.097396</td>\n","      <td>-0.077544</td>\n","      <td>-0.09176</td>\n","      <td>-0.072312</td>\n","      <td>-0.011024</td>\n","      <td>-0.031379</td>\n","      <td>-0.025692</td>\n","      <td>-0.036663</td>\n","      <td>-0.096636</td>\n","      <td>-0.021654</td>\n","      <td>-0.075865</td>\n","      <td>-0.109888</td>\n","      <td>-0.044136</td>\n","      <td>-0.038741</td>\n","      <td>-0.022442</td>\n","      <td>-0.059613</td>\n","      <td>-0.019693</td>\n","      <td>-0.020697</td>\n","      <td>-0.022052</td>\n","      <td>-0.111494</td>\n","      <td>-0.017181</td>\n","      <td>-0.027542</td>\n","      <td>-0.012955</td>\n","      <td>-0.026796</td>\n","      <td>-0.004811</td>\n","      <td>-0.015215</td>\n","      <td>-0.060052</td>\n","      <td>-0.08109</td>\n","      <td>-0.055933</td>\n","      <td>-0.014027</td>\n","      <td>-0.025006</td>\n","      <td>-0.021385</td>\n","      <td>-0.045878</td>\n","      <td>-0.030725</td>\n","      <td>-0.072996</td>\n","      <td>-0.034038</td>\n","      <td>-0.092335</td>\n","      <td>-0.223068</td>\n","      <td>-0.012266</td>\n","      <td>-0.015591</td>\n","      <td>-0.104757</td>\n","      <td>-0.150343</td>\n","      <td>-0.038666</td>\n","      <td>-0.028676</td>\n","      <td>-0.053377</td>\n","      <td>-0.064273</td>\n","      <td>-0.144796</td>\n","      <td>-0.020415</td>\n","      <td>-0.054989</td>\n","      <td>-0.045752</td>\n","      <td>-0.082381</td>\n","      <td>-0.07941</td>\n","      <td>-0.196261</td>\n","      <td>-0.079263</td>\n","      <td>-0.039557</td>\n","      <td>-0.032647</td>\n","      <td>-0.03025</td>\n","      <td>-0.028978</td>\n","      <td>-0.061205</td>\n","      <td>-0.028166</td>\n","      <td>-0.044201</td>\n","      <td>-0.113237</td>\n","      <td>-0.162721</td>\n","      <td>-0.024656</td>\n","      <td>-0.043805</td>\n","      <td>5.617208</td>\n","      <td>-0.053649</td>\n","      <td>-0.048645</td>\n","      <td>-0.12157</td>\n","      <td>-0.018944</td>\n","      <td>-0.028978</td>\n","      <td>-0.038891</td>\n","      <td>-0.090241</td>\n","      <td>-0.042459</td>\n","      <td>-0.036663</td>\n","      <td>-0.060439</td>\n","      <td>-0.076173</td>\n","      <td>-0.166301</td>\n","      <td>-0.097909</td>\n","      <td>-0.072916</td>\n","      <td>-0.039336</td>\n","      <td>-0.047128</td>\n","      <td>-0.031655</td>\n","      <td>-0.055568</td>\n","      <td>-0.089324</td>\n","      <td>-0.078221</td>\n","      <td>-0.088331</td>\n","      <td>-0.029078</td>\n","      <td>-0.172891</td>\n","      <td>-0.151786</td>\n","      <td>-0.035539</td>\n","      <td>-0.068622</td>\n","      <td>-0.022442</td>\n","      <td>-0.045624</td>\n","      <td>-0.034038</td>\n","      <td>-0.058578</td>\n","      <td>-0.028371</td>\n","      <td>-0.009001</td>\n","      <td>-0.084317</td>\n","      <td>-0.0125</td>\n","      <td>-0.114203</td>\n","      <td>-0.07728</td>\n","      <td>-0.057727</td>\n","      <td>-0.106438</td>\n","      <td>-0.124334</td>\n","      <td>-0.073833</td>\n","      <td>-0.030058</td>\n","      <td>-0.020273</td>\n","      <td>-0.047128</td>\n","      <td>-0.215988</td>\n","      <td>-0.095994</td>\n","      <td>-0.03082</td>\n","      <td>-0.03941</td>\n","      <td>-0.138353</td>\n","      <td>-0.076938</td>\n","      <td>-0.026579</td>\n","      <td>-0.043406</td>\n","      <td>-0.040067</td>\n","      <td>-0.122254</td>\n","      <td>-0.055881</td>\n","      <td>-0.065887</td>\n","      <td>-0.043473</td>\n","      <td>-0.053812</td>\n","      <td>-0.035293</td>\n","      <td>-0.08456</td>\n","      <td>-0.0322</td>\n","      <td>-0.139306</td>\n","      <td>-0.02152</td>\n","      <td>-0.156196</td>\n","      <td>-0.012955</td>\n","      <td>-0.029572</td>\n","      <td>-0.016317</td>\n","      <td>-0.120561</td>\n","      <td>-0.147604</td>\n","      <td>-0.054936</td>\n","      <td>-0.064364</td>\n","      <td>-0.053541</td>\n","      <td>-0.02152</td>\n","      <td>-0.049416</td>\n","      <td>-0.036105</td>\n","      <td>-0.03963</td>\n","      <td>-0.14633</td>\n","      <td>-0.029767</td>\n","      <td>-0.048105</td>\n","      <td>-0.026579</td>\n","      <td>-0.031928</td>\n","      <td>-0.089619</td>\n","      <td>-0.092685</td>\n","      <td>-0.025006</td>\n","      <td>-0.11784</td>\n","      <td>-0.037832</td>\n","      <td>-0.093192</td>\n","      <td>-0.040284</td>\n","      <td>-0.029177</td>\n","      <td>-0.054403</td>\n","      <td>-0.103594</td>\n","      <td>-0.025579</td>\n","      <td>-0.0322</td>\n","      <td>-0.040067</td>\n","      <td>-0.0322</td>\n","      <td>-0.051037</td>\n","      <td>-0.141534</td>\n","      <td>-0.066985</td>\n","      <td>-0.022183</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.307414</td>\n","      <td>-0.198447</td>\n","      <td>-2.019196</td>\n","      <td>-0.326723</td>\n","      <td>-0.542916</td>\n","      <td>-0.264041</td>\n","      <td>-0.08432</td>\n","      <td>-0.01557</td>\n","      <td>-0.369989</td>\n","      <td>0.009728</td>\n","      <td>-0.028455</td>\n","      <td>-0.224846</td>\n","      <td>-0.636566</td>\n","      <td>-0.636051</td>\n","      <td>-0.067116</td>\n","      <td>-0.563342</td>\n","      <td>-0.513072</td>\n","      <td>-0.287578</td>\n","      <td>0.675031</td>\n","      <td>-0.08672</td>\n","      <td>-1.081998</td>\n","      <td>1.055574</td>\n","      <td>0.147337</td>\n","      <td>0.078146</td>\n","      <td>-0.654237</td>\n","      <td>-0.432702</td>\n","      <td>-0.33426</td>\n","      <td>6.272299</td>\n","      <td>-1.120286</td>\n","      <td>0.67101</td>\n","      <td>1.485850</td>\n","      <td>1.766646</td>\n","      <td>-0.594752</td>\n","      <td>-0.607508</td>\n","      <td>-0.541083</td>\n","      <td>-0.033</td>\n","      <td>-0.145087</td>\n","      <td>-0.04121</td>\n","      <td>-0.199623</td>\n","      <td>-0.036584</td>\n","      <td>-0.052331</td>\n","      <td>-0.070387</td>\n","      <td>-0.097759</td>\n","      <td>-0.057777</td>\n","      <td>-0.052387</td>\n","      <td>-0.094291</td>\n","      <td>-0.048765</td>\n","      <td>-0.053432</td>\n","      <td>-0.048346</td>\n","      <td>-0.084526</td>\n","      <td>-0.063451</td>\n","      <td>-0.059955</td>\n","      <td>-0.040139</td>\n","      <td>-0.009622</td>\n","      <td>-0.064318</td>\n","      <td>-0.085869</td>\n","      <td>-0.051829</td>\n","      <td>-0.072474</td>\n","      <td>-0.066504</td>\n","      <td>-0.041839</td>\n","      <td>-0.324079</td>\n","      <td>0.561241</td>\n","      <td>-0.033436</td>\n","      <td>-0.054723</td>\n","      <td>-0.069468</td>\n","      <td>-0.101491</td>\n","      <td>-0.038213</td>\n","      <td>-0.092748</td>\n","      <td>-0.155436</td>\n","      <td>-0.097396</td>\n","      <td>-0.077544</td>\n","      <td>-0.09176</td>\n","      <td>-0.072312</td>\n","      <td>-0.011024</td>\n","      <td>-0.031379</td>\n","      <td>-0.025692</td>\n","      <td>-0.036663</td>\n","      <td>-0.096636</td>\n","      <td>-0.021654</td>\n","      <td>-0.075865</td>\n","      <td>-0.109888</td>\n","      <td>-0.044136</td>\n","      <td>-0.038741</td>\n","      <td>-0.022442</td>\n","      <td>-0.059613</td>\n","      <td>-0.019693</td>\n","      <td>-0.020697</td>\n","      <td>-0.022052</td>\n","      <td>-0.111494</td>\n","      <td>-0.017181</td>\n","      <td>-0.027542</td>\n","      <td>-0.012955</td>\n","      <td>-0.026796</td>\n","      <td>-0.004811</td>\n","      <td>-0.015215</td>\n","      <td>-0.060052</td>\n","      <td>-0.08109</td>\n","      <td>-0.055933</td>\n","      <td>-0.014027</td>\n","      <td>-0.025006</td>\n","      <td>-0.021385</td>\n","      <td>-0.045878</td>\n","      <td>-0.030725</td>\n","      <td>-0.072996</td>\n","      <td>-0.034038</td>\n","      <td>-0.092335</td>\n","      <td>-0.223068</td>\n","      <td>-0.012266</td>\n","      <td>-0.015591</td>\n","      <td>-0.104757</td>\n","      <td>-0.150343</td>\n","      <td>-0.038666</td>\n","      <td>-0.028676</td>\n","      <td>-0.053377</td>\n","      <td>-0.064273</td>\n","      <td>-0.144796</td>\n","      <td>-0.020415</td>\n","      <td>-0.054989</td>\n","      <td>-0.045752</td>\n","      <td>-0.082381</td>\n","      <td>-0.07941</td>\n","      <td>-0.196261</td>\n","      <td>-0.079263</td>\n","      <td>-0.039557</td>\n","      <td>-0.032647</td>\n","      <td>-0.03025</td>\n","      <td>-0.028978</td>\n","      <td>-0.061205</td>\n","      <td>-0.028166</td>\n","      <td>-0.044201</td>\n","      <td>-0.113237</td>\n","      <td>-0.162721</td>\n","      <td>-0.024656</td>\n","      <td>-0.043805</td>\n","      <td>5.617208</td>\n","      <td>-0.053649</td>\n","      <td>-0.048645</td>\n","      <td>-0.12157</td>\n","      <td>-0.018944</td>\n","      <td>-0.028978</td>\n","      <td>-0.038891</td>\n","      <td>-0.090241</td>\n","      <td>-0.042459</td>\n","      <td>-0.036663</td>\n","      <td>-0.060439</td>\n","      <td>-0.076173</td>\n","      <td>-0.166301</td>\n","      <td>-0.097909</td>\n","      <td>-0.072916</td>\n","      <td>-0.039336</td>\n","      <td>-0.047128</td>\n","      <td>-0.031655</td>\n","      <td>-0.055568</td>\n","      <td>-0.089324</td>\n","      <td>-0.078221</td>\n","      <td>-0.088331</td>\n","      <td>-0.029078</td>\n","      <td>-0.172891</td>\n","      <td>-0.151786</td>\n","      <td>-0.035539</td>\n","      <td>-0.068622</td>\n","      <td>-0.022442</td>\n","      <td>-0.045624</td>\n","      <td>-0.034038</td>\n","      <td>-0.058578</td>\n","      <td>-0.028371</td>\n","      <td>-0.009001</td>\n","      <td>-0.084317</td>\n","      <td>-0.0125</td>\n","      <td>-0.114203</td>\n","      <td>-0.07728</td>\n","      <td>-0.057727</td>\n","      <td>-0.106438</td>\n","      <td>-0.124334</td>\n","      <td>-0.073833</td>\n","      <td>-0.030058</td>\n","      <td>-0.020273</td>\n","      <td>-0.047128</td>\n","      <td>-0.215988</td>\n","      <td>-0.095994</td>\n","      <td>-0.03082</td>\n","      <td>-0.03941</td>\n","      <td>-0.138353</td>\n","      <td>-0.076938</td>\n","      <td>-0.026579</td>\n","      <td>-0.043406</td>\n","      <td>-0.040067</td>\n","      <td>-0.122254</td>\n","      <td>-0.055881</td>\n","      <td>-0.065887</td>\n","      <td>-0.043473</td>\n","      <td>-0.053812</td>\n","      <td>-0.035293</td>\n","      <td>-0.08456</td>\n","      <td>-0.0322</td>\n","      <td>-0.139306</td>\n","      <td>-0.02152</td>\n","      <td>-0.156196</td>\n","      <td>-0.012955</td>\n","      <td>-0.029572</td>\n","      <td>-0.016317</td>\n","      <td>-0.120561</td>\n","      <td>-0.147604</td>\n","      <td>-0.054936</td>\n","      <td>-0.064364</td>\n","      <td>-0.053541</td>\n","      <td>-0.02152</td>\n","      <td>-0.049416</td>\n","      <td>-0.036105</td>\n","      <td>-0.03963</td>\n","      <td>-0.14633</td>\n","      <td>-0.029767</td>\n","      <td>-0.048105</td>\n","      <td>-0.026579</td>\n","      <td>-0.031928</td>\n","      <td>-0.089619</td>\n","      <td>-0.092685</td>\n","      <td>-0.025006</td>\n","      <td>-0.11784</td>\n","      <td>-0.037832</td>\n","      <td>-0.093192</td>\n","      <td>-0.040284</td>\n","      <td>-0.029177</td>\n","      <td>-0.054403</td>\n","      <td>-0.103594</td>\n","      <td>-0.025579</td>\n","      <td>-0.0322</td>\n","      <td>-0.040067</td>\n","      <td>-0.0322</td>\n","      <td>-0.051037</td>\n","      <td>-0.141534</td>\n","      <td>-0.066985</td>\n","      <td>-0.022183</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.288786</td>\n","      <td>-0.198447</td>\n","      <td>-0.326129</td>\n","      <td>1.762865</td>\n","      <td>0.762853</td>\n","      <td>0.903184</td>\n","      <td>-0.08432</td>\n","      <td>-0.01557</td>\n","      <td>3.482626</td>\n","      <td>0.261912</td>\n","      <td>1.168003</td>\n","      <td>0.223308</td>\n","      <td>-0.450812</td>\n","      <td>0.610507</td>\n","      <td>-0.094083</td>\n","      <td>-0.587589</td>\n","      <td>-0.400451</td>\n","      <td>-0.287578</td>\n","      <td>0.675031</td>\n","      <td>-0.08672</td>\n","      <td>0.924216</td>\n","      <td>1.055574</td>\n","      <td>0.147337</td>\n","      <td>0.078146</td>\n","      <td>-0.654237</td>\n","      <td>-0.432702</td>\n","      <td>-0.33426</td>\n","      <td>-0.159431</td>\n","      <td>-1.120286</td>\n","      <td>0.67101</td>\n","      <td>-0.673015</td>\n","      <td>-0.566044</td>\n","      <td>-0.594752</td>\n","      <td>-0.607508</td>\n","      <td>1.848147</td>\n","      <td>-0.033</td>\n","      <td>-0.145087</td>\n","      <td>-0.04121</td>\n","      <td>-0.199623</td>\n","      <td>-0.036584</td>\n","      <td>-0.052331</td>\n","      <td>-0.070387</td>\n","      <td>-0.097759</td>\n","      <td>-0.057777</td>\n","      <td>-0.052387</td>\n","      <td>-0.094291</td>\n","      <td>-0.048765</td>\n","      <td>-0.053432</td>\n","      <td>-0.048346</td>\n","      <td>-0.084526</td>\n","      <td>-0.063451</td>\n","      <td>-0.059955</td>\n","      <td>-0.040139</td>\n","      <td>-0.009622</td>\n","      <td>-0.064318</td>\n","      <td>-0.085869</td>\n","      <td>-0.051829</td>\n","      <td>-0.072474</td>\n","      <td>-0.066504</td>\n","      <td>-0.041839</td>\n","      <td>-0.324079</td>\n","      <td>0.561241</td>\n","      <td>-0.033436</td>\n","      <td>-0.054723</td>\n","      <td>-0.069468</td>\n","      <td>-0.101491</td>\n","      <td>-0.038213</td>\n","      <td>-0.092748</td>\n","      <td>-0.155436</td>\n","      <td>-0.097396</td>\n","      <td>-0.077544</td>\n","      <td>-0.09176</td>\n","      <td>-0.072312</td>\n","      <td>-0.011024</td>\n","      <td>-0.031379</td>\n","      <td>-0.025692</td>\n","      <td>-0.036663</td>\n","      <td>-0.096636</td>\n","      <td>-0.021654</td>\n","      <td>-0.075865</td>\n","      <td>-0.109888</td>\n","      <td>-0.044136</td>\n","      <td>-0.038741</td>\n","      <td>-0.022442</td>\n","      <td>-0.059613</td>\n","      <td>-0.019693</td>\n","      <td>-0.020697</td>\n","      <td>-0.022052</td>\n","      <td>-0.111494</td>\n","      <td>-0.017181</td>\n","      <td>-0.027542</td>\n","      <td>-0.012955</td>\n","      <td>-0.026796</td>\n","      <td>-0.004811</td>\n","      <td>-0.015215</td>\n","      <td>-0.060052</td>\n","      <td>-0.08109</td>\n","      <td>-0.055933</td>\n","      <td>-0.014027</td>\n","      <td>-0.025006</td>\n","      <td>-0.021385</td>\n","      <td>-0.045878</td>\n","      <td>-0.030725</td>\n","      <td>-0.072996</td>\n","      <td>-0.034038</td>\n","      <td>-0.092335</td>\n","      <td>-0.223068</td>\n","      <td>-0.012266</td>\n","      <td>-0.015591</td>\n","      <td>-0.104757</td>\n","      <td>-0.150343</td>\n","      <td>-0.038666</td>\n","      <td>-0.028676</td>\n","      <td>-0.053377</td>\n","      <td>-0.064273</td>\n","      <td>-0.144796</td>\n","      <td>-0.020415</td>\n","      <td>-0.054989</td>\n","      <td>-0.045752</td>\n","      <td>-0.082381</td>\n","      <td>-0.07941</td>\n","      <td>-0.196261</td>\n","      <td>-0.079263</td>\n","      <td>-0.039557</td>\n","      <td>-0.032647</td>\n","      <td>-0.03025</td>\n","      <td>-0.028978</td>\n","      <td>-0.061205</td>\n","      <td>-0.028166</td>\n","      <td>-0.044201</td>\n","      <td>-0.113237</td>\n","      <td>-0.162721</td>\n","      <td>-0.024656</td>\n","      <td>-0.043805</td>\n","      <td>-0.178024</td>\n","      <td>-0.053649</td>\n","      <td>-0.048645</td>\n","      <td>-0.12157</td>\n","      <td>-0.018944</td>\n","      <td>-0.028978</td>\n","      <td>-0.038891</td>\n","      <td>-0.090241</td>\n","      <td>-0.042459</td>\n","      <td>-0.036663</td>\n","      <td>-0.060439</td>\n","      <td>-0.076173</td>\n","      <td>-0.166301</td>\n","      <td>-0.097909</td>\n","      <td>-0.072916</td>\n","      <td>-0.039336</td>\n","      <td>-0.047128</td>\n","      <td>-0.031655</td>\n","      <td>-0.055568</td>\n","      <td>-0.089324</td>\n","      <td>-0.078221</td>\n","      <td>-0.088331</td>\n","      <td>-0.029078</td>\n","      <td>5.783990</td>\n","      <td>-0.151786</td>\n","      <td>-0.035539</td>\n","      <td>-0.068622</td>\n","      <td>-0.022442</td>\n","      <td>-0.045624</td>\n","      <td>-0.034038</td>\n","      <td>-0.058578</td>\n","      <td>-0.028371</td>\n","      <td>-0.009001</td>\n","      <td>-0.084317</td>\n","      <td>-0.0125</td>\n","      <td>-0.114203</td>\n","      <td>-0.07728</td>\n","      <td>-0.057727</td>\n","      <td>-0.106438</td>\n","      <td>-0.124334</td>\n","      <td>-0.073833</td>\n","      <td>-0.030058</td>\n","      <td>-0.020273</td>\n","      <td>-0.047128</td>\n","      <td>-0.215988</td>\n","      <td>-0.095994</td>\n","      <td>-0.03082</td>\n","      <td>-0.03941</td>\n","      <td>-0.138353</td>\n","      <td>-0.076938</td>\n","      <td>-0.026579</td>\n","      <td>-0.043406</td>\n","      <td>-0.040067</td>\n","      <td>-0.122254</td>\n","      <td>-0.055881</td>\n","      <td>-0.065887</td>\n","      <td>-0.043473</td>\n","      <td>-0.053812</td>\n","      <td>-0.035293</td>\n","      <td>-0.08456</td>\n","      <td>-0.0322</td>\n","      <td>-0.139306</td>\n","      <td>-0.02152</td>\n","      <td>-0.156196</td>\n","      <td>-0.012955</td>\n","      <td>-0.029572</td>\n","      <td>-0.016317</td>\n","      <td>-0.120561</td>\n","      <td>-0.147604</td>\n","      <td>-0.054936</td>\n","      <td>-0.064364</td>\n","      <td>-0.053541</td>\n","      <td>-0.02152</td>\n","      <td>-0.049416</td>\n","      <td>-0.036105</td>\n","      <td>-0.03963</td>\n","      <td>-0.14633</td>\n","      <td>-0.029767</td>\n","      <td>-0.048105</td>\n","      <td>-0.026579</td>\n","      <td>-0.031928</td>\n","      <td>-0.089619</td>\n","      <td>-0.092685</td>\n","      <td>-0.025006</td>\n","      <td>-0.11784</td>\n","      <td>-0.037832</td>\n","      <td>-0.093192</td>\n","      <td>-0.040284</td>\n","      <td>-0.029177</td>\n","      <td>-0.054403</td>\n","      <td>-0.103594</td>\n","      <td>-0.025579</td>\n","      <td>-0.0322</td>\n","      <td>-0.040067</td>\n","      <td>-0.0322</td>\n","      <td>-0.051037</td>\n","      <td>-0.141534</td>\n","      <td>-0.066985</td>\n","      <td>-0.022183</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0         1         2         3         4         5        6    \\\n","0 -0.298547 -0.198447 -0.941790 -0.301747 -0.542916 -0.264041 -0.08432   \n","1 -0.307414 -0.198447 -2.019196 -0.326723 -0.542916 -0.264041 -0.08432   \n","2 -0.288786 -0.198447 -0.326129  1.762865  0.762853  0.903184 -0.08432   \n","\n","       7         8         9         10        11        12        13   \\\n","0 -0.01557 -1.130397 -0.746823 -0.215401 -0.224846 -0.265058 -0.639590   \n","1 -0.01557 -0.369989  0.009728 -0.028455 -0.224846 -0.636566 -0.636051   \n","2 -0.01557  3.482626  0.261912  1.168003  0.223308 -0.450812  0.610507   \n","\n","        14        15        16        17        18       19        20   \\\n","0 -0.094083 -0.563342  0.613136 -0.287578  0.675031 -0.08672 -1.081998   \n","1 -0.067116 -0.563342 -0.513072 -0.287578  0.675031 -0.08672 -1.081998   \n","2 -0.094083 -0.587589 -0.400451 -0.287578  0.675031 -0.08672  0.924216   \n","\n","        21        22        23        24        25       26        27   \\\n","0 -0.947352  0.147337  0.078146 -0.654237 -0.432702 -0.33426 -0.159431   \n","1  1.055574  0.147337  0.078146 -0.654237 -0.432702 -0.33426  6.272299   \n","2  1.055574  0.147337  0.078146 -0.654237 -0.432702 -0.33426 -0.159431   \n","\n","        28       29        30        31        32        33        34     35   \\\n","0 -1.120286 -1.49029 -0.673015  1.766646 -0.594752 -0.607508 -0.541083 -0.033   \n","1 -1.120286  0.67101  1.485850  1.766646 -0.594752 -0.607508 -0.541083 -0.033   \n","2 -1.120286  0.67101 -0.673015 -0.566044 -0.594752 -0.607508  1.848147 -0.033   \n","\n","        36       37        38        39        40        41        42   \\\n","0 -0.145087 -0.04121 -0.199623 -0.036584 -0.052331 -0.070387 -0.097759   \n","1 -0.145087 -0.04121 -0.199623 -0.036584 -0.052331 -0.070387 -0.097759   \n","2 -0.145087 -0.04121 -0.199623 -0.036584 -0.052331 -0.070387 -0.097759   \n","\n","        43        44        45        46        47        48        49   \\\n","0 -0.057777 -0.052387 -0.094291 -0.048765 -0.053432 -0.048346 -0.084526   \n","1 -0.057777 -0.052387 -0.094291 -0.048765 -0.053432 -0.048346 -0.084526   \n","2 -0.057777 -0.052387 -0.094291 -0.048765 -0.053432 -0.048346 -0.084526   \n","\n","        50        51        52        53        54        55        56   \\\n","0 -0.063451 -0.059955 -0.040139 -0.009622 -0.064318 -0.085869 -0.051829   \n","1 -0.063451 -0.059955 -0.040139 -0.009622 -0.064318 -0.085869 -0.051829   \n","2 -0.063451 -0.059955 -0.040139 -0.009622 -0.064318 -0.085869 -0.051829   \n","\n","        57        58        59        60        61        62        63   \\\n","0 -0.072474 -0.066504 -0.041839 -0.324079  0.561241 -0.033436 -0.054723   \n","1 -0.072474 -0.066504 -0.041839 -0.324079  0.561241 -0.033436 -0.054723   \n","2 -0.072474 -0.066504 -0.041839 -0.324079  0.561241 -0.033436 -0.054723   \n","\n","        64        65        66        67        68        69        70   \\\n","0 -0.069468 -0.101491 -0.038213 -0.092748 -0.155436 -0.097396 -0.077544   \n","1 -0.069468 -0.101491 -0.038213 -0.092748 -0.155436 -0.097396 -0.077544   \n","2 -0.069468 -0.101491 -0.038213 -0.092748 -0.155436 -0.097396 -0.077544   \n","\n","       71        72        73        74        75        76        77   \\\n","0 -0.09176 -0.072312 -0.011024 -0.031379 -0.025692 -0.036663 -0.096636   \n","1 -0.09176 -0.072312 -0.011024 -0.031379 -0.025692 -0.036663 -0.096636   \n","2 -0.09176 -0.072312 -0.011024 -0.031379 -0.025692 -0.036663 -0.096636   \n","\n","        78        79        80        81        82        83        84   \\\n","0 -0.021654 -0.075865 -0.109888 -0.044136 -0.038741 -0.022442 -0.059613   \n","1 -0.021654 -0.075865 -0.109888 -0.044136 -0.038741 -0.022442 -0.059613   \n","2 -0.021654 -0.075865 -0.109888 -0.044136 -0.038741 -0.022442 -0.059613   \n","\n","        85        86        87        88        89        90        91   \\\n","0 -0.019693 -0.020697 -0.022052 -0.111494 -0.017181 -0.027542 -0.012955   \n","1 -0.019693 -0.020697 -0.022052 -0.111494 -0.017181 -0.027542 -0.012955   \n","2 -0.019693 -0.020697 -0.022052 -0.111494 -0.017181 -0.027542 -0.012955   \n","\n","        92        93        94        95       96        97        98   \\\n","0 -0.026796 -0.004811 -0.015215 -0.060052 -0.08109 -0.055933 -0.014027   \n","1 -0.026796 -0.004811 -0.015215 -0.060052 -0.08109 -0.055933 -0.014027   \n","2 -0.026796 -0.004811 -0.015215 -0.060052 -0.08109 -0.055933 -0.014027   \n","\n","        99        100       101       102       103       104       105  \\\n","0 -0.025006 -0.021385 -0.045878 -0.030725 -0.072996 -0.034038 -0.092335   \n","1 -0.025006 -0.021385 -0.045878 -0.030725 -0.072996 -0.034038 -0.092335   \n","2 -0.025006 -0.021385 -0.045878 -0.030725 -0.072996 -0.034038 -0.092335   \n","\n","        106       107       108       109       110       111       112  \\\n","0 -0.223068 -0.012266 -0.015591 -0.104757 -0.150343 -0.038666 -0.028676   \n","1 -0.223068 -0.012266 -0.015591 -0.104757 -0.150343 -0.038666 -0.028676   \n","2 -0.223068 -0.012266 -0.015591 -0.104757 -0.150343 -0.038666 -0.028676   \n","\n","        113       114       115       116       117       118       119  \\\n","0 -0.053377 -0.064273 -0.144796 -0.020415 -0.054989 -0.045752 -0.082381   \n","1 -0.053377 -0.064273 -0.144796 -0.020415 -0.054989 -0.045752 -0.082381   \n","2 -0.053377 -0.064273 -0.144796 -0.020415 -0.054989 -0.045752 -0.082381   \n","\n","       120       121       122       123       124      125       126  \\\n","0 -0.07941 -0.196261 -0.079263 -0.039557 -0.032647 -0.03025 -0.028978   \n","1 -0.07941 -0.196261 -0.079263 -0.039557 -0.032647 -0.03025 -0.028978   \n","2 -0.07941 -0.196261 -0.079263 -0.039557 -0.032647 -0.03025 -0.028978   \n","\n","        127       128       129       130       131       132       133  \\\n","0 -0.061205 -0.028166 -0.044201 -0.113237 -0.162721 -0.024656 -0.043805   \n","1 -0.061205 -0.028166 -0.044201 -0.113237 -0.162721 -0.024656 -0.043805   \n","2 -0.061205 -0.028166 -0.044201 -0.113237 -0.162721 -0.024656 -0.043805   \n","\n","        134       135       136      137       138       139       140  \\\n","0  5.617208 -0.053649 -0.048645 -0.12157 -0.018944 -0.028978 -0.038891   \n","1  5.617208 -0.053649 -0.048645 -0.12157 -0.018944 -0.028978 -0.038891   \n","2 -0.178024 -0.053649 -0.048645 -0.12157 -0.018944 -0.028978 -0.038891   \n","\n","        141       142       143       144       145       146       147  \\\n","0 -0.090241 -0.042459 -0.036663 -0.060439 -0.076173 -0.166301 -0.097909   \n","1 -0.090241 -0.042459 -0.036663 -0.060439 -0.076173 -0.166301 -0.097909   \n","2 -0.090241 -0.042459 -0.036663 -0.060439 -0.076173 -0.166301 -0.097909   \n","\n","        148       149       150       151       152       153       154  \\\n","0 -0.072916 -0.039336 -0.047128 -0.031655 -0.055568 -0.089324 -0.078221   \n","1 -0.072916 -0.039336 -0.047128 -0.031655 -0.055568 -0.089324 -0.078221   \n","2 -0.072916 -0.039336 -0.047128 -0.031655 -0.055568 -0.089324 -0.078221   \n","\n","        155       156       157       158       159       160       161  \\\n","0 -0.088331 -0.029078 -0.172891 -0.151786 -0.035539 -0.068622 -0.022442   \n","1 -0.088331 -0.029078 -0.172891 -0.151786 -0.035539 -0.068622 -0.022442   \n","2 -0.088331 -0.029078  5.783990 -0.151786 -0.035539 -0.068622 -0.022442   \n","\n","        162       163       164       165       166       167     168  \\\n","0 -0.045624 -0.034038 -0.058578 -0.028371 -0.009001 -0.084317 -0.0125   \n","1 -0.045624 -0.034038 -0.058578 -0.028371 -0.009001 -0.084317 -0.0125   \n","2 -0.045624 -0.034038 -0.058578 -0.028371 -0.009001 -0.084317 -0.0125   \n","\n","        169      170       171       172       173       174       175  \\\n","0 -0.114203 -0.07728 -0.057727 -0.106438 -0.124334 -0.073833 -0.030058   \n","1 -0.114203 -0.07728 -0.057727 -0.106438 -0.124334 -0.073833 -0.030058   \n","2 -0.114203 -0.07728 -0.057727 -0.106438 -0.124334 -0.073833 -0.030058   \n","\n","        176       177       178       179      180      181       182  \\\n","0 -0.020273 -0.047128 -0.215988 -0.095994 -0.03082 -0.03941 -0.138353   \n","1 -0.020273 -0.047128 -0.215988 -0.095994 -0.03082 -0.03941 -0.138353   \n","2 -0.020273 -0.047128 -0.215988 -0.095994 -0.03082 -0.03941 -0.138353   \n","\n","        183       184       185       186       187       188       189  \\\n","0 -0.076938 -0.026579 -0.043406 -0.040067 -0.122254 -0.055881 -0.065887   \n","1 -0.076938 -0.026579 -0.043406 -0.040067 -0.122254 -0.055881 -0.065887   \n","2 -0.076938 -0.026579 -0.043406 -0.040067 -0.122254 -0.055881 -0.065887   \n","\n","        190       191       192      193     194       195      196       197  \\\n","0 -0.043473 -0.053812 -0.035293 -0.08456 -0.0322 -0.139306 -0.02152 -0.156196   \n","1 -0.043473 -0.053812 -0.035293 -0.08456 -0.0322 -0.139306 -0.02152 -0.156196   \n","2 -0.043473 -0.053812 -0.035293 -0.08456 -0.0322 -0.139306 -0.02152 -0.156196   \n","\n","        198       199       200       201       202       203       204  \\\n","0 -0.012955 -0.029572 -0.016317 -0.120561 -0.147604 -0.054936 -0.064364   \n","1 -0.012955 -0.029572 -0.016317 -0.120561 -0.147604 -0.054936 -0.064364   \n","2 -0.012955 -0.029572 -0.016317 -0.120561 -0.147604 -0.054936 -0.064364   \n","\n","        205      206       207       208      209      210       211  \\\n","0 -0.053541 -0.02152 -0.049416 -0.036105 -0.03963 -0.14633 -0.029767   \n","1 -0.053541 -0.02152 -0.049416 -0.036105 -0.03963 -0.14633 -0.029767   \n","2 -0.053541 -0.02152 -0.049416 -0.036105 -0.03963 -0.14633 -0.029767   \n","\n","        212       213       214       215       216       217      218  \\\n","0 -0.048105 -0.026579 -0.031928 -0.089619 -0.092685 -0.025006 -0.11784   \n","1 -0.048105 -0.026579 -0.031928 -0.089619 -0.092685 -0.025006 -0.11784   \n","2 -0.048105 -0.026579 -0.031928 -0.089619 -0.092685 -0.025006 -0.11784   \n","\n","        219       220       221       222       223       224       225  \\\n","0 -0.037832 -0.093192 -0.040284 -0.029177 -0.054403 -0.103594 -0.025579   \n","1 -0.037832 -0.093192 -0.040284 -0.029177 -0.054403 -0.103594 -0.025579   \n","2 -0.037832 -0.093192 -0.040284 -0.029177 -0.054403 -0.103594 -0.025579   \n","\n","      226       227     228       229       230       231       232  \n","0 -0.0322 -0.040067 -0.0322 -0.051037 -0.141534 -0.066985 -0.022183  \n","1 -0.0322 -0.040067 -0.0322 -0.051037 -0.141534 -0.066985 -0.022183  \n","2 -0.0322 -0.040067 -0.0322 -0.051037 -0.141534 -0.066985 -0.022183  "]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yl-iF9u9LeH1","executionInfo":{"status":"ok","timestamp":1611299851972,"user_tz":-60,"elapsed":43898,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"6835e52b-d7a5-4158-e3fa-ba79aa516f87"},"source":["model = Sequential()\n","model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=L1(l1=0.0000), kernel_initializer=GlorotNormal(seed=seed_value)))\n","model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","history = model.fit(X_subtrain_transformed, y_subtrain, validation_data=(X_val_transformed, y_val) ,batch_size=256, epochs=100, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","print(\"Training Accuracy: {:.3f}\".format(model.evaluate(X_subtrain_transformed, y_subtrain, batch_size=256, verbose=0)[1]))\n","print(\"Validation Accuracy: {:.3f}\".format(model.evaluate(X_val_transformed, y_val, batch_size=256, verbose=0)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.6629 - binary_accuracy: 0.6449 - val_loss: 0.5172 - val_binary_accuracy: 0.7577\n","Epoch 2/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5015 - binary_accuracy: 0.7620 - val_loss: 0.5072 - val_binary_accuracy: 0.7609\n","Epoch 3/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4976 - binary_accuracy: 0.7636 - val_loss: 0.5024 - val_binary_accuracy: 0.7620\n","Epoch 4/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4918 - binary_accuracy: 0.7653 - val_loss: 0.4989 - val_binary_accuracy: 0.7642\n","Epoch 5/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4939 - binary_accuracy: 0.7654 - val_loss: 0.4956 - val_binary_accuracy: 0.7659\n","Epoch 6/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.4911 - binary_accuracy: 0.7669 - val_loss: 0.4939 - val_binary_accuracy: 0.7663\n","Epoch 7/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.4898 - binary_accuracy: 0.7665 - val_loss: 0.4923 - val_binary_accuracy: 0.7658\n","Epoch 8/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4918 - binary_accuracy: 0.7663 - val_loss: 0.4904 - val_binary_accuracy: 0.7669\n","Epoch 9/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4848 - binary_accuracy: 0.7689 - val_loss: 0.4890 - val_binary_accuracy: 0.7671\n","Epoch 10/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4827 - binary_accuracy: 0.7709 - val_loss: 0.4875 - val_binary_accuracy: 0.7678\n","Epoch 11/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.4808 - binary_accuracy: 0.7700 - val_loss: 0.4869 - val_binary_accuracy: 0.7677\n","Epoch 12/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.4817 - binary_accuracy: 0.7697 - val_loss: 0.4864 - val_binary_accuracy: 0.7679\n","Epoch 13/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4855 - binary_accuracy: 0.7701 - val_loss: 0.4859 - val_binary_accuracy: 0.7686\n","Epoch 14/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4800 - binary_accuracy: 0.7695 - val_loss: 0.4858 - val_binary_accuracy: 0.7687\n","Epoch 15/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.4892 - binary_accuracy: 0.7695 - val_loss: 0.4855 - val_binary_accuracy: 0.7679\n","Epoch 16/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.4828 - binary_accuracy: 0.7689 - val_loss: 0.4849 - val_binary_accuracy: 0.7676\n","Epoch 17/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4784 - binary_accuracy: 0.7725 - val_loss: 0.4847 - val_binary_accuracy: 0.7690\n","Epoch 18/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4852 - binary_accuracy: 0.7683 - val_loss: 0.4841 - val_binary_accuracy: 0.7677\n","Epoch 19/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4806 - binary_accuracy: 0.7698 - val_loss: 0.4848 - val_binary_accuracy: 0.7685\n","Epoch 20/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4808 - binary_accuracy: 0.7708 - val_loss: 0.4840 - val_binary_accuracy: 0.7680\n","Epoch 21/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4804 - binary_accuracy: 0.7697 - val_loss: 0.4835 - val_binary_accuracy: 0.7693\n","Epoch 22/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4797 - binary_accuracy: 0.7714 - val_loss: 0.4838 - val_binary_accuracy: 0.7689\n","Epoch 23/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4796 - binary_accuracy: 0.7706 - val_loss: 0.4835 - val_binary_accuracy: 0.7686\n","Epoch 24/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4816 - binary_accuracy: 0.7695 - val_loss: 0.4838 - val_binary_accuracy: 0.7683\n","Epoch 25/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4806 - binary_accuracy: 0.7723 - val_loss: 0.4838 - val_binary_accuracy: 0.7683\n","Epoch 26/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4824 - binary_accuracy: 0.7702 - val_loss: 0.4836 - val_binary_accuracy: 0.7695\n","Epoch 27/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4816 - binary_accuracy: 0.7712 - val_loss: 0.4831 - val_binary_accuracy: 0.7687\n","Epoch 28/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4794 - binary_accuracy: 0.7701 - val_loss: 0.4828 - val_binary_accuracy: 0.7691\n","Epoch 29/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4803 - binary_accuracy: 0.7692 - val_loss: 0.4829 - val_binary_accuracy: 0.7684\n","Epoch 30/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4810 - binary_accuracy: 0.7696 - val_loss: 0.4826 - val_binary_accuracy: 0.7687\n","Epoch 31/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4828 - binary_accuracy: 0.7699 - val_loss: 0.4822 - val_binary_accuracy: 0.7689\n","Restoring model weights from the end of the best epoch.\n","Epoch 00031: early stopping\n","Training Accuracy: 0.771\n","Validation Accuracy: 0.769\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ToMIjd2_QuUK"},"source":["##### 2. Standardization: Only Numerical Attributes"]},{"cell_type":"code","metadata":{"id":"EuT5OuapQ10U"},"source":["# Specify pre-processing pipeline for the different types of attributes\n","preprocessing = ColumnTransformer(\n","    [\n","      (\"numeric\", StandardScaler(), numeric_features)\n","    ],\n","    remainder=\"passthrough\", verbose=True, n_jobs=-1\n",").fit(X_subtrain)\n","X_subtrain_transformed = preprocessing.transform(X_subtrain)\n","X_val_transformed = preprocessing.transform(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"Y9Sc9vnVQ10V","executionInfo":{"status":"ok","timestamp":1611299876571,"user_tz":-60,"elapsed":753,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"a7afb87a-c802-4822-ddb7-878b31aa3776"},"source":["# Validation\n","tmp = pd.DataFrame(X_subtrain_transformed)\n","print(tmp.shape)\n","tmp.head(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(172824, 233)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","      <th>110</th>\n","      <th>111</th>\n","      <th>112</th>\n","      <th>113</th>\n","      <th>114</th>\n","      <th>115</th>\n","      <th>116</th>\n","      <th>117</th>\n","      <th>118</th>\n","      <th>119</th>\n","      <th>120</th>\n","      <th>121</th>\n","      <th>122</th>\n","      <th>123</th>\n","      <th>124</th>\n","      <th>125</th>\n","      <th>126</th>\n","      <th>127</th>\n","      <th>128</th>\n","      <th>129</th>\n","      <th>130</th>\n","      <th>131</th>\n","      <th>132</th>\n","      <th>133</th>\n","      <th>134</th>\n","      <th>135</th>\n","      <th>136</th>\n","      <th>137</th>\n","      <th>138</th>\n","      <th>139</th>\n","      <th>140</th>\n","      <th>141</th>\n","      <th>142</th>\n","      <th>143</th>\n","      <th>144</th>\n","      <th>145</th>\n","      <th>146</th>\n","      <th>147</th>\n","      <th>148</th>\n","      <th>149</th>\n","      <th>150</th>\n","      <th>151</th>\n","      <th>152</th>\n","      <th>153</th>\n","      <th>154</th>\n","      <th>155</th>\n","      <th>156</th>\n","      <th>157</th>\n","      <th>158</th>\n","      <th>159</th>\n","      <th>160</th>\n","      <th>161</th>\n","      <th>162</th>\n","      <th>163</th>\n","      <th>164</th>\n","      <th>165</th>\n","      <th>166</th>\n","      <th>167</th>\n","      <th>168</th>\n","      <th>169</th>\n","      <th>170</th>\n","      <th>171</th>\n","      <th>172</th>\n","      <th>173</th>\n","      <th>174</th>\n","      <th>175</th>\n","      <th>176</th>\n","      <th>177</th>\n","      <th>178</th>\n","      <th>179</th>\n","      <th>180</th>\n","      <th>181</th>\n","      <th>182</th>\n","      <th>183</th>\n","      <th>184</th>\n","      <th>185</th>\n","      <th>186</th>\n","      <th>187</th>\n","      <th>188</th>\n","      <th>189</th>\n","      <th>190</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","      <th>200</th>\n","      <th>201</th>\n","      <th>202</th>\n","      <th>203</th>\n","      <th>204</th>\n","      <th>205</th>\n","      <th>206</th>\n","      <th>207</th>\n","      <th>208</th>\n","      <th>209</th>\n","      <th>210</th>\n","      <th>211</th>\n","      <th>212</th>\n","      <th>213</th>\n","      <th>214</th>\n","      <th>215</th>\n","      <th>216</th>\n","      <th>217</th>\n","      <th>218</th>\n","      <th>219</th>\n","      <th>220</th>\n","      <th>221</th>\n","      <th>222</th>\n","      <th>223</th>\n","      <th>224</th>\n","      <th>225</th>\n","      <th>226</th>\n","      <th>227</th>\n","      <th>228</th>\n","      <th>229</th>\n","      <th>230</th>\n","      <th>231</th>\n","      <th>232</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.298547</td>\n","      <td>-0.198447</td>\n","      <td>-0.941790</td>\n","      <td>-0.301747</td>\n","      <td>-0.542916</td>\n","      <td>-0.264041</td>\n","      <td>-0.08432</td>\n","      <td>-0.01557</td>\n","      <td>-1.130397</td>\n","      <td>-0.746823</td>\n","      <td>-0.215401</td>\n","      <td>-0.224846</td>\n","      <td>-0.265058</td>\n","      <td>-0.639590</td>\n","      <td>-0.094083</td>\n","      <td>-0.563342</td>\n","      <td>0.613136</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.307414</td>\n","      <td>-0.198447</td>\n","      <td>-2.019196</td>\n","      <td>-0.326723</td>\n","      <td>-0.542916</td>\n","      <td>-0.264041</td>\n","      <td>-0.08432</td>\n","      <td>-0.01557</td>\n","      <td>-0.369989</td>\n","      <td>0.009728</td>\n","      <td>-0.028455</td>\n","      <td>-0.224846</td>\n","      <td>-0.636566</td>\n","      <td>-0.636051</td>\n","      <td>-0.067116</td>\n","      <td>-0.563342</td>\n","      <td>-0.513072</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.288786</td>\n","      <td>-0.198447</td>\n","      <td>-0.326129</td>\n","      <td>1.762865</td>\n","      <td>0.762853</td>\n","      <td>0.903184</td>\n","      <td>-0.08432</td>\n","      <td>-0.01557</td>\n","      <td>3.482626</td>\n","      <td>0.261912</td>\n","      <td>1.168003</td>\n","      <td>0.223308</td>\n","      <td>-0.450812</td>\n","      <td>0.610507</td>\n","      <td>-0.094083</td>\n","      <td>-0.587589</td>\n","      <td>-0.400451</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0         1         2         3         4         5        6    \\\n","0 -0.298547 -0.198447 -0.941790 -0.301747 -0.542916 -0.264041 -0.08432   \n","1 -0.307414 -0.198447 -2.019196 -0.326723 -0.542916 -0.264041 -0.08432   \n","2 -0.288786 -0.198447 -0.326129  1.762865  0.762853  0.903184 -0.08432   \n","\n","       7         8         9         10        11        12        13   \\\n","0 -0.01557 -1.130397 -0.746823 -0.215401 -0.224846 -0.265058 -0.639590   \n","1 -0.01557 -0.369989  0.009728 -0.028455 -0.224846 -0.636566 -0.636051   \n","2 -0.01557  3.482626  0.261912  1.168003  0.223308 -0.450812  0.610507   \n","\n","        14        15        16   17   18   19   20   21   22   23   24   25   \\\n","0 -0.094083 -0.563342  0.613136  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0   \n","1 -0.067116 -0.563342 -0.513072  0.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0   \n","2 -0.094083 -0.587589 -0.400451  0.0  1.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0   \n","\n","   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   \\\n","0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   56   57   58   59   60   61   62   63   64   65   66   67   68   69   70   \\\n","0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   71   72   73   74   75   76   77   78   79   80   81   82   83   84   85   \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   86   87   88   89   90   91   92   93   94   95   96   97   98   99   100  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  \\\n","0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n","\n","   161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   221  222  223  224  225  226  227  228  229  230  231  232  \n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  "]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84WnQJAnQ10X","executionInfo":{"status":"ok","timestamp":1611299913341,"user_tz":-60,"elapsed":18947,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"3ddb2f00-524f-4a1e-8f42-5ea179a7c6e7"},"source":["model = Sequential()\n","model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=L1(l1=0.0000), kernel_initializer=GlorotNormal(seed=seed_value)))\n","model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","history = model.fit(X_subtrain_transformed, y_subtrain, validation_data=(X_val_transformed, y_val) ,batch_size=256, epochs=100, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","print(\"Training Accuracy: {:.3f}\".format(model.evaluate(X_subtrain_transformed, y_subtrain, batch_size=256, verbose=0)[1]))\n","print(\"Validation Accuracy: {:.3f}\".format(model.evaluate(X_val_transformed, y_val, batch_size=256, verbose=0)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.6023 - binary_accuracy: 0.6623 - val_loss: 0.5160 - val_binary_accuracy: 0.7394\n","Epoch 2/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5066 - binary_accuracy: 0.7508 - val_loss: 0.4949 - val_binary_accuracy: 0.7577\n","Epoch 3/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4918 - binary_accuracy: 0.7611 - val_loss: 0.4862 - val_binary_accuracy: 0.7621\n","Epoch 4/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4842 - binary_accuracy: 0.7642 - val_loss: 0.4825 - val_binary_accuracy: 0.7634\n","Epoch 5/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4826 - binary_accuracy: 0.7653 - val_loss: 0.4802 - val_binary_accuracy: 0.7658\n","Epoch 6/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4805 - binary_accuracy: 0.7667 - val_loss: 0.4789 - val_binary_accuracy: 0.7678\n","Epoch 7/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4783 - binary_accuracy: 0.7659 - val_loss: 0.4782 - val_binary_accuracy: 0.7669\n","Epoch 8/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4807 - binary_accuracy: 0.7660 - val_loss: 0.4777 - val_binary_accuracy: 0.7675\n","Epoch 9/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4766 - binary_accuracy: 0.7690 - val_loss: 0.4774 - val_binary_accuracy: 0.7691\n","Epoch 10/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4762 - binary_accuracy: 0.7709 - val_loss: 0.4770 - val_binary_accuracy: 0.7691\n","Epoch 11/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4754 - binary_accuracy: 0.7703 - val_loss: 0.4771 - val_binary_accuracy: 0.7685\n","Epoch 12/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4742 - binary_accuracy: 0.7700 - val_loss: 0.4771 - val_binary_accuracy: 0.7682\n","Epoch 13/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4776 - binary_accuracy: 0.7696 - val_loss: 0.4771 - val_binary_accuracy: 0.7681\n","Epoch 14/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4749 - binary_accuracy: 0.7692 - val_loss: 0.4768 - val_binary_accuracy: 0.7689\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","Training Accuracy: 0.769\n","Validation Accuracy: 0.769\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iEKyBBZ-RKY8"},"source":["Result: Standardizing Attributes (both Numerical + Binary or Only Numerical) helped to increase performance of the model"]},{"cell_type":"markdown","metadata":{"id":"EmzaXt6qRVbs"},"source":["##### 3. Normalization: Numerical + Binary Features"]},{"cell_type":"code","metadata":{"id":"oo3FtCjnRhUd"},"source":["# Specify pre-processing pipeline for the different types of attributes\n","preprocessing = ColumnTransformer(\n","    [\n","      (\"numeric\", Pipeline([\n","                            (\"normalization\", StandardScaler()),\n","                            (\"standarization\", MinMaxScaler(feature_range=(-1,1)))\n","                          ]), numeric_features),\n","      (\"binary\", MinMaxScaler(feature_range=(-1,1)), binary_features)\n","    ],\n","    remainder=\"drop\", verbose=True, n_jobs=-1\n",").fit(X_subtrain)\n","X_subtrain_transformed = preprocessing.transform(X_subtrain)\n","X_val_transformed = preprocessing.transform(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"K-MQUrS-RhUe","executionInfo":{"status":"ok","timestamp":1611299995063,"user_tz":-60,"elapsed":818,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"970f28e5-e1c6-4151-8655-fb19c5f04487"},"source":["# Validation\n","tmp = pd.DataFrame(X_subtrain_transformed)\n","print(tmp.shape)\n","tmp.head(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(172824, 233)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","      <th>110</th>\n","      <th>111</th>\n","      <th>112</th>\n","      <th>113</th>\n","      <th>114</th>\n","      <th>115</th>\n","      <th>116</th>\n","      <th>117</th>\n","      <th>118</th>\n","      <th>119</th>\n","      <th>120</th>\n","      <th>121</th>\n","      <th>122</th>\n","      <th>123</th>\n","      <th>124</th>\n","      <th>125</th>\n","      <th>126</th>\n","      <th>127</th>\n","      <th>128</th>\n","      <th>129</th>\n","      <th>130</th>\n","      <th>131</th>\n","      <th>132</th>\n","      <th>133</th>\n","      <th>134</th>\n","      <th>135</th>\n","      <th>136</th>\n","      <th>137</th>\n","      <th>138</th>\n","      <th>139</th>\n","      <th>140</th>\n","      <th>141</th>\n","      <th>142</th>\n","      <th>143</th>\n","      <th>144</th>\n","      <th>145</th>\n","      <th>146</th>\n","      <th>147</th>\n","      <th>148</th>\n","      <th>149</th>\n","      <th>150</th>\n","      <th>151</th>\n","      <th>152</th>\n","      <th>153</th>\n","      <th>154</th>\n","      <th>155</th>\n","      <th>156</th>\n","      <th>157</th>\n","      <th>158</th>\n","      <th>159</th>\n","      <th>160</th>\n","      <th>161</th>\n","      <th>162</th>\n","      <th>163</th>\n","      <th>164</th>\n","      <th>165</th>\n","      <th>166</th>\n","      <th>167</th>\n","      <th>168</th>\n","      <th>169</th>\n","      <th>170</th>\n","      <th>171</th>\n","      <th>172</th>\n","      <th>173</th>\n","      <th>174</th>\n","      <th>175</th>\n","      <th>176</th>\n","      <th>177</th>\n","      <th>178</th>\n","      <th>179</th>\n","      <th>180</th>\n","      <th>181</th>\n","      <th>182</th>\n","      <th>183</th>\n","      <th>184</th>\n","      <th>185</th>\n","      <th>186</th>\n","      <th>187</th>\n","      <th>188</th>\n","      <th>189</th>\n","      <th>190</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","      <th>200</th>\n","      <th>201</th>\n","      <th>202</th>\n","      <th>203</th>\n","      <th>204</th>\n","      <th>205</th>\n","      <th>206</th>\n","      <th>207</th>\n","      <th>208</th>\n","      <th>209</th>\n","      <th>210</th>\n","      <th>211</th>\n","      <th>212</th>\n","      <th>213</th>\n","      <th>214</th>\n","      <th>215</th>\n","      <th>216</th>\n","      <th>217</th>\n","      <th>218</th>\n","      <th>219</th>\n","      <th>220</th>\n","      <th>221</th>\n","      <th>222</th>\n","      <th>223</th>\n","      <th>224</th>\n","      <th>225</th>\n","      <th>226</th>\n","      <th>227</th>\n","      <th>228</th>\n","      <th>229</th>\n","      <th>230</th>\n","      <th>231</th>\n","      <th>232</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.999650</td>\n","      <td>-1.0</td>\n","      <td>-0.567010</td>\n","      <td>-0.995506</td>\n","      <td>-1.00000</td>\n","      <td>-1.000000</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-0.988894</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>-0.947137</td>\n","      <td>-0.929240</td>\n","      <td>-0.999815</td>\n","      <td>-0.99200</td>\n","      <td>-0.777778</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.999862</td>\n","      <td>-1.0</td>\n","      <td>-0.855670</td>\n","      <td>-0.997432</td>\n","      <td>-1.00000</td>\n","      <td>-1.000000</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-0.918719</td>\n","      <td>-0.960265</td>\n","      <td>-0.994299</td>\n","      <td>-1.000000</td>\n","      <td>-0.964758</td>\n","      <td>-0.928947</td>\n","      <td>-0.998155</td>\n","      <td>-0.99200</td>\n","      <td>-0.916667</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.999416</td>\n","      <td>-1.0</td>\n","      <td>-0.402062</td>\n","      <td>-0.836276</td>\n","      <td>-0.87027</td>\n","      <td>-0.935484</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-0.563174</td>\n","      <td>-0.947020</td>\n","      <td>-0.957811</td>\n","      <td>-0.963636</td>\n","      <td>-0.955947</td>\n","      <td>-0.825965</td>\n","      <td>-0.999815</td>\n","      <td>-0.99712</td>\n","      <td>-0.902778</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","      <td>-1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0    1         2         3        4         5    6    7         8    \\\n","0 -0.999650 -1.0 -0.567010 -0.995506 -1.00000 -1.000000 -1.0 -1.0 -0.988894   \n","1 -0.999862 -1.0 -0.855670 -0.997432 -1.00000 -1.000000 -1.0 -1.0 -0.918719   \n","2 -0.999416 -1.0 -0.402062 -0.836276 -0.87027 -0.935484 -1.0 -1.0 -0.563174   \n","\n","        9         10        11        12        13        14       15   \\\n","0 -1.000000 -1.000000 -1.000000 -0.947137 -0.929240 -0.999815 -0.99200   \n","1 -0.960265 -0.994299 -1.000000 -0.964758 -0.928947 -0.998155 -0.99200   \n","2 -0.947020 -0.957811 -0.963636 -0.955947 -0.825965 -0.999815 -0.99712   \n","\n","        16   17   18   19   20   21   22   23   24   25   26   27   28   29   \\\n","0 -0.777778 -1.0  1.0 -1.0 -1.0 -1.0  1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -0.916667 -1.0  1.0 -1.0 -1.0  1.0  1.0  1.0 -1.0 -1.0 -1.0  1.0 -1.0  1.0   \n","2 -0.902778 -1.0  1.0 -1.0  1.0  1.0  1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0   \n","\n","   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   \\\n","0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1  1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   45   46   47   48   49   50   51   52   53   54   55   56   57   58   59   \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   60   61   62   63   64   65   66   67   68   69   70   71   72   73   74   \\\n","0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   90   91   92   93   94   95   96   97   98   99   100  101  102  103  104  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  \\\n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n","\n","   225  226  227  228  229  230  231  232  \n","0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n","1 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n","2 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  "]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6U8JJm8wRhUe","executionInfo":{"status":"ok","timestamp":1611300036976,"user_tz":-60,"elapsed":30097,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"34eca716-9280-4e6a-ad13-ea72c0d29c6d"},"source":["model = Sequential()\n","model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=L1(l1=0.0000), kernel_initializer=GlorotNormal(seed=seed_value)))\n","model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","history = model.fit(X_subtrain_transformed, y_subtrain, validation_data=(X_val_transformed, y_val) ,batch_size=256, epochs=100, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","print(\"Training Accuracy: {:.3f}\".format(model.evaluate(X_subtrain_transformed, y_subtrain, batch_size=256, verbose=0)[1]))\n","print(\"Validation Accuracy: {:.3f}\".format(model.evaluate(X_val_transformed, y_val, batch_size=256, verbose=0)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.6423 - binary_accuracy: 0.6496 - val_loss: 0.5990 - val_binary_accuracy: 0.6765\n","Epoch 2/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5801 - binary_accuracy: 0.6990 - val_loss: 0.5730 - val_binary_accuracy: 0.7116\n","Epoch 3/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5658 - binary_accuracy: 0.7084 - val_loss: 0.5886 - val_binary_accuracy: 0.6878\n","Epoch 4/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5575 - binary_accuracy: 0.7141 - val_loss: 0.5552 - val_binary_accuracy: 0.7143\n","Epoch 5/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5520 - binary_accuracy: 0.7206 - val_loss: 0.5480 - val_binary_accuracy: 0.7241\n","Epoch 6/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5481 - binary_accuracy: 0.7225 - val_loss: 0.5471 - val_binary_accuracy: 0.7271\n","Epoch 7/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5453 - binary_accuracy: 0.7249 - val_loss: 0.5801 - val_binary_accuracy: 0.6964\n","Epoch 8/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5453 - binary_accuracy: 0.7240 - val_loss: 0.5435 - val_binary_accuracy: 0.7229\n","Epoch 9/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5386 - binary_accuracy: 0.7280 - val_loss: 0.5462 - val_binary_accuracy: 0.7262\n","Epoch 10/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5373 - binary_accuracy: 0.7297 - val_loss: 0.5422 - val_binary_accuracy: 0.7284\n","Epoch 11/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5366 - binary_accuracy: 0.7292 - val_loss: 0.5378 - val_binary_accuracy: 0.7277\n","Epoch 12/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5341 - binary_accuracy: 0.7306 - val_loss: 0.5756 - val_binary_accuracy: 0.7005\n","Epoch 13/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5337 - binary_accuracy: 0.7322 - val_loss: 0.5368 - val_binary_accuracy: 0.7284\n","Epoch 14/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5321 - binary_accuracy: 0.7319 - val_loss: 0.5312 - val_binary_accuracy: 0.7338\n","Epoch 15/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5329 - binary_accuracy: 0.7313 - val_loss: 0.5385 - val_binary_accuracy: 0.7271\n","Epoch 16/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5306 - binary_accuracy: 0.7320 - val_loss: 0.5299 - val_binary_accuracy: 0.7342\n","Epoch 17/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5262 - binary_accuracy: 0.7369 - val_loss: 0.5288 - val_binary_accuracy: 0.7350\n","Epoch 18/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5303 - binary_accuracy: 0.7325 - val_loss: 0.5303 - val_binary_accuracy: 0.7325\n","Epoch 19/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5286 - binary_accuracy: 0.7337 - val_loss: 0.5435 - val_binary_accuracy: 0.7273\n","Epoch 20/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5262 - binary_accuracy: 0.7351 - val_loss: 0.5456 - val_binary_accuracy: 0.7265\n","Epoch 21/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5269 - binary_accuracy: 0.7344 - val_loss: 0.5309 - val_binary_accuracy: 0.7344\n","Epoch 22/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.5246 - binary_accuracy: 0.7366 - val_loss: 0.5628 - val_binary_accuracy: 0.7093\n","Restoring model weights from the end of the best epoch.\n","Epoch 00022: early stopping\n","Training Accuracy: 0.735\n","Validation Accuracy: 0.735\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v_P27C2CVsVf"},"source":["Result: Normalization is not helpful"]},{"cell_type":"markdown","metadata":{"id":"qx0cm4AxVzO5"},"source":["##### 4. Power Transformation of Numerical Features"]},{"cell_type":"code","metadata":{"id":"KiIywtmSWC5I"},"source":["# Specify pre-processing pipeline for the different types of attributes\n","preprocessing = ColumnTransformer(\n","    [\n","      (\"numeric\", PowerTransformer(method=\"yeo-johnson\", standardize=True), numeric_features)\n","    ],\n","    remainder=\"passthrough\", verbose=True, n_jobs=-1\n",").fit(X_subtrain)\n","X_subtrain_transformed = preprocessing.transform(X_subtrain)\n","X_val_transformed = preprocessing.transform(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"BamqpwPAWC5K","executionInfo":{"status":"ok","timestamp":1611300267378,"user_tz":-60,"elapsed":1118,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"6337374f-5c9d-46ff-b6ab-a7a3bf4cb548"},"source":["# Validation\n","tmp = pd.DataFrame(X_subtrain_transformed)\n","print(tmp.shape)\n","tmp.head(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(172824, 233)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","      <th>110</th>\n","      <th>111</th>\n","      <th>112</th>\n","      <th>113</th>\n","      <th>114</th>\n","      <th>115</th>\n","      <th>116</th>\n","      <th>117</th>\n","      <th>118</th>\n","      <th>119</th>\n","      <th>120</th>\n","      <th>121</th>\n","      <th>122</th>\n","      <th>123</th>\n","      <th>124</th>\n","      <th>125</th>\n","      <th>126</th>\n","      <th>127</th>\n","      <th>128</th>\n","      <th>129</th>\n","      <th>130</th>\n","      <th>131</th>\n","      <th>132</th>\n","      <th>133</th>\n","      <th>134</th>\n","      <th>135</th>\n","      <th>136</th>\n","      <th>137</th>\n","      <th>138</th>\n","      <th>139</th>\n","      <th>140</th>\n","      <th>141</th>\n","      <th>142</th>\n","      <th>143</th>\n","      <th>144</th>\n","      <th>145</th>\n","      <th>146</th>\n","      <th>147</th>\n","      <th>148</th>\n","      <th>149</th>\n","      <th>150</th>\n","      <th>151</th>\n","      <th>152</th>\n","      <th>153</th>\n","      <th>154</th>\n","      <th>155</th>\n","      <th>156</th>\n","      <th>157</th>\n","      <th>158</th>\n","      <th>159</th>\n","      <th>160</th>\n","      <th>161</th>\n","      <th>162</th>\n","      <th>163</th>\n","      <th>164</th>\n","      <th>165</th>\n","      <th>166</th>\n","      <th>167</th>\n","      <th>168</th>\n","      <th>169</th>\n","      <th>170</th>\n","      <th>171</th>\n","      <th>172</th>\n","      <th>173</th>\n","      <th>174</th>\n","      <th>175</th>\n","      <th>176</th>\n","      <th>177</th>\n","      <th>178</th>\n","      <th>179</th>\n","      <th>180</th>\n","      <th>181</th>\n","      <th>182</th>\n","      <th>183</th>\n","      <th>184</th>\n","      <th>185</th>\n","      <th>186</th>\n","      <th>187</th>\n","      <th>188</th>\n","      <th>189</th>\n","      <th>190</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","      <th>200</th>\n","      <th>201</th>\n","      <th>202</th>\n","      <th>203</th>\n","      <th>204</th>\n","      <th>205</th>\n","      <th>206</th>\n","      <th>207</th>\n","      <th>208</th>\n","      <th>209</th>\n","      <th>210</th>\n","      <th>211</th>\n","      <th>212</th>\n","      <th>213</th>\n","      <th>214</th>\n","      <th>215</th>\n","      <th>216</th>\n","      <th>217</th>\n","      <th>218</th>\n","      <th>219</th>\n","      <th>220</th>\n","      <th>221</th>\n","      <th>222</th>\n","      <th>223</th>\n","      <th>224</th>\n","      <th>225</th>\n","      <th>226</th>\n","      <th>227</th>\n","      <th>228</th>\n","      <th>229</th>\n","      <th>230</th>\n","      <th>231</th>\n","      <th>232</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.083699</td>\n","      <td>-0.239021</td>\n","      <td>-0.986135</td>\n","      <td>-0.190185</td>\n","      <td>-0.933752</td>\n","      <td>-0.366213</td>\n","      <td>-0.13909</td>\n","      <td>-0.025692</td>\n","      <td>-2.667854</td>\n","      <td>-1.554894</td>\n","      <td>-0.878420</td>\n","      <td>-0.612750</td>\n","      <td>-0.063636</td>\n","      <td>-0.619896</td>\n","      <td>-1.382878</td>\n","      <td>-0.74978</td>\n","      <td>0.866256</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.621048</td>\n","      <td>-0.239021</td>\n","      <td>-2.927746</td>\n","      <td>-0.514813</td>\n","      <td>-0.933752</td>\n","      <td>-0.366213</td>\n","      <td>-0.13909</td>\n","      <td>-0.025692</td>\n","      <td>-0.148796</td>\n","      <td>0.467339</td>\n","      <td>1.228547</td>\n","      <td>-0.612750</td>\n","      <td>-0.601676</td>\n","      <td>-0.613763</td>\n","      <td>0.471788</td>\n","      <td>-0.74978</td>\n","      <td>-0.472150</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.768322</td>\n","      <td>-0.239021</td>\n","      <td>-0.229063</td>\n","      <td>1.802791</td>\n","      <td>1.300685</td>\n","      <td>2.727485</td>\n","      <td>-0.13909</td>\n","      <td>-0.025692</td>\n","      <td>2.257514</td>\n","      <td>0.751221</td>\n","      <td>1.769156</td>\n","      <td>1.758142</td>\n","      <td>-0.313814</td>\n","      <td>0.831888</td>\n","      <td>-1.382878</td>\n","      <td>-1.25173</td>\n","      <td>-0.290671</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0         1         2         3         4         5        6    \\\n","0 -1.083699 -0.239021 -0.986135 -0.190185 -0.933752 -0.366213 -0.13909   \n","1 -1.621048 -0.239021 -2.927746 -0.514813 -0.933752 -0.366213 -0.13909   \n","2 -0.768322 -0.239021 -0.229063  1.802791  1.300685  2.727485 -0.13909   \n","\n","        7         8         9         10        11        12        13   \\\n","0 -0.025692 -2.667854 -1.554894 -0.878420 -0.612750 -0.063636 -0.619896   \n","1 -0.025692 -0.148796  0.467339  1.228547 -0.612750 -0.601676 -0.613763   \n","2 -0.025692  2.257514  0.751221  1.769156  1.758142 -0.313814  0.831888   \n","\n","        14       15        16   17   18   19   20   21   22   23   24   25   \\\n","0 -1.382878 -0.74978  0.866256  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0   \n","1  0.471788 -0.74978 -0.472150  0.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0   \n","2 -1.382878 -1.25173 -0.290671  0.0  1.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0   \n","\n","   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   \\\n","0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   41   42   43   44   45   46   47   48   49   50   51   52   53   54   55   \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   56   57   58   59   60   61   62   63   64   65   66   67   68   69   70   \\\n","0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   71   72   73   74   75   76   77   78   79   80   81   82   83   84   85   \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   86   87   88   89   90   91   92   93   94   95   96   97   98   99   100  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  \\\n","0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n","\n","   161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  \\\n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n","\n","   221  222  223  224  225  226  227  228  229  230  231  232  \n","0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n","1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n","2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  "]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCLE4qb7WC5L","executionInfo":{"status":"ok","timestamp":1611300294042,"user_tz":-60,"elapsed":21944,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"70f5ca0c-e151-44fb-e1f1-91fc22e4adf5"},"source":["model = Sequential()\n","model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=L1(l1=0.0000), kernel_initializer=GlorotNormal(seed=seed_value)))\n","model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","history = model.fit(X_subtrain_transformed, y_subtrain, validation_data=(X_val_transformed, y_val) ,batch_size=256, epochs=100, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","print(\"Training Accuracy: {:.3f}\".format(model.evaluate(X_subtrain_transformed, y_subtrain, batch_size=256, verbose=0)[1]))\n","print(\"Validation Accuracy: {:.3f}\".format(model.evaluate(X_val_transformed, y_val, batch_size=256, verbose=0)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.5602 - binary_accuracy: 0.7040 - val_loss: 0.4566 - val_binary_accuracy: 0.7780\n","Epoch 2/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4468 - binary_accuracy: 0.7823 - val_loss: 0.4371 - val_binary_accuracy: 0.7894\n","Epoch 3/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4322 - binary_accuracy: 0.7905 - val_loss: 0.4316 - val_binary_accuracy: 0.7916\n","Epoch 4/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7924 - val_loss: 0.4297 - val_binary_accuracy: 0.7918\n","Epoch 5/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7934 - val_loss: 0.4284 - val_binary_accuracy: 0.7935\n","Epoch 6/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4244 - binary_accuracy: 0.7954 - val_loss: 0.4279 - val_binary_accuracy: 0.7934\n","Epoch 7/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7930 - val_loss: 0.4277 - val_binary_accuracy: 0.7929\n","Epoch 8/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7936 - val_loss: 0.4273 - val_binary_accuracy: 0.7938\n","Epoch 9/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7947 - val_loss: 0.4272 - val_binary_accuracy: 0.7934\n","Epoch 10/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4217 - binary_accuracy: 0.7961 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 11/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7961 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 12/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 13/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 14/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7951 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 15/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7959 - val_loss: 0.4272 - val_binary_accuracy: 0.7940\n","Epoch 16/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7946 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Restoring model weights from the end of the best epoch.\n","Epoch 00016: early stopping\n","Training Accuracy: 0.796\n","Validation Accuracy: 0.794\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IMf4ZlKsWn6e"},"source":["Result: Transforming Numerical Features into a Normal Distribution helps extremely"]},{"cell_type":"markdown","metadata":{"id":"LHMJDfUWYdmN"},"source":["##### 5. Decorrelation with PCA"]},{"cell_type":"code","metadata":{"id":"MS6EozXnYu5R"},"source":["# Specify pre-processing pipeline for the different types of attributes\n","preprocessing = ColumnTransformer(\n","    [\n","      (\"numeric\", Pipeline([\n","                            (\"powertransform\", PowerTransformer(method=\"yeo-johnson\", standardize=True)),\n","                            (\"pca\", PCA())\n","                          ]), numeric_features),\n","     (\"binary\", PCA(), binary_features)\n","    ],\n","    remainder=\"passthrough\", verbose=True, n_jobs=-1\n",").fit(X_subtrain)\n","X_subtrain_transformed = preprocessing.transform(X_subtrain)\n","X_val_transformed = preprocessing.transform(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"8Ev5xgdRYu5S","executionInfo":{"status":"ok","timestamp":1611300450154,"user_tz":-60,"elapsed":862,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"9c5218da-01e2-424b-e3c8-a52fc42ff500"},"source":["# Validation\n","tmp = pd.DataFrame(X_subtrain_transformed)\n","print(tmp.shape)\n","tmp.head(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(172824, 233)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","      <th>110</th>\n","      <th>111</th>\n","      <th>112</th>\n","      <th>113</th>\n","      <th>114</th>\n","      <th>115</th>\n","      <th>116</th>\n","      <th>117</th>\n","      <th>118</th>\n","      <th>119</th>\n","      <th>120</th>\n","      <th>121</th>\n","      <th>122</th>\n","      <th>123</th>\n","      <th>124</th>\n","      <th>125</th>\n","      <th>126</th>\n","      <th>127</th>\n","      <th>128</th>\n","      <th>129</th>\n","      <th>130</th>\n","      <th>131</th>\n","      <th>132</th>\n","      <th>133</th>\n","      <th>134</th>\n","      <th>135</th>\n","      <th>136</th>\n","      <th>137</th>\n","      <th>138</th>\n","      <th>139</th>\n","      <th>140</th>\n","      <th>141</th>\n","      <th>142</th>\n","      <th>143</th>\n","      <th>144</th>\n","      <th>145</th>\n","      <th>146</th>\n","      <th>147</th>\n","      <th>148</th>\n","      <th>149</th>\n","      <th>150</th>\n","      <th>151</th>\n","      <th>152</th>\n","      <th>153</th>\n","      <th>154</th>\n","      <th>155</th>\n","      <th>156</th>\n","      <th>157</th>\n","      <th>158</th>\n","      <th>159</th>\n","      <th>160</th>\n","      <th>161</th>\n","      <th>162</th>\n","      <th>163</th>\n","      <th>164</th>\n","      <th>165</th>\n","      <th>166</th>\n","      <th>167</th>\n","      <th>168</th>\n","      <th>169</th>\n","      <th>170</th>\n","      <th>171</th>\n","      <th>172</th>\n","      <th>173</th>\n","      <th>174</th>\n","      <th>175</th>\n","      <th>176</th>\n","      <th>177</th>\n","      <th>178</th>\n","      <th>179</th>\n","      <th>180</th>\n","      <th>181</th>\n","      <th>182</th>\n","      <th>183</th>\n","      <th>184</th>\n","      <th>185</th>\n","      <th>186</th>\n","      <th>187</th>\n","      <th>188</th>\n","      <th>189</th>\n","      <th>190</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","      <th>200</th>\n","      <th>201</th>\n","      <th>202</th>\n","      <th>203</th>\n","      <th>204</th>\n","      <th>205</th>\n","      <th>206</th>\n","      <th>207</th>\n","      <th>208</th>\n","      <th>209</th>\n","      <th>210</th>\n","      <th>211</th>\n","      <th>212</th>\n","      <th>213</th>\n","      <th>214</th>\n","      <th>215</th>\n","      <th>216</th>\n","      <th>217</th>\n","      <th>218</th>\n","      <th>219</th>\n","      <th>220</th>\n","      <th>221</th>\n","      <th>222</th>\n","      <th>223</th>\n","      <th>224</th>\n","      <th>225</th>\n","      <th>226</th>\n","      <th>227</th>\n","      <th>228</th>\n","      <th>229</th>\n","      <th>230</th>\n","      <th>231</th>\n","      <th>232</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-2.601059</td>\n","      <td>-0.410007</td>\n","      <td>-0.404783</td>\n","      <td>1.164287</td>\n","      <td>-0.091849</td>\n","      <td>-0.212521</td>\n","      <td>-0.529216</td>\n","      <td>2.294785</td>\n","      <td>0.580161</td>\n","      <td>0.953602</td>\n","      <td>-0.148654</td>\n","      <td>-0.430559</td>\n","      <td>0.612913</td>\n","      <td>0.851988</td>\n","      <td>-0.351759</td>\n","      <td>0.900483</td>\n","      <td>0.139061</td>\n","      <td>-1.099618</td>\n","      <td>-0.327502</td>\n","      <td>-0.019383</td>\n","      <td>-0.226823</td>\n","      <td>0.747183</td>\n","      <td>-0.429928</td>\n","      <td>-0.461063</td>\n","      <td>0.082005</td>\n","      <td>-0.414749</td>\n","      <td>0.110693</td>\n","      <td>-0.026495</td>\n","      <td>0.099997</td>\n","      <td>0.105971</td>\n","      <td>-0.116056</td>\n","      <td>-0.053963</td>\n","      <td>0.024474</td>\n","      <td>-0.122303</td>\n","      <td>-0.250888</td>\n","      <td>0.853770</td>\n","      <td>0.142416</td>\n","      <td>-0.004165</td>\n","      <td>0.074606</td>\n","      <td>0.108945</td>\n","      <td>-0.018510</td>\n","      <td>0.015295</td>\n","      <td>0.043422</td>\n","      <td>0.028247</td>\n","      <td>0.006209</td>\n","      <td>0.025120</td>\n","      <td>-0.063271</td>\n","      <td>-0.086629</td>\n","      <td>0.007027</td>\n","      <td>-0.002120</td>\n","      <td>-0.012182</td>\n","      <td>0.042487</td>\n","      <td>-0.003254</td>\n","      <td>0.006190</td>\n","      <td>0.001184</td>\n","      <td>-0.001217</td>\n","      <td>-0.012217</td>\n","      <td>0.009180</td>\n","      <td>0.002767</td>\n","      <td>0.001048</td>\n","      <td>-0.004724</td>\n","      <td>-0.011325</td>\n","      <td>-0.002727</td>\n","      <td>-0.006939</td>\n","      <td>0.000472</td>\n","      <td>0.006187</td>\n","      <td>0.000041</td>\n","      <td>0.000298</td>\n","      <td>0.004510</td>\n","      <td>0.010738</td>\n","      <td>0.006230</td>\n","      <td>0.001477</td>\n","      <td>0.003657</td>\n","      <td>0.002938</td>\n","      <td>-0.000782</td>\n","      <td>-0.001346</td>\n","      <td>-0.000818</td>\n","      <td>0.003122</td>\n","      <td>-0.005166</td>\n","      <td>0.003877</td>\n","      <td>0.005918</td>\n","      <td>0.005787</td>\n","      <td>-0.005882</td>\n","      <td>0.00263</td>\n","      <td>-0.002904</td>\n","      <td>-0.005847</td>\n","      <td>-0.003477</td>\n","      <td>0.000527</td>\n","      <td>0.000739</td>\n","      <td>0.004867</td>\n","      <td>0.000880</td>\n","      <td>-0.005517</td>\n","      <td>0.002925</td>\n","      <td>-0.000497</td>\n","      <td>0.003080</td>\n","      <td>0.007818</td>\n","      <td>-0.001841</td>\n","      <td>0.002798</td>\n","      <td>0.002100</td>\n","      <td>-0.002972</td>\n","      <td>0.001075</td>\n","      <td>-0.000830</td>\n","      <td>-0.003603</td>\n","      <td>0.000896</td>\n","      <td>0.002835</td>\n","      <td>0.004152</td>\n","      <td>0.002494</td>\n","      <td>0.000274</td>\n","      <td>0.003012</td>\n","      <td>0.000236</td>\n","      <td>0.002472</td>\n","      <td>0.007334</td>\n","      <td>-0.007456</td>\n","      <td>0.002126</td>\n","      <td>0.001394</td>\n","      <td>-0.004337</td>\n","      <td>-0.007888</td>\n","      <td>0.005529</td>\n","      <td>0.000918</td>\n","      <td>0.000720</td>\n","      <td>0.000280</td>\n","      <td>-0.001919</td>\n","      <td>0.001508</td>\n","      <td>0.001949</td>\n","      <td>-0.001497</td>\n","      <td>0.001230</td>\n","      <td>0.000807</td>\n","      <td>0.000015</td>\n","      <td>-0.000064</td>\n","      <td>0.001504</td>\n","      <td>-0.001990</td>\n","      <td>-0.000015</td>\n","      <td>0.000775</td>\n","      <td>-0.000523</td>\n","      <td>0.002154</td>\n","      <td>0.001255</td>\n","      <td>0.000414</td>\n","      <td>0.000577</td>\n","      <td>0.000330</td>\n","      <td>0.001437</td>\n","      <td>0.001105</td>\n","      <td>-0.000101</td>\n","      <td>-0.000120</td>\n","      <td>0.000657</td>\n","      <td>0.000166</td>\n","      <td>-0.001112</td>\n","      <td>0.000889</td>\n","      <td>0.000372</td>\n","      <td>0.000657</td>\n","      <td>0.000928</td>\n","      <td>-0.000215</td>\n","      <td>0.000224</td>\n","      <td>0.000565</td>\n","      <td>0.000330</td>\n","      <td>0.000292</td>\n","      <td>0.000197</td>\n","      <td>-0.000598</td>\n","      <td>0.000787</td>\n","      <td>0.000429</td>\n","      <td>0.000866</td>\n","      <td>0.000944</td>\n","      <td>0.000434</td>\n","      <td>-0.000398</td>\n","      <td>0.000217</td>\n","      <td>-0.000186</td>\n","      <td>-0.000214</td>\n","      <td>0.001613</td>\n","      <td>0.001532</td>\n","      <td>0.000447</td>\n","      <td>-0.000391</td>\n","      <td>0.000676</td>\n","      <td>-0.001097</td>\n","      <td>0.000478</td>\n","      <td>0.000832</td>\n","      <td>0.000190</td>\n","      <td>-0.000282</td>\n","      <td>-0.000400</td>\n","      <td>-0.000315</td>\n","      <td>0.000918</td>\n","      <td>-0.000315</td>\n","      <td>0.000428</td>\n","      <td>0.000410</td>\n","      <td>0.000147</td>\n","      <td>-0.000568</td>\n","      <td>-0.000256</td>\n","      <td>-0.000437</td>\n","      <td>0.000433</td>\n","      <td>-0.000034</td>\n","      <td>-0.000689</td>\n","      <td>0.000114</td>\n","      <td>0.000350</td>\n","      <td>-0.000156</td>\n","      <td>0.000121</td>\n","      <td>0.000029</td>\n","      <td>0.000091</td>\n","      <td>0.000517</td>\n","      <td>-0.000020</td>\n","      <td>-0.000059</td>\n","      <td>-0.000363</td>\n","      <td>-0.000185</td>\n","      <td>0.000341</td>\n","      <td>-0.000432</td>\n","      <td>0.000191</td>\n","      <td>-0.000258</td>\n","      <td>0.000201</td>\n","      <td>0.000255</td>\n","      <td>0.000112</td>\n","      <td>0.000332</td>\n","      <td>0.000195</td>\n","      <td>-0.000183</td>\n","      <td>-0.000160</td>\n","      <td>-0.000344</td>\n","      <td>-0.000035</td>\n","      <td>-0.000120</td>\n","      <td>-0.000132</td>\n","      <td>0.000176</td>\n","      <td>0.000098</td>\n","      <td>0.000209</td>\n","      <td>0.000072</td>\n","      <td>-0.000039</td>\n","      <td>0.000108</td>\n","      <td>-0.000052</td>\n","      <td>-0.000006</td>\n","      <td>0.000150</td>\n","      <td>-0.000046</td>\n","      <td>0.000253</td>\n","      <td>0.000087</td>\n","      <td>0.000020</td>\n","      <td>9.947490e-06</td>\n","      <td>0.000020</td>\n","      <td>-1.545393e-07</td>\n","      <td>-1.332516e-07</td>\n","      <td>-3.557214e-08</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.390127</td>\n","      <td>-2.211286</td>\n","      <td>0.641949</td>\n","      <td>0.480664</td>\n","      <td>-0.239196</td>\n","      <td>-0.387141</td>\n","      <td>2.109289</td>\n","      <td>0.654540</td>\n","      <td>-0.251707</td>\n","      <td>-0.133599</td>\n","      <td>-0.882627</td>\n","      <td>0.938515</td>\n","      <td>1.286549</td>\n","      <td>-0.575849</td>\n","      <td>0.198403</td>\n","      <td>-0.318513</td>\n","      <td>0.027164</td>\n","      <td>-0.074123</td>\n","      <td>0.297622</td>\n","      <td>-0.815348</td>\n","      <td>-0.735152</td>\n","      <td>0.686201</td>\n","      <td>0.206959</td>\n","      <td>-0.296421</td>\n","      <td>0.058766</td>\n","      <td>-0.559377</td>\n","      <td>0.551333</td>\n","      <td>0.051256</td>\n","      <td>0.121324</td>\n","      <td>0.057215</td>\n","      <td>-0.134397</td>\n","      <td>0.032183</td>\n","      <td>0.035923</td>\n","      <td>-0.147303</td>\n","      <td>-0.228000</td>\n","      <td>0.889540</td>\n","      <td>0.078292</td>\n","      <td>-0.156148</td>\n","      <td>0.020600</td>\n","      <td>0.068942</td>\n","      <td>0.882819</td>\n","      <td>0.002429</td>\n","      <td>0.141881</td>\n","      <td>0.217306</td>\n","      <td>0.200017</td>\n","      <td>0.068899</td>\n","      <td>0.026851</td>\n","      <td>-0.084228</td>\n","      <td>-0.157691</td>\n","      <td>0.009721</td>\n","      <td>0.103782</td>\n","      <td>0.051113</td>\n","      <td>0.018131</td>\n","      <td>0.040565</td>\n","      <td>0.011437</td>\n","      <td>-0.032416</td>\n","      <td>-0.058184</td>\n","      <td>-0.036572</td>\n","      <td>0.011721</td>\n","      <td>-0.015764</td>\n","      <td>0.003151</td>\n","      <td>-0.010298</td>\n","      <td>-0.026554</td>\n","      <td>-0.056117</td>\n","      <td>-0.010862</td>\n","      <td>0.014048</td>\n","      <td>-0.007604</td>\n","      <td>-0.006000</td>\n","      <td>0.001828</td>\n","      <td>0.009058</td>\n","      <td>0.044494</td>\n","      <td>0.034462</td>\n","      <td>0.001919</td>\n","      <td>0.018852</td>\n","      <td>0.012703</td>\n","      <td>-0.006174</td>\n","      <td>-0.010952</td>\n","      <td>0.011183</td>\n","      <td>0.008523</td>\n","      <td>0.015606</td>\n","      <td>0.007442</td>\n","      <td>-0.005025</td>\n","      <td>-0.000957</td>\n","      <td>0.01720</td>\n","      <td>0.019514</td>\n","      <td>0.004926</td>\n","      <td>-0.005495</td>\n","      <td>0.001981</td>\n","      <td>-0.000666</td>\n","      <td>-0.007510</td>\n","      <td>-0.004496</td>\n","      <td>-0.002500</td>\n","      <td>0.004563</td>\n","      <td>-0.001909</td>\n","      <td>-0.000678</td>\n","      <td>0.006479</td>\n","      <td>-0.007231</td>\n","      <td>-0.007920</td>\n","      <td>-0.006523</td>\n","      <td>0.004197</td>\n","      <td>-0.004591</td>\n","      <td>-0.002859</td>\n","      <td>-0.005204</td>\n","      <td>-0.002665</td>\n","      <td>0.003520</td>\n","      <td>-0.002690</td>\n","      <td>0.001776</td>\n","      <td>-0.004852</td>\n","      <td>0.001078</td>\n","      <td>-0.002523</td>\n","      <td>-0.005887</td>\n","      <td>-0.005219</td>\n","      <td>-0.005446</td>\n","      <td>0.000081</td>\n","      <td>0.000600</td>\n","      <td>-0.004281</td>\n","      <td>-0.006978</td>\n","      <td>0.005739</td>\n","      <td>0.002589</td>\n","      <td>-0.000909</td>\n","      <td>-0.003316</td>\n","      <td>0.001044</td>\n","      <td>0.002545</td>\n","      <td>-0.000693</td>\n","      <td>0.002172</td>\n","      <td>-0.002990</td>\n","      <td>0.002236</td>\n","      <td>-0.004104</td>\n","      <td>0.002639</td>\n","      <td>0.000089</td>\n","      <td>-0.001249</td>\n","      <td>-0.000356</td>\n","      <td>0.003223</td>\n","      <td>0.001742</td>\n","      <td>-0.001034</td>\n","      <td>0.004711</td>\n","      <td>0.000209</td>\n","      <td>0.000699</td>\n","      <td>-0.003861</td>\n","      <td>0.002382</td>\n","      <td>0.003050</td>\n","      <td>-0.001392</td>\n","      <td>-0.001263</td>\n","      <td>-0.001655</td>\n","      <td>0.001659</td>\n","      <td>-0.001105</td>\n","      <td>-0.000599</td>\n","      <td>-0.002129</td>\n","      <td>-0.000347</td>\n","      <td>-0.000446</td>\n","      <td>-0.001706</td>\n","      <td>-0.000002</td>\n","      <td>-0.001699</td>\n","      <td>0.000757</td>\n","      <td>0.000570</td>\n","      <td>0.000820</td>\n","      <td>-0.003218</td>\n","      <td>-0.002266</td>\n","      <td>0.001395</td>\n","      <td>-0.000741</td>\n","      <td>-0.001512</td>\n","      <td>-0.000076</td>\n","      <td>0.000283</td>\n","      <td>0.000443</td>\n","      <td>-0.003419</td>\n","      <td>0.000608</td>\n","      <td>0.000563</td>\n","      <td>0.002869</td>\n","      <td>-0.000031</td>\n","      <td>0.000708</td>\n","      <td>-0.000062</td>\n","      <td>-0.002083</td>\n","      <td>-0.001518</td>\n","      <td>0.000346</td>\n","      <td>-0.000668</td>\n","      <td>-0.000377</td>\n","      <td>-0.001048</td>\n","      <td>-0.000127</td>\n","      <td>-0.000518</td>\n","      <td>-0.001645</td>\n","      <td>0.000168</td>\n","      <td>0.000666</td>\n","      <td>-0.000371</td>\n","      <td>-0.001173</td>\n","      <td>-0.000071</td>\n","      <td>-0.001056</td>\n","      <td>-0.001224</td>\n","      <td>-0.000079</td>\n","      <td>-0.001266</td>\n","      <td>-0.000801</td>\n","      <td>-0.001100</td>\n","      <td>-0.000838</td>\n","      <td>0.000203</td>\n","      <td>-0.000026</td>\n","      <td>-0.000576</td>\n","      <td>-0.000038</td>\n","      <td>-0.000510</td>\n","      <td>-0.000743</td>\n","      <td>-0.000249</td>\n","      <td>0.000763</td>\n","      <td>0.001236</td>\n","      <td>-0.000686</td>\n","      <td>-0.000292</td>\n","      <td>-0.000636</td>\n","      <td>0.000019</td>\n","      <td>-0.000356</td>\n","      <td>0.000756</td>\n","      <td>-0.000111</td>\n","      <td>0.000481</td>\n","      <td>-0.000924</td>\n","      <td>-0.000439</td>\n","      <td>-0.000023</td>\n","      <td>-0.000683</td>\n","      <td>-0.000052</td>\n","      <td>0.000369</td>\n","      <td>-0.000166</td>\n","      <td>-0.000728</td>\n","      <td>0.000103</td>\n","      <td>0.000423</td>\n","      <td>0.000416</td>\n","      <td>-0.000305</td>\n","      <td>0.000158</td>\n","      <td>0.000093</td>\n","      <td>0.000172</td>\n","      <td>0.000300</td>\n","      <td>0.000302</td>\n","      <td>0.000222</td>\n","      <td>0.000134</td>\n","      <td>-1.737526e-04</td>\n","      <td>0.000051</td>\n","      <td>-6.513233e-08</td>\n","      <td>-2.189333e-07</td>\n","      <td>1.680766e-09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.884293</td>\n","      <td>-2.409525</td>\n","      <td>2.013652</td>\n","      <td>-0.189625</td>\n","      <td>0.385735</td>\n","      <td>0.039777</td>\n","      <td>-1.415718</td>\n","      <td>0.185991</td>\n","      <td>-2.201274</td>\n","      <td>-0.502807</td>\n","      <td>0.497177</td>\n","      <td>-0.674013</td>\n","      <td>0.354346</td>\n","      <td>0.169585</td>\n","      <td>1.200940</td>\n","      <td>-1.191646</td>\n","      <td>-0.339670</td>\n","      <td>0.112941</td>\n","      <td>-0.206415</td>\n","      <td>-0.195165</td>\n","      <td>-0.335771</td>\n","      <td>0.263079</td>\n","      <td>0.331129</td>\n","      <td>1.013563</td>\n","      <td>0.043525</td>\n","      <td>-0.518445</td>\n","      <td>-0.522968</td>\n","      <td>0.279505</td>\n","      <td>-0.198370</td>\n","      <td>-0.215808</td>\n","      <td>0.000596</td>\n","      <td>-0.120045</td>\n","      <td>-0.088476</td>\n","      <td>-0.105309</td>\n","      <td>-0.117557</td>\n","      <td>-0.188855</td>\n","      <td>0.803332</td>\n","      <td>0.414211</td>\n","      <td>0.068096</td>\n","      <td>0.049307</td>\n","      <td>0.073627</td>\n","      <td>0.010122</td>\n","      <td>0.047019</td>\n","      <td>0.056625</td>\n","      <td>0.043259</td>\n","      <td>0.023700</td>\n","      <td>0.031280</td>\n","      <td>0.003789</td>\n","      <td>0.022373</td>\n","      <td>0.002391</td>\n","      <td>-0.001303</td>\n","      <td>0.053578</td>\n","      <td>0.032936</td>\n","      <td>0.012566</td>\n","      <td>0.009272</td>\n","      <td>-0.005530</td>\n","      <td>-0.014927</td>\n","      <td>0.014571</td>\n","      <td>0.004073</td>\n","      <td>-0.006173</td>\n","      <td>-0.004719</td>\n","      <td>-0.020182</td>\n","      <td>-0.011060</td>\n","      <td>-0.012471</td>\n","      <td>0.008982</td>\n","      <td>0.010254</td>\n","      <td>0.000717</td>\n","      <td>0.004821</td>\n","      <td>-0.006255</td>\n","      <td>0.001092</td>\n","      <td>0.011104</td>\n","      <td>0.005731</td>\n","      <td>-0.000022</td>\n","      <td>0.000968</td>\n","      <td>0.003213</td>\n","      <td>-0.004678</td>\n","      <td>0.000685</td>\n","      <td>-0.012931</td>\n","      <td>0.000987</td>\n","      <td>-0.008520</td>\n","      <td>0.001498</td>\n","      <td>0.005992</td>\n","      <td>-0.003532</td>\n","      <td>-0.00511</td>\n","      <td>-0.010210</td>\n","      <td>-0.008831</td>\n","      <td>0.007250</td>\n","      <td>0.000628</td>\n","      <td>0.003255</td>\n","      <td>-0.000029</td>\n","      <td>-0.001687</td>\n","      <td>-0.005488</td>\n","      <td>-0.000504</td>\n","      <td>-0.002835</td>\n","      <td>-0.001372</td>\n","      <td>-0.005509</td>\n","      <td>-0.002581</td>\n","      <td>0.000775</td>\n","      <td>0.000466</td>\n","      <td>0.000149</td>\n","      <td>-0.011500</td>\n","      <td>-0.002281</td>\n","      <td>-0.000902</td>\n","      <td>-0.007497</td>\n","      <td>-0.002984</td>\n","      <td>-0.001963</td>\n","      <td>-0.008788</td>\n","      <td>-0.004431</td>\n","      <td>-0.001542</td>\n","      <td>-0.000685</td>\n","      <td>-0.007583</td>\n","      <td>-0.007108</td>\n","      <td>0.005410</td>\n","      <td>-0.002953</td>\n","      <td>-0.002745</td>\n","      <td>-0.000229</td>\n","      <td>0.003836</td>\n","      <td>-0.007513</td>\n","      <td>0.001524</td>\n","      <td>0.001783</td>\n","      <td>0.001365</td>\n","      <td>-0.000685</td>\n","      <td>-0.000743</td>\n","      <td>-0.001943</td>\n","      <td>-0.000762</td>\n","      <td>-0.001817</td>\n","      <td>0.002229</td>\n","      <td>-0.000119</td>\n","      <td>0.002088</td>\n","      <td>0.001517</td>\n","      <td>0.004343</td>\n","      <td>0.000093</td>\n","      <td>-0.001983</td>\n","      <td>-0.001530</td>\n","      <td>-0.002327</td>\n","      <td>-0.001071</td>\n","      <td>-0.001766</td>\n","      <td>0.001082</td>\n","      <td>-0.000493</td>\n","      <td>0.000349</td>\n","      <td>0.000624</td>\n","      <td>-0.002752</td>\n","      <td>0.001623</td>\n","      <td>-0.000490</td>\n","      <td>-0.001594</td>\n","      <td>0.000308</td>\n","      <td>0.000010</td>\n","      <td>0.000839</td>\n","      <td>-0.001047</td>\n","      <td>-0.000008</td>\n","      <td>0.000230</td>\n","      <td>-0.000584</td>\n","      <td>-0.001259</td>\n","      <td>0.000971</td>\n","      <td>0.000530</td>\n","      <td>0.000526</td>\n","      <td>-0.001603</td>\n","      <td>0.000823</td>\n","      <td>-0.000905</td>\n","      <td>0.001621</td>\n","      <td>-0.000245</td>\n","      <td>-0.000780</td>\n","      <td>-0.000135</td>\n","      <td>-0.000498</td>\n","      <td>0.000354</td>\n","      <td>0.000332</td>\n","      <td>-0.000123</td>\n","      <td>-0.000273</td>\n","      <td>-0.000275</td>\n","      <td>-0.001011</td>\n","      <td>-0.000682</td>\n","      <td>-0.001315</td>\n","      <td>0.000010</td>\n","      <td>-0.000450</td>\n","      <td>0.000766</td>\n","      <td>-0.001626</td>\n","      <td>-0.000391</td>\n","      <td>-0.000018</td>\n","      <td>-0.001035</td>\n","      <td>0.000383</td>\n","      <td>0.000431</td>\n","      <td>0.000074</td>\n","      <td>0.000223</td>\n","      <td>-0.000091</td>\n","      <td>-0.000572</td>\n","      <td>-0.000729</td>\n","      <td>-0.000107</td>\n","      <td>-0.000059</td>\n","      <td>0.000061</td>\n","      <td>-0.000247</td>\n","      <td>-0.000163</td>\n","      <td>0.000034</td>\n","      <td>-0.000021</td>\n","      <td>-0.000820</td>\n","      <td>0.000320</td>\n","      <td>0.000045</td>\n","      <td>-0.000011</td>\n","      <td>0.000143</td>\n","      <td>-0.000082</td>\n","      <td>-0.000565</td>\n","      <td>0.000409</td>\n","      <td>-0.000395</td>\n","      <td>0.000224</td>\n","      <td>-0.000379</td>\n","      <td>-0.000316</td>\n","      <td>0.000210</td>\n","      <td>-0.000110</td>\n","      <td>0.000059</td>\n","      <td>-0.000200</td>\n","      <td>-0.000718</td>\n","      <td>0.000290</td>\n","      <td>-0.000282</td>\n","      <td>-0.000136</td>\n","      <td>-0.000400</td>\n","      <td>-0.000139</td>\n","      <td>-0.000032</td>\n","      <td>0.000243</td>\n","      <td>0.000073</td>\n","      <td>0.000050</td>\n","      <td>0.000210</td>\n","      <td>-0.000051</td>\n","      <td>0.000060</td>\n","      <td>0.000032</td>\n","      <td>0.000018</td>\n","      <td>-0.000119</td>\n","      <td>0.000259</td>\n","      <td>-0.000013</td>\n","      <td>0.000189</td>\n","      <td>4.750088e-07</td>\n","      <td>0.000061</td>\n","      <td>2.105464e-07</td>\n","      <td>1.017211e-08</td>\n","      <td>-2.812156e-08</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0         1         2         3         4         5         6    \\\n","0 -2.601059 -0.410007 -0.404783  1.164287 -0.091849 -0.212521 -0.529216   \n","1 -1.390127 -2.211286  0.641949  0.480664 -0.239196 -0.387141  2.109289   \n","2  2.884293 -2.409525  2.013652 -0.189625  0.385735  0.039777 -1.415718   \n","\n","        7         8         9         10        11        12        13   \\\n","0  2.294785  0.580161  0.953602 -0.148654 -0.430559  0.612913  0.851988   \n","1  0.654540 -0.251707 -0.133599 -0.882627  0.938515  1.286549 -0.575849   \n","2  0.185991 -2.201274 -0.502807  0.497177 -0.674013  0.354346  0.169585   \n","\n","        14        15        16        17        18        19        20   \\\n","0 -0.351759  0.900483  0.139061 -1.099618 -0.327502 -0.019383 -0.226823   \n","1  0.198403 -0.318513  0.027164 -0.074123  0.297622 -0.815348 -0.735152   \n","2  1.200940 -1.191646 -0.339670  0.112941 -0.206415 -0.195165 -0.335771   \n","\n","        21        22        23        24        25        26        27   \\\n","0  0.747183 -0.429928 -0.461063  0.082005 -0.414749  0.110693 -0.026495   \n","1  0.686201  0.206959 -0.296421  0.058766 -0.559377  0.551333  0.051256   \n","2  0.263079  0.331129  1.013563  0.043525 -0.518445 -0.522968  0.279505   \n","\n","        28        29        30        31        32        33        34   \\\n","0  0.099997  0.105971 -0.116056 -0.053963  0.024474 -0.122303 -0.250888   \n","1  0.121324  0.057215 -0.134397  0.032183  0.035923 -0.147303 -0.228000   \n","2 -0.198370 -0.215808  0.000596 -0.120045 -0.088476 -0.105309 -0.117557   \n","\n","        35        36        37        38        39        40        41   \\\n","0  0.853770  0.142416 -0.004165  0.074606  0.108945 -0.018510  0.015295   \n","1  0.889540  0.078292 -0.156148  0.020600  0.068942  0.882819  0.002429   \n","2 -0.188855  0.803332  0.414211  0.068096  0.049307  0.073627  0.010122   \n","\n","        42        43        44        45        46        47        48   \\\n","0  0.043422  0.028247  0.006209  0.025120 -0.063271 -0.086629  0.007027   \n","1  0.141881  0.217306  0.200017  0.068899  0.026851 -0.084228 -0.157691   \n","2  0.047019  0.056625  0.043259  0.023700  0.031280  0.003789  0.022373   \n","\n","        49        50        51        52        53        54        55   \\\n","0 -0.002120 -0.012182  0.042487 -0.003254  0.006190  0.001184 -0.001217   \n","1  0.009721  0.103782  0.051113  0.018131  0.040565  0.011437 -0.032416   \n","2  0.002391 -0.001303  0.053578  0.032936  0.012566  0.009272 -0.005530   \n","\n","        56        57        58        59        60        61        62   \\\n","0 -0.012217  0.009180  0.002767  0.001048 -0.004724 -0.011325 -0.002727   \n","1 -0.058184 -0.036572  0.011721 -0.015764  0.003151 -0.010298 -0.026554   \n","2 -0.014927  0.014571  0.004073 -0.006173 -0.004719 -0.020182 -0.011060   \n","\n","        63        64        65        66        67        68        69   \\\n","0 -0.006939  0.000472  0.006187  0.000041  0.000298  0.004510  0.010738   \n","1 -0.056117 -0.010862  0.014048 -0.007604 -0.006000  0.001828  0.009058   \n","2 -0.012471  0.008982  0.010254  0.000717  0.004821 -0.006255  0.001092   \n","\n","        70        71        72        73        74        75        76   \\\n","0  0.006230  0.001477  0.003657  0.002938 -0.000782 -0.001346 -0.000818   \n","1  0.044494  0.034462  0.001919  0.018852  0.012703 -0.006174 -0.010952   \n","2  0.011104  0.005731 -0.000022  0.000968  0.003213 -0.004678  0.000685   \n","\n","        77        78        79        80        81        82       83   \\\n","0  0.003122 -0.005166  0.003877  0.005918  0.005787 -0.005882  0.00263   \n","1  0.011183  0.008523  0.015606  0.007442 -0.005025 -0.000957  0.01720   \n","2 -0.012931  0.000987 -0.008520  0.001498  0.005992 -0.003532 -0.00511   \n","\n","        84        85        86        87        88        89        90   \\\n","0 -0.002904 -0.005847 -0.003477  0.000527  0.000739  0.004867  0.000880   \n","1  0.019514  0.004926 -0.005495  0.001981 -0.000666 -0.007510 -0.004496   \n","2 -0.010210 -0.008831  0.007250  0.000628  0.003255 -0.000029 -0.001687   \n","\n","        91        92        93        94        95        96        97   \\\n","0 -0.005517  0.002925 -0.000497  0.003080  0.007818 -0.001841  0.002798   \n","1 -0.002500  0.004563 -0.001909 -0.000678  0.006479 -0.007231 -0.007920   \n","2 -0.005488 -0.000504 -0.002835 -0.001372 -0.005509 -0.002581  0.000775   \n","\n","        98        99        100       101       102       103       104  \\\n","0  0.002100 -0.002972  0.001075 -0.000830 -0.003603  0.000896  0.002835   \n","1 -0.006523  0.004197 -0.004591 -0.002859 -0.005204 -0.002665  0.003520   \n","2  0.000466  0.000149 -0.011500 -0.002281 -0.000902 -0.007497 -0.002984   \n","\n","        105       106       107       108       109       110       111  \\\n","0  0.004152  0.002494  0.000274  0.003012  0.000236  0.002472  0.007334   \n","1 -0.002690  0.001776 -0.004852  0.001078 -0.002523 -0.005887 -0.005219   \n","2 -0.001963 -0.008788 -0.004431 -0.001542 -0.000685 -0.007583 -0.007108   \n","\n","        112       113       114       115       116       117       118  \\\n","0 -0.007456  0.002126  0.001394 -0.004337 -0.007888  0.005529  0.000918   \n","1 -0.005446  0.000081  0.000600 -0.004281 -0.006978  0.005739  0.002589   \n","2  0.005410 -0.002953 -0.002745 -0.000229  0.003836 -0.007513  0.001524   \n","\n","        119       120       121       122       123       124       125  \\\n","0  0.000720  0.000280 -0.001919  0.001508  0.001949 -0.001497  0.001230   \n","1 -0.000909 -0.003316  0.001044  0.002545 -0.000693  0.002172 -0.002990   \n","2  0.001783  0.001365 -0.000685 -0.000743 -0.001943 -0.000762 -0.001817   \n","\n","        126       127       128       129       130       131       132  \\\n","0  0.000807  0.000015 -0.000064  0.001504 -0.001990 -0.000015  0.000775   \n","1  0.002236 -0.004104  0.002639  0.000089 -0.001249 -0.000356  0.003223   \n","2  0.002229 -0.000119  0.002088  0.001517  0.004343  0.000093 -0.001983   \n","\n","        133       134       135       136       137       138       139  \\\n","0 -0.000523  0.002154  0.001255  0.000414  0.000577  0.000330  0.001437   \n","1  0.001742 -0.001034  0.004711  0.000209  0.000699 -0.003861  0.002382   \n","2 -0.001530 -0.002327 -0.001071 -0.001766  0.001082 -0.000493  0.000349   \n","\n","        140       141       142       143       144       145       146  \\\n","0  0.001105 -0.000101 -0.000120  0.000657  0.000166 -0.001112  0.000889   \n","1  0.003050 -0.001392 -0.001263 -0.001655  0.001659 -0.001105 -0.000599   \n","2  0.000624 -0.002752  0.001623 -0.000490 -0.001594  0.000308  0.000010   \n","\n","        147       148       149       150       151       152       153  \\\n","0  0.000372  0.000657  0.000928 -0.000215  0.000224  0.000565  0.000330   \n","1 -0.002129 -0.000347 -0.000446 -0.001706 -0.000002 -0.001699  0.000757   \n","2  0.000839 -0.001047 -0.000008  0.000230 -0.000584 -0.001259  0.000971   \n","\n","        154       155       156       157       158       159       160  \\\n","0  0.000292  0.000197 -0.000598  0.000787  0.000429  0.000866  0.000944   \n","1  0.000570  0.000820 -0.003218 -0.002266  0.001395 -0.000741 -0.001512   \n","2  0.000530  0.000526 -0.001603  0.000823 -0.000905  0.001621 -0.000245   \n","\n","        161       162       163       164       165       166       167  \\\n","0  0.000434 -0.000398  0.000217 -0.000186 -0.000214  0.001613  0.001532   \n","1 -0.000076  0.000283  0.000443 -0.003419  0.000608  0.000563  0.002869   \n","2 -0.000780 -0.000135 -0.000498  0.000354  0.000332 -0.000123 -0.000273   \n","\n","        168       169       170       171       172       173       174  \\\n","0  0.000447 -0.000391  0.000676 -0.001097  0.000478  0.000832  0.000190   \n","1 -0.000031  0.000708 -0.000062 -0.002083 -0.001518  0.000346 -0.000668   \n","2 -0.000275 -0.001011 -0.000682 -0.001315  0.000010 -0.000450  0.000766   \n","\n","        175       176       177       178       179       180       181  \\\n","0 -0.000282 -0.000400 -0.000315  0.000918 -0.000315  0.000428  0.000410   \n","1 -0.000377 -0.001048 -0.000127 -0.000518 -0.001645  0.000168  0.000666   \n","2 -0.001626 -0.000391 -0.000018 -0.001035  0.000383  0.000431  0.000074   \n","\n","        182       183       184       185       186       187       188  \\\n","0  0.000147 -0.000568 -0.000256 -0.000437  0.000433 -0.000034 -0.000689   \n","1 -0.000371 -0.001173 -0.000071 -0.001056 -0.001224 -0.000079 -0.001266   \n","2  0.000223 -0.000091 -0.000572 -0.000729 -0.000107 -0.000059  0.000061   \n","\n","        189       190       191       192       193       194       195  \\\n","0  0.000114  0.000350 -0.000156  0.000121  0.000029  0.000091  0.000517   \n","1 -0.000801 -0.001100 -0.000838  0.000203 -0.000026 -0.000576 -0.000038   \n","2 -0.000247 -0.000163  0.000034 -0.000021 -0.000820  0.000320  0.000045   \n","\n","        196       197       198       199       200       201       202  \\\n","0 -0.000020 -0.000059 -0.000363 -0.000185  0.000341 -0.000432  0.000191   \n","1 -0.000510 -0.000743 -0.000249  0.000763  0.001236 -0.000686 -0.000292   \n","2 -0.000011  0.000143 -0.000082 -0.000565  0.000409 -0.000395  0.000224   \n","\n","        203       204       205       206       207       208       209  \\\n","0 -0.000258  0.000201  0.000255  0.000112  0.000332  0.000195 -0.000183   \n","1 -0.000636  0.000019 -0.000356  0.000756 -0.000111  0.000481 -0.000924   \n","2 -0.000379 -0.000316  0.000210 -0.000110  0.000059 -0.000200 -0.000718   \n","\n","        210       211       212       213       214       215       216  \\\n","0 -0.000160 -0.000344 -0.000035 -0.000120 -0.000132  0.000176  0.000098   \n","1 -0.000439 -0.000023 -0.000683 -0.000052  0.000369 -0.000166 -0.000728   \n","2  0.000290 -0.000282 -0.000136 -0.000400 -0.000139 -0.000032  0.000243   \n","\n","        217       218       219       220       221       222       223  \\\n","0  0.000209  0.000072 -0.000039  0.000108 -0.000052 -0.000006  0.000150   \n","1  0.000103  0.000423  0.000416 -0.000305  0.000158  0.000093  0.000172   \n","2  0.000073  0.000050  0.000210 -0.000051  0.000060  0.000032  0.000018   \n","\n","        224       225       226       227           228       229  \\\n","0 -0.000046  0.000253  0.000087  0.000020  9.947490e-06  0.000020   \n","1  0.000300  0.000302  0.000222  0.000134 -1.737526e-04  0.000051   \n","2 -0.000119  0.000259 -0.000013  0.000189  4.750088e-07  0.000061   \n","\n","            230           231           232  \n","0 -1.545393e-07 -1.332516e-07 -3.557214e-08  \n","1 -6.513233e-08 -2.189333e-07  1.680766e-09  \n","2  2.105464e-07  1.017211e-08 -2.812156e-08  "]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWOSDy4CYu5T","executionInfo":{"status":"ok","timestamp":1611300478687,"user_tz":-60,"elapsed":25490,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"92fa1870-01c5-4197-d7a0-1143f7404fca"},"source":["model = Sequential()\n","model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=L1(l1=0.0000), kernel_initializer=GlorotNormal(seed=seed_value)))\n","model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","history = model.fit(X_subtrain_transformed, y_subtrain, validation_data=(X_val_transformed, y_val) ,batch_size=256, epochs=100, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","print(\"Training Accuracy: {:.3f}\".format(model.evaluate(X_subtrain_transformed, y_subtrain, batch_size=256, verbose=0)[1]))\n","print(\"Validation Accuracy: {:.3f}\".format(model.evaluate(X_val_transformed, y_val, batch_size=256, verbose=0)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.6173 - binary_accuracy: 0.6518 - val_loss: 0.4579 - val_binary_accuracy: 0.7836\n","Epoch 2/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4465 - binary_accuracy: 0.7873 - val_loss: 0.4353 - val_binary_accuracy: 0.7898\n","Epoch 3/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4306 - binary_accuracy: 0.7919 - val_loss: 0.4307 - val_binary_accuracy: 0.7920\n","Epoch 4/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4275 - binary_accuracy: 0.7932 - val_loss: 0.4291 - val_binary_accuracy: 0.7927\n","Epoch 5/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4259 - binary_accuracy: 0.7933 - val_loss: 0.4281 - val_binary_accuracy: 0.7935\n","Epoch 6/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4240 - binary_accuracy: 0.7961 - val_loss: 0.4276 - val_binary_accuracy: 0.7933\n","Epoch 7/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4254 - binary_accuracy: 0.7937 - val_loss: 0.4274 - val_binary_accuracy: 0.7938\n","Epoch 8/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4255 - binary_accuracy: 0.7943 - val_loss: 0.4270 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4240 - binary_accuracy: 0.7948 - val_loss: 0.4270 - val_binary_accuracy: 0.7937\n","Epoch 10/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4214 - binary_accuracy: 0.7961 - val_loss: 0.4269 - val_binary_accuracy: 0.7942\n","Epoch 11/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7963 - val_loss: 0.4269 - val_binary_accuracy: 0.7943\n","Epoch 12/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4221 - binary_accuracy: 0.7968 - val_loss: 0.4268 - val_binary_accuracy: 0.7943\n","Epoch 13/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4221 - binary_accuracy: 0.7963 - val_loss: 0.4268 - val_binary_accuracy: 0.7944\n","Epoch 14/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7952 - val_loss: 0.4267 - val_binary_accuracy: 0.7941\n","Epoch 15/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7965 - val_loss: 0.4269 - val_binary_accuracy: 0.7938\n","Epoch 16/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4245 - binary_accuracy: 0.7950 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 17/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4188 - binary_accuracy: 0.7990 - val_loss: 0.4269 - val_binary_accuracy: 0.7940\n","Epoch 18/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4255 - binary_accuracy: 0.7942 - val_loss: 0.4267 - val_binary_accuracy: 0.7943\n","Restoring model weights from the end of the best epoch.\n","Epoch 00018: early stopping\n","Training Accuracy: 0.796\n","Validation Accuracy: 0.794\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mpv2Y13Dal6P"},"source":["Result: De-correlated dataset performed on par + it makes the features uninterpretable (i.e. not use PCA)"]},{"cell_type":"markdown","metadata":{"id":"b31IIEU2alm_"},"source":["##### Perform Final Dataset Transformation:"]},{"cell_type":"code","metadata":{"id":"6NH-JrKzQd6v"},"source":["# Strategy: PowerTransform + Standardize Numerical Features; Leave Binary Features as they are\n","preprocessing = ColumnTransformer(\n","    [\n","      (\"numeric\", PowerTransformer(method=\"yeo-johnson\", standardize=True), numeric_features)\n","    ],\n","    remainder=\"passthrough\", verbose=True, n_jobs=-1\n",").fit(X_subtrain)\n","X_subtrain = preprocessing.transform(X_subtrain)\n","X_val = preprocessing.transform(X_val)\n","\n","preprocessing2 = ColumnTransformer(\n","    [\n","      (\"numeric\", PowerTransformer(method=\"yeo-johnson\", standardize=True), numeric_features)\n","    ],\n","    remainder=\"passthrough\", verbose=True, n_jobs=-1\n",").fit(X_train)\n","X_train = preprocessing2.transform(X_train)\n","X_test = preprocessing2.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtryG_j8cRai"},"source":["# Re-arrange feature_names, since they have been changed by the ColumnTransformer\n","feature_names = numeric_feature_names + binary_feature_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"tsnMpfPAQdxV","executionInfo":{"status":"ok","timestamp":1615720821680,"user_tz":-60,"elapsed":792,"user":{"displayName":"David George","photoUrl":"","userId":"01025812402211939868"}},"outputId":"df6c7f77-7dcb-4a81-8841-ec5ba46c4b38"},"source":["# Validation\n","tmp = pd.DataFrame(X_test, columns=feature_names)\n","print(tmp.shape)\n","tmp.head(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(37033, 233)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>goal</th>\n","      <th>number_of_collaborators</th>\n","      <th>funding_period</th>\n","      <th>days_between_created_and_launched</th>\n","      <th>number_of_images</th>\n","      <th>number_of_videos</th>\n","      <th>number_of_audios</th>\n","      <th>number_of_interactives</th>\n","      <th>number_of_words</th>\n","      <th>number_of_links</th>\n","      <th>number_of_creator_backings</th>\n","      <th>number_of_creator_projects</th>\n","      <th>number_of_rewards</th>\n","      <th>number_of_words_per_reward</th>\n","      <th>lowest_pledge_level</th>\n","      <th>highest_pledge_level</th>\n","      <th>avg_months_until_reward</th>\n","      <th>staff_pick</th>\n","      <th>campaign_has_demo_video</th>\n","      <th>campaign_has_environmental_commitments</th>\n","      <th>creator_verified_identity</th>\n","      <th>creator_fb_auth</th>\n","      <th>creator_has_image</th>\n","      <th>creator_allows_follows</th>\n","      <th>facebook_linked</th>\n","      <th>twitter_linked</th>\n","      <th>instagram_linked</th>\n","      <th>linkedin_linked</th>\n","      <th>has_limited_rewards</th>\n","      <th>has_shipped_rewards</th>\n","      <th>has_restricted_shipping_rewards</th>\n","      <th>launch_quartal_1</th>\n","      <th>launch_quartal_2</th>\n","      <th>launch_quartal_3</th>\n","      <th>launch_quartal_4</th>\n","      <th>location_Africa</th>\n","      <th>location_Australia</th>\n","      <th>location_Belgium</th>\n","      <th>location_Canada</th>\n","      <th>location_China</th>\n","      <th>location_Denmark</th>\n","      <th>location_France</th>\n","      <th>location_Germany</th>\n","      <th>location_Hong Kong</th>\n","      <th>location_Ireland</th>\n","      <th>location_Italy</th>\n","      <th>location_Japan</th>\n","      <th>location_Latin and South America</th>\n","      <th>location_Mexico</th>\n","      <th>location_Netherlands</th>\n","      <th>location_New Zealand</th>\n","      <th>location_No Location</th>\n","      <th>location_Norway</th>\n","      <th>location_Oceania and Antarctica</th>\n","      <th>location_Rest of Asia</th>\n","      <th>location_Rest of Europe</th>\n","      <th>location_Singapore</th>\n","      <th>location_Spain</th>\n","      <th>location_Sweden</th>\n","      <th>location_Switzerland</th>\n","      <th>location_United Kingdom</th>\n","      <th>location_United States</th>\n","      <th>category_Art_Ceramics</th>\n","      <th>category_Art_Conceptual Art</th>\n","      <th>category_Art_Digital Art</th>\n","      <th>category_Art_Illustration</th>\n","      <th>category_Art_Installations</th>\n","      <th>category_Art_Mixed Media</th>\n","      <th>category_Art_No Subcategory</th>\n","      <th>category_Art_Painting</th>\n","      <th>category_Art_Performance Art</th>\n","      <th>category_Art_Public Art</th>\n","      <th>category_Art_Sculpture</th>\n","      <th>category_Art_Social Practice</th>\n","      <th>category_Art_Textiles</th>\n","      <th>category_Art_Video Art</th>\n","      <th>category_Comics_Anthologies</th>\n","      <th>category_Comics_Comic Books</th>\n","      <th>category_Comics_Events</th>\n","      <th>category_Comics_Graphic Novels</th>\n","      <th>category_Comics_No Subcategory</th>\n","      <th>category_Comics_Webcomics</th>\n","      <th>category_Crafts_Candles</th>\n","      <th>category_Crafts_Crochet</th>\n","      <th>category_Crafts_DIY</th>\n","      <th>category_Crafts_Embroidery</th>\n","      <th>category_Crafts_Glass</th>\n","      <th>category_Crafts_Knitting</th>\n","      <th>category_Crafts_No Subcategory</th>\n","      <th>category_Crafts_Pottery</th>\n","      <th>category_Crafts_Printing</th>\n","      <th>category_Crafts_Quilts</th>\n","      <th>category_Crafts_Stationery</th>\n","      <th>category_Crafts_Taxidermy</th>\n","      <th>category_Crafts_Weaving</th>\n","      <th>category_Crafts_Woodworking</th>\n","      <th>category_Dance_No Subcategory</th>\n","      <th>category_Dance_Performances</th>\n","      <th>category_Dance_Residencies</th>\n","      <th>category_Dance_Spaces</th>\n","      <th>category_Dance_Workshops</th>\n","      <th>category_Design_Architecture</th>\n","      <th>category_Design_Civic Design</th>\n","      <th>category_Design_Graphic Design</th>\n","      <th>category_Design_Interactive Design</th>\n","      <th>category_Design_No Subcategory</th>\n","      <th>category_Design_Product Design</th>\n","      <th>category_Design_Toys</th>\n","      <th>category_Design_Typography</th>\n","      <th>category_Fashion_Accessories</th>\n","      <th>category_Fashion_Apparel</th>\n","      <th>category_Fashion_Childrenswear</th>\n","      <th>category_Fashion_Couture</th>\n","      <th>category_Fashion_Footwear</th>\n","      <th>category_Fashion_Jewelry</th>\n","      <th>category_Fashion_No Subcategory</th>\n","      <th>category_Fashion_Pet Fashion</th>\n","      <th>category_Fashion_Ready-to-wear</th>\n","      <th>category_Film &amp; Video_Action</th>\n","      <th>category_Film &amp; Video_Animation</th>\n","      <th>category_Film &amp; Video_Comedy</th>\n","      <th>category_Film &amp; Video_Documentary</th>\n","      <th>category_Film &amp; Video_Drama</th>\n","      <th>category_Film &amp; Video_Experimental</th>\n","      <th>category_Film &amp; Video_Family</th>\n","      <th>category_Film &amp; Video_Fantasy</th>\n","      <th>category_Film &amp; Video_Festivals</th>\n","      <th>category_Film &amp; Video_Horror</th>\n","      <th>category_Film &amp; Video_Movie Theaters</th>\n","      <th>category_Film &amp; Video_Music Videos</th>\n","      <th>category_Film &amp; Video_Narrative Film</th>\n","      <th>category_Film &amp; Video_No Subcategory</th>\n","      <th>category_Film &amp; Video_Romance</th>\n","      <th>category_Film &amp; Video_Science Fiction</th>\n","      <th>category_Film &amp; Video_Shorts</th>\n","      <th>category_Film &amp; Video_Television</th>\n","      <th>category_Film &amp; Video_Thrillers</th>\n","      <th>category_Film &amp; Video_Webseries</th>\n","      <th>category_Food_Bacon</th>\n","      <th>category_Food_Community Gardens</th>\n","      <th>category_Food_Cookbooks</th>\n","      <th>category_Food_Drinks</th>\n","      <th>category_Food_Events</th>\n","      <th>category_Food_Farmer's Markets</th>\n","      <th>category_Food_Farms</th>\n","      <th>category_Food_Food Trucks</th>\n","      <th>category_Food_No Subcategory</th>\n","      <th>category_Food_Restaurants</th>\n","      <th>category_Food_Small Batch</th>\n","      <th>category_Food_Spaces</th>\n","      <th>category_Food_Vegan</th>\n","      <th>category_Games_Gaming Hardware</th>\n","      <th>category_Games_Live Games</th>\n","      <th>category_Games_Mobile Games</th>\n","      <th>category_Games_No Subcategory</th>\n","      <th>category_Games_Playing Cards</th>\n","      <th>category_Games_Puzzles</th>\n","      <th>category_Games_Tabletop Games</th>\n","      <th>category_Games_Video Games</th>\n","      <th>category_Journalism_Audio</th>\n","      <th>category_Journalism_No Subcategory</th>\n","      <th>category_Journalism_Photo</th>\n","      <th>category_Journalism_Print</th>\n","      <th>category_Journalism_Video</th>\n","      <th>category_Journalism_Web</th>\n","      <th>category_Music_Blues</th>\n","      <th>category_Music_Chiptune</th>\n","      <th>category_Music_Classical Music</th>\n","      <th>category_Music_Comedy</th>\n","      <th>category_Music_Country &amp; Folk</th>\n","      <th>category_Music_Electronic Music</th>\n","      <th>category_Music_Faith</th>\n","      <th>category_Music_Hip-Hop</th>\n","      <th>category_Music_Indie Rock</th>\n","      <th>category_Music_Jazz</th>\n","      <th>category_Music_Kids</th>\n","      <th>category_Music_Latin</th>\n","      <th>category_Music_Metal</th>\n","      <th>category_Music_No Subcategory</th>\n","      <th>category_Music_Pop</th>\n","      <th>category_Music_Punk</th>\n","      <th>category_Music_R&amp;B</th>\n","      <th>category_Music_Rock</th>\n","      <th>category_Music_World Music</th>\n","      <th>category_Photography_Animals</th>\n","      <th>category_Photography_Fine Art</th>\n","      <th>category_Photography_Nature</th>\n","      <th>category_Photography_No Subcategory</th>\n","      <th>category_Photography_People</th>\n","      <th>category_Photography_Photobooks</th>\n","      <th>category_Photography_Places</th>\n","      <th>category_Publishing_Academic</th>\n","      <th>category_Publishing_Anthologies</th>\n","      <th>category_Publishing_Art Books</th>\n","      <th>category_Publishing_Calendars</th>\n","      <th>category_Publishing_Children's Books</th>\n","      <th>category_Publishing_Comedy</th>\n","      <th>category_Publishing_Fiction</th>\n","      <th>category_Publishing_Letterpress</th>\n","      <th>category_Publishing_Literary Journals</th>\n","      <th>category_Publishing_Literary Spaces</th>\n","      <th>category_Publishing_No Subcategory</th>\n","      <th>category_Publishing_Nonfiction</th>\n","      <th>category_Publishing_Periodicals</th>\n","      <th>category_Publishing_Poetry</th>\n","      <th>category_Publishing_Radio &amp; Podcasts</th>\n","      <th>category_Publishing_Translations</th>\n","      <th>category_Publishing_Young Adult</th>\n","      <th>category_Publishing_Zines</th>\n","      <th>category_Technology_3D Printing</th>\n","      <th>category_Technology_Apps</th>\n","      <th>category_Technology_Camera Equipment</th>\n","      <th>category_Technology_DIY Electronics</th>\n","      <th>category_Technology_Fabrication Tools</th>\n","      <th>category_Technology_Flight</th>\n","      <th>category_Technology_Gadgets</th>\n","      <th>category_Technology_Hardware</th>\n","      <th>category_Technology_Makerspaces</th>\n","      <th>category_Technology_No Subcategory</th>\n","      <th>category_Technology_Robots</th>\n","      <th>category_Technology_Software</th>\n","      <th>category_Technology_Sound</th>\n","      <th>category_Technology_Space Exploration</th>\n","      <th>category_Technology_Wearables</th>\n","      <th>category_Technology_Web</th>\n","      <th>category_Theater_Comedy</th>\n","      <th>category_Theater_Experimental</th>\n","      <th>category_Theater_Festivals</th>\n","      <th>category_Theater_Immersive</th>\n","      <th>category_Theater_Musical</th>\n","      <th>category_Theater_No Subcategory</th>\n","      <th>category_Theater_Plays</th>\n","      <th>category_Theater_Spaces</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.354174</td>\n","      <td>-0.238324</td>\n","      <td>-0.229345</td>\n","      <td>0.460504</td>\n","      <td>-0.933743</td>\n","      <td>-0.36667</td>\n","      <td>-0.13895</td>\n","      <td>-0.026021</td>\n","      <td>-0.138803</td>\n","      <td>-1.555439</td>\n","      <td>-0.878126</td>\n","      <td>-0.611327</td>\n","      <td>-0.064253</td>\n","      <td>-0.442102</td>\n","      <td>-0.094702</td>\n","      <td>-0.228953</td>\n","      <td>-0.471828</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.282328</td>\n","      <td>4.195960</td>\n","      <td>-0.229345</td>\n","      <td>0.704278</td>\n","      <td>0.355405</td>\n","      <td>-0.36667</td>\n","      <td>-0.13895</td>\n","      <td>-0.026021</td>\n","      <td>-0.630609</td>\n","      <td>-0.483575</td>\n","      <td>-0.878126</td>\n","      <td>1.370198</td>\n","      <td>-0.314416</td>\n","      <td>-0.380573</td>\n","      <td>2.202968</td>\n","      <td>0.051035</td>\n","      <td>-0.897264</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.354174</td>\n","      <td>-0.238324</td>\n","      <td>3.220498</td>\n","      <td>-1.190667</td>\n","      <td>-0.933743</td>\n","      <td>-0.36667</td>\n","      <td>-0.13895</td>\n","      <td>-0.026021</td>\n","      <td>-1.206624</td>\n","      <td>-1.555439</td>\n","      <td>1.308827</td>\n","      <td>-0.611327</td>\n","      <td>-0.943067</td>\n","      <td>-0.812086</td>\n","      <td>0.470124</td>\n","      <td>-0.750988</td>\n","      <td>-2.315936</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       goal  number_of_collaborators  funding_period  \\\n","0 -0.354174                -0.238324       -0.229345   \n","1  1.282328                 4.195960       -0.229345   \n","2 -0.354174                -0.238324        3.220498   \n","\n","   days_between_created_and_launched  number_of_images  number_of_videos  \\\n","0                           0.460504         -0.933743          -0.36667   \n","1                           0.704278          0.355405          -0.36667   \n","2                          -1.190667         -0.933743          -0.36667   \n","\n","   number_of_audios  number_of_interactives  number_of_words  number_of_links  \\\n","0          -0.13895               -0.026021        -0.138803        -1.555439   \n","1          -0.13895               -0.026021        -0.630609        -0.483575   \n","2          -0.13895               -0.026021        -1.206624        -1.555439   \n","\n","   number_of_creator_backings  number_of_creator_projects  number_of_rewards  \\\n","0                   -0.878126                   -0.611327          -0.064253   \n","1                   -0.878126                    1.370198          -0.314416   \n","2                    1.308827                   -0.611327          -0.943067   \n","\n","   number_of_words_per_reward  lowest_pledge_level  highest_pledge_level  \\\n","0                   -0.442102            -0.094702             -0.228953   \n","1                   -0.380573             2.202968              0.051035   \n","2                   -0.812086             0.470124             -0.750988   \n","\n","   avg_months_until_reward  staff_pick  campaign_has_demo_video  \\\n","0                -0.471828         0.0                      0.0   \n","1                -0.897264         0.0                      1.0   \n","2                -2.315936         0.0                      1.0   \n","\n","   campaign_has_environmental_commitments  creator_verified_identity  \\\n","0                                     0.0                        0.0   \n","1                                     0.0                        1.0   \n","2                                     0.0                        0.0   \n","\n","   creator_fb_auth  creator_has_image  creator_allows_follows  \\\n","0              0.0                1.0                     1.0   \n","1              0.0                1.0                     1.0   \n","2              0.0                1.0                     1.0   \n","\n","   facebook_linked  twitter_linked  instagram_linked  linkedin_linked  \\\n","0              0.0             0.0               0.0              0.0   \n","1              0.0             0.0               0.0              0.0   \n","2              0.0             0.0               0.0              0.0   \n","\n","   has_limited_rewards  has_shipped_rewards  has_restricted_shipping_rewards  \\\n","0                  0.0                  1.0                              1.0   \n","1                  0.0                  0.0                              0.0   \n","2                  0.0                  0.0                              0.0   \n","\n","   launch_quartal_1  launch_quartal_2  launch_quartal_3  launch_quartal_4  \\\n","0               0.0               0.0               1.0               0.0   \n","1               0.0               0.0               0.0               1.0   \n","2               0.0               1.0               0.0               0.0   \n","\n","   location_Africa  location_Australia  location_Belgium  location_Canada  \\\n","0              0.0                 0.0               0.0              0.0   \n","1              0.0                 0.0               0.0              0.0   \n","2              0.0                 0.0               0.0              0.0   \n","\n","   location_China  location_Denmark  location_France  location_Germany  \\\n","0             0.0               0.0              0.0               0.0   \n","1             0.0               0.0              0.0               0.0   \n","2             0.0               0.0              0.0               1.0   \n","\n","   location_Hong Kong  location_Ireland  location_Italy  location_Japan  \\\n","0                 0.0               0.0             0.0             0.0   \n","1                 0.0               0.0             0.0             0.0   \n","2                 0.0               0.0             0.0             0.0   \n","\n","   location_Latin and South America  location_Mexico  location_Netherlands  \\\n","0                               0.0              0.0                   0.0   \n","1                               0.0              0.0                   0.0   \n","2                               0.0              0.0                   0.0   \n","\n","   location_New Zealand  location_No Location  location_Norway  \\\n","0                   0.0                   0.0              0.0   \n","1                   0.0                   0.0              0.0   \n","2                   0.0                   0.0              0.0   \n","\n","   location_Oceania and Antarctica  location_Rest of Asia  \\\n","0                              0.0                    0.0   \n","1                              0.0                    0.0   \n","2                              0.0                    0.0   \n","\n","   location_Rest of Europe  location_Singapore  location_Spain  \\\n","0                      0.0                 0.0             0.0   \n","1                      0.0                 0.0             0.0   \n","2                      0.0                 0.0             0.0   \n","\n","   location_Sweden  location_Switzerland  location_United Kingdom  \\\n","0              0.0                   0.0                      0.0   \n","1              0.0                   0.0                      1.0   \n","2              0.0                   0.0                      0.0   \n","\n","   location_United States  category_Art_Ceramics  category_Art_Conceptual Art  \\\n","0                     1.0                    0.0                          0.0   \n","1                     0.0                    0.0                          0.0   \n","2                     0.0                    0.0                          0.0   \n","\n","   category_Art_Digital Art  category_Art_Illustration  \\\n","0                       0.0                        0.0   \n","1                       0.0                        0.0   \n","2                       0.0                        0.0   \n","\n","   category_Art_Installations  category_Art_Mixed Media  \\\n","0                         0.0                       0.0   \n","1                         0.0                       0.0   \n","2                         0.0                       0.0   \n","\n","   category_Art_No Subcategory  category_Art_Painting  \\\n","0                          0.0                    0.0   \n","1                          0.0                    0.0   \n","2                          0.0                    0.0   \n","\n","   category_Art_Performance Art  category_Art_Public Art  \\\n","0                           0.0                      0.0   \n","1                           0.0                      0.0   \n","2                           0.0                      0.0   \n","\n","   category_Art_Sculpture  category_Art_Social Practice  \\\n","0                     0.0                           0.0   \n","1                     0.0                           0.0   \n","2                     0.0                           0.0   \n","\n","   category_Art_Textiles  category_Art_Video Art  category_Comics_Anthologies  \\\n","0                    0.0                     0.0                          0.0   \n","1                    0.0                     0.0                          0.0   \n","2                    0.0                     0.0                          0.0   \n","\n","   category_Comics_Comic Books  category_Comics_Events  \\\n","0                          0.0                     0.0   \n","1                          0.0                     0.0   \n","2                          0.0                     0.0   \n","\n","   category_Comics_Graphic Novels  category_Comics_No Subcategory  \\\n","0                             0.0                             0.0   \n","1                             0.0                             0.0   \n","2                             0.0                             0.0   \n","\n","   category_Comics_Webcomics  category_Crafts_Candles  \\\n","0                        0.0                      1.0   \n","1                        0.0                      0.0   \n","2                        0.0                      0.0   \n","\n","   category_Crafts_Crochet  category_Crafts_DIY  category_Crafts_Embroidery  \\\n","0                      0.0                  0.0                         0.0   \n","1                      0.0                  0.0                         0.0   \n","2                      0.0                  0.0                         0.0   \n","\n","   category_Crafts_Glass  category_Crafts_Knitting  \\\n","0                    0.0                       0.0   \n","1                    0.0                       0.0   \n","2                    0.0                       0.0   \n","\n","   category_Crafts_No Subcategory  category_Crafts_Pottery  \\\n","0                             0.0                      0.0   \n","1                             0.0                      0.0   \n","2                             0.0                      0.0   \n","\n","   category_Crafts_Printing  category_Crafts_Quilts  \\\n","0                       0.0                     0.0   \n","1                       0.0                     0.0   \n","2                       0.0                     0.0   \n","\n","   category_Crafts_Stationery  category_Crafts_Taxidermy  \\\n","0                         0.0                        0.0   \n","1                         0.0                        0.0   \n","2                         0.0                        0.0   \n","\n","   category_Crafts_Weaving  category_Crafts_Woodworking  \\\n","0                      0.0                          0.0   \n","1                      0.0                          0.0   \n","2                      0.0                          0.0   \n","\n","   category_Dance_No Subcategory  category_Dance_Performances  \\\n","0                            0.0                          0.0   \n","1                            0.0                          0.0   \n","2                            0.0                          0.0   \n","\n","   category_Dance_Residencies  category_Dance_Spaces  \\\n","0                         0.0                    0.0   \n","1                         0.0                    0.0   \n","2                         0.0                    0.0   \n","\n","   category_Dance_Workshops  category_Design_Architecture  \\\n","0                       0.0                           0.0   \n","1                       0.0                           0.0   \n","2                       0.0                           0.0   \n","\n","   category_Design_Civic Design  category_Design_Graphic Design  \\\n","0                           0.0                             0.0   \n","1                           0.0                             0.0   \n","2                           0.0                             0.0   \n","\n","   category_Design_Interactive Design  category_Design_No Subcategory  \\\n","0                                 0.0                             0.0   \n","1                                 0.0                             0.0   \n","2                                 0.0                             0.0   \n","\n","   category_Design_Product Design  category_Design_Toys  \\\n","0                             0.0                   0.0   \n","1                             0.0                   0.0   \n","2                             0.0                   0.0   \n","\n","   category_Design_Typography  category_Fashion_Accessories  \\\n","0                         0.0                           0.0   \n","1                         0.0                           0.0   \n","2                         0.0                           0.0   \n","\n","   category_Fashion_Apparel  category_Fashion_Childrenswear  \\\n","0                       0.0                             0.0   \n","1                       0.0                             0.0   \n","2                       0.0                             0.0   \n","\n","   category_Fashion_Couture  category_Fashion_Footwear  \\\n","0                       0.0                        0.0   \n","1                       0.0                        0.0   \n","2                       0.0                        0.0   \n","\n","   category_Fashion_Jewelry  category_Fashion_No Subcategory  \\\n","0                       0.0                              0.0   \n","1                       0.0                              0.0   \n","2                       0.0                              0.0   \n","\n","   category_Fashion_Pet Fashion  category_Fashion_Ready-to-wear  \\\n","0                           0.0                             0.0   \n","1                           0.0                             0.0   \n","2                           0.0                             0.0   \n","\n","   category_Film & Video_Action  category_Film & Video_Animation  \\\n","0                           0.0                              0.0   \n","1                           0.0                              0.0   \n","2                           0.0                              0.0   \n","\n","   category_Film & Video_Comedy  category_Film & Video_Documentary  \\\n","0                           0.0                                0.0   \n","1                           0.0                                0.0   \n","2                           0.0                                0.0   \n","\n","   category_Film & Video_Drama  category_Film & Video_Experimental  \\\n","0                          0.0                                 0.0   \n","1                          0.0                                 0.0   \n","2                          0.0                                 0.0   \n","\n","   category_Film & Video_Family  category_Film & Video_Fantasy  \\\n","0                           0.0                            0.0   \n","1                           0.0                            0.0   \n","2                           0.0                            0.0   \n","\n","   category_Film & Video_Festivals  category_Film & Video_Horror  \\\n","0                              0.0                           0.0   \n","1                              0.0                           0.0   \n","2                              0.0                           0.0   \n","\n","   category_Film & Video_Movie Theaters  category_Film & Video_Music Videos  \\\n","0                                   0.0                                 0.0   \n","1                                   0.0                                 0.0   \n","2                                   0.0                                 0.0   \n","\n","   category_Film & Video_Narrative Film  category_Film & Video_No Subcategory  \\\n","0                                   0.0                                   0.0   \n","1                                   0.0                                   0.0   \n","2                                   0.0                                   0.0   \n","\n","   category_Film & Video_Romance  category_Film & Video_Science Fiction  \\\n","0                            0.0                                    0.0   \n","1                            0.0                                    0.0   \n","2                            0.0                                    0.0   \n","\n","   category_Film & Video_Shorts  category_Film & Video_Television  \\\n","0                           0.0                               0.0   \n","1                           1.0                               0.0   \n","2                           0.0                               0.0   \n","\n","   category_Film & Video_Thrillers  category_Film & Video_Webseries  \\\n","0                              0.0                              0.0   \n","1                              0.0                              0.0   \n","2                              0.0                              0.0   \n","\n","   category_Food_Bacon  category_Food_Community Gardens  \\\n","0                  0.0                              0.0   \n","1                  0.0                              0.0   \n","2                  0.0                              0.0   \n","\n","   category_Food_Cookbooks  category_Food_Drinks  category_Food_Events  \\\n","0                      0.0                   0.0                   0.0   \n","1                      0.0                   0.0                   0.0   \n","2                      0.0                   0.0                   0.0   \n","\n","   category_Food_Farmer's Markets  category_Food_Farms  \\\n","0                             0.0                  0.0   \n","1                             0.0                  0.0   \n","2                             0.0                  0.0   \n","\n","   category_Food_Food Trucks  category_Food_No Subcategory  \\\n","0                        0.0                           0.0   \n","1                        0.0                           0.0   \n","2                        0.0                           0.0   \n","\n","   category_Food_Restaurants  category_Food_Small Batch  category_Food_Spaces  \\\n","0                        0.0                        0.0                   0.0   \n","1                        0.0                        0.0                   0.0   \n","2                        0.0                        0.0                   0.0   \n","\n","   category_Food_Vegan  category_Games_Gaming Hardware  \\\n","0                  0.0                             0.0   \n","1                  0.0                             0.0   \n","2                  0.0                             0.0   \n","\n","   category_Games_Live Games  category_Games_Mobile Games  \\\n","0                        0.0                          0.0   \n","1                        0.0                          0.0   \n","2                        0.0                          0.0   \n","\n","   category_Games_No Subcategory  category_Games_Playing Cards  \\\n","0                            0.0                           0.0   \n","1                            0.0                           0.0   \n","2                            0.0                           0.0   \n","\n","   category_Games_Puzzles  category_Games_Tabletop Games  \\\n","0                     0.0                            0.0   \n","1                     0.0                            0.0   \n","2                     0.0                            0.0   \n","\n","   category_Games_Video Games  category_Journalism_Audio  \\\n","0                         0.0                        0.0   \n","1                         0.0                        0.0   \n","2                         0.0                        0.0   \n","\n","   category_Journalism_No Subcategory  category_Journalism_Photo  \\\n","0                                 0.0                        0.0   \n","1                                 0.0                        0.0   \n","2                                 0.0                        0.0   \n","\n","   category_Journalism_Print  category_Journalism_Video  \\\n","0                        0.0                        0.0   \n","1                        0.0                        0.0   \n","2                        0.0                        0.0   \n","\n","   category_Journalism_Web  category_Music_Blues  category_Music_Chiptune  \\\n","0                      0.0                   0.0                      0.0   \n","1                      0.0                   0.0                      0.0   \n","2                      0.0                   0.0                      0.0   \n","\n","   category_Music_Classical Music  category_Music_Comedy  \\\n","0                             0.0                    0.0   \n","1                             0.0                    0.0   \n","2                             0.0                    0.0   \n","\n","   category_Music_Country & Folk  category_Music_Electronic Music  \\\n","0                            0.0                              0.0   \n","1                            0.0                              0.0   \n","2                            1.0                              0.0   \n","\n","   category_Music_Faith  category_Music_Hip-Hop  category_Music_Indie Rock  \\\n","0                   0.0                     0.0                        0.0   \n","1                   0.0                     0.0                        0.0   \n","2                   0.0                     0.0                        0.0   \n","\n","   category_Music_Jazz  category_Music_Kids  category_Music_Latin  \\\n","0                  0.0                  0.0                   0.0   \n","1                  0.0                  0.0                   0.0   \n","2                  0.0                  0.0                   0.0   \n","\n","   category_Music_Metal  category_Music_No Subcategory  category_Music_Pop  \\\n","0                   0.0                            0.0                 0.0   \n","1                   0.0                            0.0                 0.0   \n","2                   0.0                            0.0                 0.0   \n","\n","   category_Music_Punk  category_Music_R&B  category_Music_Rock  \\\n","0                  0.0                 0.0                  0.0   \n","1                  0.0                 0.0                  0.0   \n","2                  0.0                 0.0                  0.0   \n","\n","   category_Music_World Music  category_Photography_Animals  \\\n","0                         0.0                           0.0   \n","1                         0.0                           0.0   \n","2                         0.0                           0.0   \n","\n","   category_Photography_Fine Art  category_Photography_Nature  \\\n","0                            0.0                          0.0   \n","1                            0.0                          0.0   \n","2                            0.0                          0.0   \n","\n","   category_Photography_No Subcategory  category_Photography_People  \\\n","0                                  0.0                          0.0   \n","1                                  0.0                          0.0   \n","2                                  0.0                          0.0   \n","\n","   category_Photography_Photobooks  category_Photography_Places  \\\n","0                              0.0                          0.0   \n","1                              0.0                          0.0   \n","2                              0.0                          0.0   \n","\n","   category_Publishing_Academic  category_Publishing_Anthologies  \\\n","0                           0.0                              0.0   \n","1                           0.0                              0.0   \n","2                           0.0                              0.0   \n","\n","   category_Publishing_Art Books  category_Publishing_Calendars  \\\n","0                            0.0                            0.0   \n","1                            0.0                            0.0   \n","2                            0.0                            0.0   \n","\n","   category_Publishing_Children's Books  category_Publishing_Comedy  \\\n","0                                   0.0                         0.0   \n","1                                   0.0                         0.0   \n","2                                   0.0                         0.0   \n","\n","   category_Publishing_Fiction  category_Publishing_Letterpress  \\\n","0                          0.0                              0.0   \n","1                          0.0                              0.0   \n","2                          0.0                              0.0   \n","\n","   category_Publishing_Literary Journals  category_Publishing_Literary Spaces  \\\n","0                                    0.0                                  0.0   \n","1                                    0.0                                  0.0   \n","2                                    0.0                                  0.0   \n","\n","   category_Publishing_No Subcategory  category_Publishing_Nonfiction  \\\n","0                                 0.0                             0.0   \n","1                                 0.0                             0.0   \n","2                                 0.0                             0.0   \n","\n","   category_Publishing_Periodicals  category_Publishing_Poetry  \\\n","0                              0.0                         0.0   \n","1                              0.0                         0.0   \n","2                              0.0                         0.0   \n","\n","   category_Publishing_Radio & Podcasts  category_Publishing_Translations  \\\n","0                                   0.0                               0.0   \n","1                                   0.0                               0.0   \n","2                                   0.0                               0.0   \n","\n","   category_Publishing_Young Adult  category_Publishing_Zines  \\\n","0                              0.0                        0.0   \n","1                              0.0                        0.0   \n","2                              0.0                        0.0   \n","\n","   category_Technology_3D Printing  category_Technology_Apps  \\\n","0                              0.0                       0.0   \n","1                              0.0                       0.0   \n","2                              0.0                       0.0   \n","\n","   category_Technology_Camera Equipment  category_Technology_DIY Electronics  \\\n","0                                   0.0                                  0.0   \n","1                                   0.0                                  0.0   \n","2                                   0.0                                  0.0   \n","\n","   category_Technology_Fabrication Tools  category_Technology_Flight  \\\n","0                                    0.0                         0.0   \n","1                                    0.0                         0.0   \n","2                                    0.0                         0.0   \n","\n","   category_Technology_Gadgets  category_Technology_Hardware  \\\n","0                          0.0                           0.0   \n","1                          0.0                           0.0   \n","2                          0.0                           0.0   \n","\n","   category_Technology_Makerspaces  category_Technology_No Subcategory  \\\n","0                              0.0                                 0.0   \n","1                              0.0                                 0.0   \n","2                              0.0                                 0.0   \n","\n","   category_Technology_Robots  category_Technology_Software  \\\n","0                         0.0                           0.0   \n","1                         0.0                           0.0   \n","2                         0.0                           0.0   \n","\n","   category_Technology_Sound  category_Technology_Space Exploration  \\\n","0                        0.0                                    0.0   \n","1                        0.0                                    0.0   \n","2                        0.0                                    0.0   \n","\n","   category_Technology_Wearables  category_Technology_Web  \\\n","0                            0.0                      0.0   \n","1                            0.0                      0.0   \n","2                            0.0                      0.0   \n","\n","   category_Theater_Comedy  category_Theater_Experimental  \\\n","0                      0.0                            0.0   \n","1                      0.0                            0.0   \n","2                      0.0                            0.0   \n","\n","   category_Theater_Festivals  category_Theater_Immersive  \\\n","0                         0.0                         0.0   \n","1                         0.0                         0.0   \n","2                         0.0                         0.0   \n","\n","   category_Theater_Musical  category_Theater_No Subcategory  \\\n","0                       0.0                              0.0   \n","1                       0.0                              0.0   \n","2                       0.0                              0.0   \n","\n","   category_Theater_Plays  category_Theater_Spaces  \n","0                     0.0                      0.0  \n","1                     0.0                      0.0  \n","2                     0.0                      0.0  "]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"9lD8O0Fud4FF"},"source":["#### d) Test Different Model Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"WrJZ92GieE7O"},"source":["##### New Baseline:"]},{"cell_type":"code","metadata":{"id":"697VsbVPsMJ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611300969503,"user_tz":-60,"elapsed":21938,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"94149ec1-d51d-45f9-92cc-5d08c1f9ccfb"},"source":["model = Sequential()\n","model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=L1(l1=0.0000), kernel_initializer=GlorotNormal(seed=seed_value)))\n","model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","history = model.fit(X_subtrain, y_subtrain, validation_data=(X_val, y_val), batch_size=256, epochs=100, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","print(\"Training Accuracy: {:.3f}\".format(model.evaluate(X_subtrain, y_subtrain, batch_size=256, verbose=0)[1]))\n","print(\"Validation Accuracy: {:.3f}\".format(model.evaluate(X_val, y_val, batch_size=256, verbose=0)[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","676/676 [==============================] - 2s 2ms/step - loss: 0.5602 - binary_accuracy: 0.7040 - val_loss: 0.4566 - val_binary_accuracy: 0.7780\n","Epoch 2/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4468 - binary_accuracy: 0.7823 - val_loss: 0.4371 - val_binary_accuracy: 0.7894\n","Epoch 3/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4322 - binary_accuracy: 0.7905 - val_loss: 0.4316 - val_binary_accuracy: 0.7916\n","Epoch 4/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7924 - val_loss: 0.4297 - val_binary_accuracy: 0.7918\n","Epoch 5/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7934 - val_loss: 0.4284 - val_binary_accuracy: 0.7935\n","Epoch 6/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4244 - binary_accuracy: 0.7954 - val_loss: 0.4279 - val_binary_accuracy: 0.7934\n","Epoch 7/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7930 - val_loss: 0.4277 - val_binary_accuracy: 0.7929\n","Epoch 8/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7936 - val_loss: 0.4273 - val_binary_accuracy: 0.7938\n","Epoch 9/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7947 - val_loss: 0.4272 - val_binary_accuracy: 0.7934\n","Epoch 10/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4217 - binary_accuracy: 0.7961 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 11/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7961 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 12/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 13/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 14/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7951 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 15/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7959 - val_loss: 0.4272 - val_binary_accuracy: 0.7940\n","Epoch 16/100\n","676/676 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7946 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Restoring model weights from the end of the best epoch.\n","Epoch 00016: early stopping\n","Training Accuracy: 0.796\n","Validation Accuracy: 0.794\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SzCe_hLXerWi"},"source":["Fixed Hyperparameters:\n","- Dataset Transformation = PowerTransformed Numerical Features; No Changes at Binary Features\n","- Loss Function = Binary Crossentropy (Recommended for Binary Classification)\n","- Learning Rate / Learning Rate Annealing = not necessary to tune, since I am using only adaptive learning rate optimizers\n","- Activation = Sigmoid must be used to obtain a probability score in [0,1]\n","- Number of Epochs = can be tuned inexpensively by using Early Stopping\n","\n","Tunable Hyperparameters:\n","- Optimizers = Adadelta, RMSprop, RMSprop_centered, Adam, Adam_amsgrad, Adamax, Nadam\n","- Mini-Batch Size = typical values range between 1 and a few hundred; can be tuned independently from other hyperparameters and then be held fiex, since it mainly affects the convergence time and not the predictive performance\n","- Regularizer = L1, L2 (L2 penalizes larger weights more strongly, whereas L1 penalizes lower weights more strongly; L1 performs automatic feature selection)\n","- Regularization Rate (start with default and increase/decrease by factor 10)\n","- Weight Initializer = typically, a truncated initializer, such as Glorot, He, or LeCun, is used\n"]},{"cell_type":"markdown","metadata":{"id":"5qVnuB1FeNh8"},"source":["##### Test Different Mini-Batch Sizes:"]},{"cell_type":"code","metadata":{"id":"5jpeOis4sMF3"},"source":["# Define the logistic regression (i.e. NN without hidden layer and 1 sigmoid output neuron)\n","def logistic_regression(X_subtrain, y_subtrain, X_val, y_val, params):\n","  \n","  # Get hyperparameters from parameter_grid\n","  optimizer = params[\"optimizer\"]\n","  regularizer = params[\"regularizer\"]\n","  reg_rate = params[\"reg_rate\"]\n","  weight_initializer = params[\"weight_initializer\"]\n","  batch_size = params[\"batch_size\"]\n","  if regularizer==\"L1\": reg = L1(reg_rate)\n","  if regularizer==\"L2\": reg = L2(reg_rate)\n","  if weight_initializer==\"glorot_normal\" : init = GlorotNormal(seed=seed_value)\n","  if weight_initializer==\"glorot_uniform\" : init = GlorotUniform(seed=seed_value)\n","  if weight_initializer==\"he_normal\" : init = HeNormal(seed=seed_value)\n","  if weight_initializer==\"he_uniform\" : init = HeUniform(seed=seed_value)\n","  if weight_initializer==\"lecun_normal\" : init = LecunNormal(seed=seed_value)\n","  if weight_initializer==\"lecun_uniform\" : init = LecunUniform(seed=seed_value)\n","  if optimizer==\"Adadelta\": opt = Adadelta()\n","  if optimizer==\"RMSprop\": opt = RMSprop()\n","  if optimizer==\"RMSprop_centered\": opt = RMSprop(centered=True)\n","  if optimizer==\"Adam\": opt = Adam()\n","  if optimizer==\"Adam_amsgrad\": opt = Adam(amsgrad=True)\n","  if optimizer==\"Adamax\": opt = Adamax()\n","  if optimizer==\"Nadam\": opt = Nadam()\n","\n","  # Define Model\n","  model = Sequential()\n","  model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=reg, kernel_initializer=init))\n","\n","  # Compile Model\n","  model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","  \n","  # Fit Model to Training Data\n","  history = model.fit(X_subtrain, y_subtrain, validation_data=(X_val, y_val), batch_size=batch_size, epochs=100, shuffle=True, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","  return history, model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEExy7hpsMAP"},"source":["# Run Hyperparameter Search\n","params = {\n","    \"optimizer\" : [\"RMSprop\"],\n","    \"regularizer\" : [\"L1\"],\n","    \"reg_rate\" : [0.00],\n","    \"weight_initializer\" : [\"glorot_normal\"],\n","    \"batch_size\" : [16, 32, 64, 128, 256, 512, 1024],\n","}\n","search_object = talos.Scan(x=X_subtrain, y=y_subtrain, x_val=X_val, y_val=y_val, model=logistic_regression, params=params, experiment_name=\"test1\", seed=seed_value, print_params=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"4ijHQu8ZsL8P","executionInfo":{"status":"ok","timestamp":1611301816964,"user_tz":-60,"elapsed":811,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"3e4425ea-c6cf-4c6d-d959-ef7d399c794f"},"source":["# Get search results for all evaluated hyperparameter combinations sorted by val_accuracy\n","analyze_object = talos.Analyze(search_object)\n","analyze_object.data[[\"duration\", \"loss\", \"binary_accuracy\", \"val_loss\", \"val_binary_accuracy\", \"optimizer\", \"regularizer\", \"reg_rate\", \"weight_initializer\", \"batch_size\"]]\\\n",".sort_values(by=\"val_binary_accuracy\", ascending=False).round(4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duration</th>\n","      <th>loss</th>\n","      <th>binary_accuracy</th>\n","      <th>val_loss</th>\n","      <th>val_binary_accuracy</th>\n","      <th>optimizer</th>\n","      <th>regularizer</th>\n","      <th>reg_rate</th>\n","      <th>weight_initializer</th>\n","      <th>batch_size</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>15.7162</td>\n","      <td>0.4228</td>\n","      <td>0.7957</td>\n","      <td>0.4270</td>\n","      <td>0.7940</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>31.9643</td>\n","      <td>0.4247</td>\n","      <td>0.7964</td>\n","      <td>0.4287</td>\n","      <td>0.7939</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>41.8425</td>\n","      <td>0.4239</td>\n","      <td>0.7964</td>\n","      <td>0.4283</td>\n","      <td>0.7938</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>128</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>21.8410</td>\n","      <td>0.4229</td>\n","      <td>0.7959</td>\n","      <td>0.4273</td>\n","      <td>0.7933</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>256</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>10.8992</td>\n","      <td>0.4233</td>\n","      <td>0.7951</td>\n","      <td>0.4272</td>\n","      <td>0.7933</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>1024</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>104.9411</td>\n","      <td>0.4347</td>\n","      <td>0.7942</td>\n","      <td>0.4392</td>\n","      <td>0.7926</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>110.8820</td>\n","      <td>0.4433</td>\n","      <td>0.7920</td>\n","      <td>0.4492</td>\n","      <td>0.7909</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>16</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   duration    loss  binary_accuracy  val_loss  val_binary_accuracy optimizer  \\\n","5   15.7162  0.4228           0.7957    0.4270               0.7940   RMSprop   \n","2   31.9643  0.4247           0.7964    0.4287               0.7939   RMSprop   \n","3   41.8425  0.4239           0.7964    0.4283               0.7938   RMSprop   \n","4   21.8410  0.4229           0.7959    0.4273               0.7933   RMSprop   \n","6   10.8992  0.4233           0.7951    0.4272               0.7933   RMSprop   \n","1  104.9411  0.4347           0.7942    0.4392               0.7926   RMSprop   \n","0  110.8820  0.4433           0.7920    0.4492               0.7909   RMSprop   \n","\n","  regularizer  reg_rate weight_initializer  batch_size  \n","5          L1       0.0      glorot_normal         512  \n","2          L1       0.0      glorot_normal          64  \n","3          L1       0.0      glorot_normal         128  \n","4          L1       0.0      glorot_normal         256  \n","6          L1       0.0      glorot_normal        1024  \n","1          L1       0.0      glorot_normal          32  \n","0          L1       0.0      glorot_normal          16  "]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"od9QnNIXotEp"},"source":["Result: \n","- too small batch size (i.e. 16, 32) led to bad results + model building took too long\n","- too large batch size (i.e. 1024) led to mediocre results, but was very fast to train\n","- i.e. keep batch size of 512, as it provides the best trade-off between fast training and validation accuracy"]},{"cell_type":"markdown","metadata":{"id":"HbFa0bV7sEjg"},"source":["##### Tune different optimizers and initializers:"]},{"cell_type":"code","metadata":{"id":"SQtpZKjMsLjW"},"source":["# Define the logistic regression (i.e. NN without hidden layer and 1 sigmoid output neuron)\n","def logistic_regression(X_subtrain, y_subtrain, X_val, y_val, params):\n","  \n","  # Get hyperparameters from parameter_grid\n","  optimizer = params[\"optimizer\"]\n","  regularizer = params[\"regularizer\"]\n","  reg_rate = params[\"reg_rate\"]\n","  weight_initializer = params[\"weight_initializer\"]\n","  batch_size = params[\"batch_size\"]\n","  if regularizer==\"L1\": reg = L1(reg_rate)\n","  if regularizer==\"L2\": reg = L2(reg_rate)\n","  if weight_initializer==\"glorot_normal\" : init = GlorotNormal(seed=seed_value)\n","  if weight_initializer==\"glorot_uniform\" : init = GlorotUniform(seed=seed_value)\n","  if weight_initializer==\"he_normal\" : init = HeNormal(seed=seed_value)\n","  if weight_initializer==\"he_uniform\" : init = HeUniform(seed=seed_value)\n","  if weight_initializer==\"lecun_normal\" : init = LecunNormal(seed=seed_value)\n","  if weight_initializer==\"lecun_uniform\" : init = LecunUniform(seed=seed_value)\n","  if optimizer==\"Adadelta\": opt = Adadelta()\n","  if optimizer==\"RMSprop\": opt = RMSprop()\n","  if optimizer==\"RMSprop_centered\": opt = RMSprop(centered=True)\n","  if optimizer==\"Adam\": opt = Adam()\n","  if optimizer==\"Adam_amsgrad\": opt = Adam(amsgrad=True)\n","  if optimizer==\"Adamax\": opt = Adamax()\n","  if optimizer==\"Nadam\": opt = Nadam()\n","\n","  # Define Model\n","  model = Sequential()\n","  model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=reg, kernel_initializer=init))\n","\n","  # Compile Model\n","  model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","  \n","  # Fit Model to Training Data\n","  history = model.fit(X_subtrain, y_subtrain, validation_data=(X_val, y_val), batch_size=batch_size, epochs=100, shuffle=True, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","  return history, model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzLlPzE4sLjX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611303143966,"user_tz":-60,"elapsed":1066915,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"36ccf0d8-167b-47df-86c6-b87aa790131c"},"source":["# Run Hyperparameter Search\n","params = {\n","    \"optimizer\" : [\"Adadelta\", \"RMSprop\", \"RMSprop_centered\", \"Adam\", \"Adam_amsgrad\", \"Adamax\", \"Nadam\"],\n","    \"regularizer\" : [\"L1\"],\n","    \"reg_rate\" : [0.00],\n","    \"weight_initializer\" : [\"glorot_normal\", \"glorot_uniform\", \"he_normal\", \"he_uniform\", \"lecun_normal\", \"lecun_uniform\"],\n","    \"batch_size\" : [512],\n","}\n","search_object = talos.Scan(x=X_subtrain, y=y_subtrain, x_val=X_val, y_val=y_val, model=logistic_regression, params=params, experiment_name=\"test1\", seed=seed_value, print_params=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/42 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adadelta', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6984 - binary_accuracy: 0.5178 - val_loss: 0.6953 - val_binary_accuracy: 0.5274\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6965 - binary_accuracy: 0.5205 - val_loss: 0.6937 - val_binary_accuracy: 0.5312\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6953 - binary_accuracy: 0.5262 - val_loss: 0.6919 - val_binary_accuracy: 0.5354\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6935 - binary_accuracy: 0.5318 - val_loss: 0.6900 - val_binary_accuracy: 0.5400\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6906 - binary_accuracy: 0.5366 - val_loss: 0.6880 - val_binary_accuracy: 0.5453\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6894 - binary_accuracy: 0.5405 - val_loss: 0.6860 - val_binary_accuracy: 0.5514\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6871 - binary_accuracy: 0.5474 - val_loss: 0.6838 - val_binary_accuracy: 0.5559\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6851 - binary_accuracy: 0.5501 - val_loss: 0.6817 - val_binary_accuracy: 0.5613\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6832 - binary_accuracy: 0.5566 - val_loss: 0.6795 - val_binary_accuracy: 0.5659\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6810 - binary_accuracy: 0.5614 - val_loss: 0.6773 - val_binary_accuracy: 0.5704\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6782 - binary_accuracy: 0.5689 - val_loss: 0.6751 - val_binary_accuracy: 0.5757\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6759 - binary_accuracy: 0.5730 - val_loss: 0.6729 - val_binary_accuracy: 0.5812\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6741 - binary_accuracy: 0.5786 - val_loss: 0.6707 - val_binary_accuracy: 0.5878\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6710 - binary_accuracy: 0.5859 - val_loss: 0.6685 - val_binary_accuracy: 0.5932\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6697 - binary_accuracy: 0.5893 - val_loss: 0.6663 - val_binary_accuracy: 0.5994\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6676 - binary_accuracy: 0.5951 - val_loss: 0.6641 - val_binary_accuracy: 0.6042\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6650 - binary_accuracy: 0.6012 - val_loss: 0.6620 - val_binary_accuracy: 0.6095\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6633 - binary_accuracy: 0.6052 - val_loss: 0.6598 - val_binary_accuracy: 0.6148\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6609 - binary_accuracy: 0.6114 - val_loss: 0.6577 - val_binary_accuracy: 0.6190\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6590 - binary_accuracy: 0.6156 - val_loss: 0.6556 - val_binary_accuracy: 0.6226\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6563 - binary_accuracy: 0.6192 - val_loss: 0.6536 - val_binary_accuracy: 0.6262\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6542 - binary_accuracy: 0.6253 - val_loss: 0.6515 - val_binary_accuracy: 0.6307\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6526 - binary_accuracy: 0.6273 - val_loss: 0.6495 - val_binary_accuracy: 0.6347\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6508 - binary_accuracy: 0.6331 - val_loss: 0.6476 - val_binary_accuracy: 0.6384\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6482 - binary_accuracy: 0.6364 - val_loss: 0.6457 - val_binary_accuracy: 0.6411\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6469 - binary_accuracy: 0.6407 - val_loss: 0.6438 - val_binary_accuracy: 0.6443\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6446 - binary_accuracy: 0.6443 - val_loss: 0.6419 - val_binary_accuracy: 0.6471\n","Epoch 28/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6440 - binary_accuracy: 0.6441 - val_loss: 0.6401 - val_binary_accuracy: 0.6499\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6411 - binary_accuracy: 0.6498 - val_loss: 0.6383 - val_binary_accuracy: 0.6534\n","Epoch 30/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6389 - binary_accuracy: 0.6537 - val_loss: 0.6365 - val_binary_accuracy: 0.6563\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6376 - binary_accuracy: 0.6555 - val_loss: 0.6348 - val_binary_accuracy: 0.6590\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6357 - binary_accuracy: 0.6582 - val_loss: 0.6331 - val_binary_accuracy: 0.6620\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6335 - binary_accuracy: 0.6625 - val_loss: 0.6315 - val_binary_accuracy: 0.6638\n","Epoch 34/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6324 - binary_accuracy: 0.6643 - val_loss: 0.6299 - val_binary_accuracy: 0.6659\n","Epoch 35/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6307 - binary_accuracy: 0.6659 - val_loss: 0.6283 - val_binary_accuracy: 0.6678\n","Epoch 36/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6293 - binary_accuracy: 0.6676 - val_loss: 0.6267 - val_binary_accuracy: 0.6692\n","Epoch 37/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6274 - binary_accuracy: 0.6705 - val_loss: 0.6252 - val_binary_accuracy: 0.6709\n","Epoch 38/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6266 - binary_accuracy: 0.6697 - val_loss: 0.6236 - val_binary_accuracy: 0.6720\n","Epoch 39/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6245 - binary_accuracy: 0.6735 - val_loss: 0.6222 - val_binary_accuracy: 0.6741\n","Epoch 40/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6224 - binary_accuracy: 0.6759 - val_loss: 0.6207 - val_binary_accuracy: 0.6756\n","Epoch 41/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6224 - binary_accuracy: 0.6749 - val_loss: 0.6193 - val_binary_accuracy: 0.6772\n","Epoch 42/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6205 - binary_accuracy: 0.6767 - val_loss: 0.6179 - val_binary_accuracy: 0.6790\n","Epoch 43/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6190 - binary_accuracy: 0.6778 - val_loss: 0.6165 - val_binary_accuracy: 0.6800\n","Epoch 44/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6175 - binary_accuracy: 0.6801 - val_loss: 0.6152 - val_binary_accuracy: 0.6808\n","Epoch 45/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6159 - binary_accuracy: 0.6809 - val_loss: 0.6139 - val_binary_accuracy: 0.6818\n","Epoch 46/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6156 - binary_accuracy: 0.6805 - val_loss: 0.6126 - val_binary_accuracy: 0.6830\n","Epoch 47/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6129 - binary_accuracy: 0.6835 - val_loss: 0.6113 - val_binary_accuracy: 0.6845\n","Epoch 48/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6125 - binary_accuracy: 0.6838 - val_loss: 0.6101 - val_binary_accuracy: 0.6854\n","Epoch 49/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6113 - binary_accuracy: 0.6848 - val_loss: 0.6089 - val_binary_accuracy: 0.6861\n","Epoch 50/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6101 - binary_accuracy: 0.6857 - val_loss: 0.6077 - val_binary_accuracy: 0.6869\n","Epoch 51/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6091 - binary_accuracy: 0.6852 - val_loss: 0.6065 - val_binary_accuracy: 0.6880\n","Epoch 52/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6073 - binary_accuracy: 0.6878 - val_loss: 0.6053 - val_binary_accuracy: 0.6890\n","Epoch 53/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6061 - binary_accuracy: 0.6873 - val_loss: 0.6042 - val_binary_accuracy: 0.6900\n","Epoch 54/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6051 - binary_accuracy: 0.6896 - val_loss: 0.6031 - val_binary_accuracy: 0.6911\n","Epoch 55/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6038 - binary_accuracy: 0.6898 - val_loss: 0.6020 - val_binary_accuracy: 0.6917\n","Epoch 56/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6041 - binary_accuracy: 0.6882 - val_loss: 0.6009 - val_binary_accuracy: 0.6926\n","Epoch 57/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6026 - binary_accuracy: 0.6913 - val_loss: 0.5999 - val_binary_accuracy: 0.6930\n","Epoch 58/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6004 - binary_accuracy: 0.6932 - val_loss: 0.5989 - val_binary_accuracy: 0.6937\n","Epoch 59/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5995 - binary_accuracy: 0.6922 - val_loss: 0.5979 - val_binary_accuracy: 0.6940\n","Epoch 60/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5990 - binary_accuracy: 0.6924 - val_loss: 0.5969 - val_binary_accuracy: 0.6944\n","Epoch 61/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5982 - binary_accuracy: 0.6934 - val_loss: 0.5959 - val_binary_accuracy: 0.6947\n","Epoch 62/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5972 - binary_accuracy: 0.6941 - val_loss: 0.5949 - val_binary_accuracy: 0.6950\n","Epoch 63/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5966 - binary_accuracy: 0.6935 - val_loss: 0.5940 - val_binary_accuracy: 0.6954\n","Epoch 64/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5958 - binary_accuracy: 0.6935 - val_loss: 0.5930 - val_binary_accuracy: 0.6958\n","Epoch 65/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5937 - binary_accuracy: 0.6958 - val_loss: 0.5921 - val_binary_accuracy: 0.6960\n","Epoch 66/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5929 - binary_accuracy: 0.6950 - val_loss: 0.5912 - val_binary_accuracy: 0.6964\n","Epoch 67/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5934 - binary_accuracy: 0.6945 - val_loss: 0.5903 - val_binary_accuracy: 0.6965\n","Epoch 68/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5918 - binary_accuracy: 0.6962 - val_loss: 0.5894 - val_binary_accuracy: 0.6971\n","Epoch 69/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5908 - binary_accuracy: 0.6953 - val_loss: 0.5885 - val_binary_accuracy: 0.6976\n","Epoch 70/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5904 - binary_accuracy: 0.6961 - val_loss: 0.5877 - val_binary_accuracy: 0.6982\n","Epoch 71/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5891 - binary_accuracy: 0.6961 - val_loss: 0.5868 - val_binary_accuracy: 0.6985\n","Epoch 72/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5872 - binary_accuracy: 0.6995 - val_loss: 0.5860 - val_binary_accuracy: 0.6990\n","Epoch 73/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5876 - binary_accuracy: 0.6977 - val_loss: 0.5852 - val_binary_accuracy: 0.6995\n","Epoch 74/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5874 - binary_accuracy: 0.6970 - val_loss: 0.5844 - val_binary_accuracy: 0.6996\n","Epoch 75/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5851 - binary_accuracy: 0.6997 - val_loss: 0.5836 - val_binary_accuracy: 0.6998\n","Epoch 76/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5840 - binary_accuracy: 0.7014 - val_loss: 0.5828 - val_binary_accuracy: 0.7002\n","Epoch 77/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5838 - binary_accuracy: 0.7001 - val_loss: 0.5821 - val_binary_accuracy: 0.7004\n","Epoch 78/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5826 - binary_accuracy: 0.7012 - val_loss: 0.5813 - val_binary_accuracy: 0.7005\n","Epoch 79/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5827 - binary_accuracy: 0.7008 - val_loss: 0.5806 - val_binary_accuracy: 0.7007\n","Epoch 80/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5819 - binary_accuracy: 0.7011 - val_loss: 0.5798 - val_binary_accuracy: 0.7011\n","Epoch 81/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5807 - binary_accuracy: 0.7014 - val_loss: 0.5791 - val_binary_accuracy: 0.7016\n","Epoch 82/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5797 - binary_accuracy: 0.7014 - val_loss: 0.5784 - val_binary_accuracy: 0.7019\n","Epoch 83/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5796 - binary_accuracy: 0.7023 - val_loss: 0.5777 - val_binary_accuracy: 0.7021\n","Epoch 84/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5788 - binary_accuracy: 0.7021 - val_loss: 0.5770 - val_binary_accuracy: 0.7024\n","Epoch 85/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5784 - binary_accuracy: 0.7026 - val_loss: 0.5763 - val_binary_accuracy: 0.7025\n","Epoch 86/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5769 - binary_accuracy: 0.7039 - val_loss: 0.5756 - val_binary_accuracy: 0.7029\n","Epoch 87/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5766 - binary_accuracy: 0.7030 - val_loss: 0.5749 - val_binary_accuracy: 0.7029\n","Epoch 88/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5759 - binary_accuracy: 0.7029 - val_loss: 0.5742 - val_binary_accuracy: 0.7032\n","Epoch 89/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5768 - binary_accuracy: 0.7017 - val_loss: 0.5736 - val_binary_accuracy: 0.7034\n","Epoch 90/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5747 - binary_accuracy: 0.7045 - val_loss: 0.5729 - val_binary_accuracy: 0.7036\n","Epoch 91/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5741 - binary_accuracy: 0.7045 - val_loss: 0.5723 - val_binary_accuracy: 0.7038\n","Epoch 92/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5728 - binary_accuracy: 0.7054 - val_loss: 0.5716 - val_binary_accuracy: 0.7039\n","Epoch 93/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5716 - binary_accuracy: 0.7074 - val_loss: 0.5710 - val_binary_accuracy: 0.7042\n","Epoch 94/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5727 - binary_accuracy: 0.7054 - val_loss: 0.5704 - val_binary_accuracy: 0.7045\n","Epoch 95/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5716 - binary_accuracy: 0.7044 - val_loss: 0.5698 - val_binary_accuracy: 0.7047\n","Epoch 96/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5716 - binary_accuracy: 0.7048 - val_loss: 0.5692 - val_binary_accuracy: 0.7049\n","Epoch 97/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5690 - binary_accuracy: 0.7064 - val_loss: 0.5685 - val_binary_accuracy: 0.7054\n","Epoch 98/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5690 - binary_accuracy: 0.7070 - val_loss: 0.5680 - val_binary_accuracy: 0.7058\n","Epoch 99/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5683 - binary_accuracy: 0.7078 - val_loss: 0.5674 - val_binary_accuracy: 0.7059\n","Epoch 100/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5680 - binary_accuracy: 0.7084 - val_loss: 0.5668 - val_binary_accuracy: 0.7062\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|         | 1/42 [01:18<53:21, 78.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adadelta', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6818 - binary_accuracy: 0.5836 - val_loss: 0.6818 - val_binary_accuracy: 0.5862\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6805 - binary_accuracy: 0.5874 - val_loss: 0.6807 - val_binary_accuracy: 0.5885\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6802 - binary_accuracy: 0.5876 - val_loss: 0.6795 - val_binary_accuracy: 0.5911\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6795 - binary_accuracy: 0.5885 - val_loss: 0.6783 - val_binary_accuracy: 0.5932\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6787 - binary_accuracy: 0.5902 - val_loss: 0.6770 - val_binary_accuracy: 0.5960\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6777 - binary_accuracy: 0.5909 - val_loss: 0.6756 - val_binary_accuracy: 0.5987\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6752 - binary_accuracy: 0.5968 - val_loss: 0.6743 - val_binary_accuracy: 0.6014\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6742 - binary_accuracy: 0.5971 - val_loss: 0.6729 - val_binary_accuracy: 0.6034\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6717 - binary_accuracy: 0.6016 - val_loss: 0.6715 - val_binary_accuracy: 0.6060\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6708 - binary_accuracy: 0.6026 - val_loss: 0.6701 - val_binary_accuracy: 0.6082\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6701 - binary_accuracy: 0.6038 - val_loss: 0.6687 - val_binary_accuracy: 0.6104\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6689 - binary_accuracy: 0.6062 - val_loss: 0.6673 - val_binary_accuracy: 0.6129\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6673 - binary_accuracy: 0.6078 - val_loss: 0.6659 - val_binary_accuracy: 0.6152\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6654 - binary_accuracy: 0.6110 - val_loss: 0.6645 - val_binary_accuracy: 0.6174\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6634 - binary_accuracy: 0.6132 - val_loss: 0.6631 - val_binary_accuracy: 0.6187\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6626 - binary_accuracy: 0.6143 - val_loss: 0.6617 - val_binary_accuracy: 0.6207\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6611 - binary_accuracy: 0.6159 - val_loss: 0.6603 - val_binary_accuracy: 0.6224\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6610 - binary_accuracy: 0.6164 - val_loss: 0.6589 - val_binary_accuracy: 0.6241\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6586 - binary_accuracy: 0.6199 - val_loss: 0.6575 - val_binary_accuracy: 0.6256\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6563 - binary_accuracy: 0.6232 - val_loss: 0.6561 - val_binary_accuracy: 0.6274\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6556 - binary_accuracy: 0.6231 - val_loss: 0.6548 - val_binary_accuracy: 0.6288\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6537 - binary_accuracy: 0.6256 - val_loss: 0.6534 - val_binary_accuracy: 0.6303\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6527 - binary_accuracy: 0.6274 - val_loss: 0.6521 - val_binary_accuracy: 0.6314\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6522 - binary_accuracy: 0.6259 - val_loss: 0.6507 - val_binary_accuracy: 0.6322\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6492 - binary_accuracy: 0.6309 - val_loss: 0.6494 - val_binary_accuracy: 0.6336\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6489 - binary_accuracy: 0.6293 - val_loss: 0.6481 - val_binary_accuracy: 0.6349\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6481 - binary_accuracy: 0.6315 - val_loss: 0.6468 - val_binary_accuracy: 0.6364\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6475 - binary_accuracy: 0.6307 - val_loss: 0.6455 - val_binary_accuracy: 0.6373\n","Epoch 29/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6464 - binary_accuracy: 0.6316 - val_loss: 0.6442 - val_binary_accuracy: 0.6378\n","Epoch 30/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6441 - binary_accuracy: 0.6339 - val_loss: 0.6430 - val_binary_accuracy: 0.6388\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6433 - binary_accuracy: 0.6348 - val_loss: 0.6417 - val_binary_accuracy: 0.6394\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6415 - binary_accuracy: 0.6370 - val_loss: 0.6405 - val_binary_accuracy: 0.6408\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6404 - binary_accuracy: 0.6375 - val_loss: 0.6393 - val_binary_accuracy: 0.6419\n","Epoch 34/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6404 - binary_accuracy: 0.6357 - val_loss: 0.6381 - val_binary_accuracy: 0.6427\n","Epoch 35/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6385 - binary_accuracy: 0.6381 - val_loss: 0.6369 - val_binary_accuracy: 0.6439\n","Epoch 36/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6371 - binary_accuracy: 0.6401 - val_loss: 0.6357 - val_binary_accuracy: 0.6444\n","Epoch 37/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6361 - binary_accuracy: 0.6423 - val_loss: 0.6345 - val_binary_accuracy: 0.6456\n","Epoch 38/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6333 - binary_accuracy: 0.6435 - val_loss: 0.6333 - val_binary_accuracy: 0.6464\n","Epoch 39/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6327 - binary_accuracy: 0.6439 - val_loss: 0.6321 - val_binary_accuracy: 0.6473\n","Epoch 40/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6314 - binary_accuracy: 0.6449 - val_loss: 0.6310 - val_binary_accuracy: 0.6481\n","Epoch 41/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6309 - binary_accuracy: 0.6441 - val_loss: 0.6298 - val_binary_accuracy: 0.6492\n","Epoch 42/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6297 - binary_accuracy: 0.6442 - val_loss: 0.6287 - val_binary_accuracy: 0.6500\n","Epoch 43/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6293 - binary_accuracy: 0.6442 - val_loss: 0.6276 - val_binary_accuracy: 0.6509\n","Epoch 44/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6279 - binary_accuracy: 0.6459 - val_loss: 0.6265 - val_binary_accuracy: 0.6515\n","Epoch 45/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6262 - binary_accuracy: 0.6479 - val_loss: 0.6254 - val_binary_accuracy: 0.6520\n","Epoch 46/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6262 - binary_accuracy: 0.6473 - val_loss: 0.6243 - val_binary_accuracy: 0.6528\n","Epoch 47/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6248 - binary_accuracy: 0.6482 - val_loss: 0.6232 - val_binary_accuracy: 0.6534\n","Epoch 48/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6229 - binary_accuracy: 0.6513 - val_loss: 0.6221 - val_binary_accuracy: 0.6540\n","Epoch 49/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6225 - binary_accuracy: 0.6489 - val_loss: 0.6211 - val_binary_accuracy: 0.6547\n","Epoch 50/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6208 - binary_accuracy: 0.6517 - val_loss: 0.6200 - val_binary_accuracy: 0.6552\n","Epoch 51/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6210 - binary_accuracy: 0.6501 - val_loss: 0.6189 - val_binary_accuracy: 0.6562\n","Epoch 52/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6193 - binary_accuracy: 0.6512 - val_loss: 0.6179 - val_binary_accuracy: 0.6568\n","Epoch 53/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6186 - binary_accuracy: 0.6516 - val_loss: 0.6169 - val_binary_accuracy: 0.6576\n","Epoch 54/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6165 - binary_accuracy: 0.6543 - val_loss: 0.6159 - val_binary_accuracy: 0.6583\n","Epoch 55/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6156 - binary_accuracy: 0.6545 - val_loss: 0.6149 - val_binary_accuracy: 0.6590\n","Epoch 56/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6152 - binary_accuracy: 0.6541 - val_loss: 0.6139 - val_binary_accuracy: 0.6597\n","Epoch 57/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6146 - binary_accuracy: 0.6536 - val_loss: 0.6129 - val_binary_accuracy: 0.6603\n","Epoch 58/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6125 - binary_accuracy: 0.6572 - val_loss: 0.6119 - val_binary_accuracy: 0.6611\n","Epoch 59/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6122 - binary_accuracy: 0.6568 - val_loss: 0.6110 - val_binary_accuracy: 0.6617\n","Epoch 60/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6108 - binary_accuracy: 0.6573 - val_loss: 0.6100 - val_binary_accuracy: 0.6623\n","Epoch 61/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6112 - binary_accuracy: 0.6566 - val_loss: 0.6091 - val_binary_accuracy: 0.6631\n","Epoch 62/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6101 - binary_accuracy: 0.6574 - val_loss: 0.6081 - val_binary_accuracy: 0.6638\n","Epoch 63/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6090 - binary_accuracy: 0.6581 - val_loss: 0.6072 - val_binary_accuracy: 0.6643\n","Epoch 64/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6075 - binary_accuracy: 0.6600 - val_loss: 0.6063 - val_binary_accuracy: 0.6649\n","Epoch 65/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6061 - binary_accuracy: 0.6608 - val_loss: 0.6054 - val_binary_accuracy: 0.6654\n","Epoch 66/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6052 - binary_accuracy: 0.6620 - val_loss: 0.6044 - val_binary_accuracy: 0.6658\n","Epoch 67/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6056 - binary_accuracy: 0.6593 - val_loss: 0.6035 - val_binary_accuracy: 0.6665\n","Epoch 68/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6050 - binary_accuracy: 0.6609 - val_loss: 0.6026 - val_binary_accuracy: 0.6669\n","Epoch 69/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6028 - binary_accuracy: 0.6621 - val_loss: 0.6018 - val_binary_accuracy: 0.6674\n","Epoch 70/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6015 - binary_accuracy: 0.6648 - val_loss: 0.6009 - val_binary_accuracy: 0.6678\n","Epoch 71/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6013 - binary_accuracy: 0.6646 - val_loss: 0.6000 - val_binary_accuracy: 0.6686\n","Epoch 72/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5996 - binary_accuracy: 0.6662 - val_loss: 0.5991 - val_binary_accuracy: 0.6694\n","Epoch 73/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6001 - binary_accuracy: 0.6646 - val_loss: 0.5983 - val_binary_accuracy: 0.6697\n","Epoch 74/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5998 - binary_accuracy: 0.6641 - val_loss: 0.5974 - val_binary_accuracy: 0.6701\n","Epoch 75/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5976 - binary_accuracy: 0.6672 - val_loss: 0.5966 - val_binary_accuracy: 0.6706\n","Epoch 76/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5962 - binary_accuracy: 0.6676 - val_loss: 0.5958 - val_binary_accuracy: 0.6709\n","Epoch 77/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5957 - binary_accuracy: 0.6686 - val_loss: 0.5950 - val_binary_accuracy: 0.6714\n","Epoch 78/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5943 - binary_accuracy: 0.6699 - val_loss: 0.5941 - val_binary_accuracy: 0.6718\n","Epoch 79/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5945 - binary_accuracy: 0.6687 - val_loss: 0.5933 - val_binary_accuracy: 0.6726\n","Epoch 80/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5928 - binary_accuracy: 0.6709 - val_loss: 0.5925 - val_binary_accuracy: 0.6733\n","Epoch 81/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5940 - binary_accuracy: 0.6687 - val_loss: 0.5918 - val_binary_accuracy: 0.6738\n","Epoch 82/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5920 - binary_accuracy: 0.6702 - val_loss: 0.5910 - val_binary_accuracy: 0.6746\n","Epoch 83/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5912 - binary_accuracy: 0.6722 - val_loss: 0.5902 - val_binary_accuracy: 0.6752\n","Epoch 84/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5913 - binary_accuracy: 0.6700 - val_loss: 0.5894 - val_binary_accuracy: 0.6759\n","Epoch 85/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5908 - binary_accuracy: 0.6710 - val_loss: 0.5887 - val_binary_accuracy: 0.6765\n","Epoch 86/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5885 - binary_accuracy: 0.6732 - val_loss: 0.5879 - val_binary_accuracy: 0.6771\n","Epoch 87/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5888 - binary_accuracy: 0.6737 - val_loss: 0.5871 - val_binary_accuracy: 0.6774\n","Epoch 88/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5880 - binary_accuracy: 0.6739 - val_loss: 0.5864 - val_binary_accuracy: 0.6779\n","Epoch 89/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5875 - binary_accuracy: 0.6733 - val_loss: 0.5857 - val_binary_accuracy: 0.6786\n","Epoch 90/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5866 - binary_accuracy: 0.6751 - val_loss: 0.5849 - val_binary_accuracy: 0.6793\n","Epoch 91/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5855 - binary_accuracy: 0.6746 - val_loss: 0.5842 - val_binary_accuracy: 0.6797\n","Epoch 92/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5844 - binary_accuracy: 0.6760 - val_loss: 0.5835 - val_binary_accuracy: 0.6805\n","Epoch 93/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5828 - binary_accuracy: 0.6783 - val_loss: 0.5828 - val_binary_accuracy: 0.6810\n","Epoch 94/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5833 - binary_accuracy: 0.6777 - val_loss: 0.5821 - val_binary_accuracy: 0.6819\n","Epoch 95/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5824 - binary_accuracy: 0.6775 - val_loss: 0.5813 - val_binary_accuracy: 0.6822\n","Epoch 96/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5819 - binary_accuracy: 0.6778 - val_loss: 0.5806 - val_binary_accuracy: 0.6828\n","Epoch 97/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5802 - binary_accuracy: 0.6816 - val_loss: 0.5800 - val_binary_accuracy: 0.6832\n","Epoch 98/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5796 - binary_accuracy: 0.6813 - val_loss: 0.5793 - val_binary_accuracy: 0.6837\n","Epoch 99/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5792 - binary_accuracy: 0.6814 - val_loss: 0.5786 - val_binary_accuracy: 0.6842\n","Epoch 100/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5779 - binary_accuracy: 0.6816 - val_loss: 0.5779 - val_binary_accuracy: 0.6847\n"],"name":"stdout"},{"output_type":"stream","text":["\r  5%|         | 2/42 [02:36<52:04, 78.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adadelta', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6984 - binary_accuracy: 0.5178 - val_loss: 0.6953 - val_binary_accuracy: 0.5274\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6965 - binary_accuracy: 0.5205 - val_loss: 0.6937 - val_binary_accuracy: 0.5311\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6954 - binary_accuracy: 0.5262 - val_loss: 0.6920 - val_binary_accuracy: 0.5354\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6936 - binary_accuracy: 0.5317 - val_loss: 0.6901 - val_binary_accuracy: 0.5399\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6907 - binary_accuracy: 0.5365 - val_loss: 0.6881 - val_binary_accuracy: 0.5452\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6894 - binary_accuracy: 0.5405 - val_loss: 0.6860 - val_binary_accuracy: 0.5514\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6871 - binary_accuracy: 0.5474 - val_loss: 0.6839 - val_binary_accuracy: 0.5558\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6851 - binary_accuracy: 0.5500 - val_loss: 0.6817 - val_binary_accuracy: 0.5612\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6832 - binary_accuracy: 0.5566 - val_loss: 0.6795 - val_binary_accuracy: 0.5659\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6810 - binary_accuracy: 0.5613 - val_loss: 0.6773 - val_binary_accuracy: 0.5702\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6783 - binary_accuracy: 0.5688 - val_loss: 0.6751 - val_binary_accuracy: 0.5757\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6759 - binary_accuracy: 0.5728 - val_loss: 0.6729 - val_binary_accuracy: 0.5812\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6741 - binary_accuracy: 0.5785 - val_loss: 0.6707 - val_binary_accuracy: 0.5876\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6710 - binary_accuracy: 0.5859 - val_loss: 0.6685 - val_binary_accuracy: 0.5930\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6698 - binary_accuracy: 0.5890 - val_loss: 0.6663 - val_binary_accuracy: 0.5993\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6677 - binary_accuracy: 0.5950 - val_loss: 0.6641 - val_binary_accuracy: 0.6040\n","Epoch 17/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6651 - binary_accuracy: 0.6011 - val_loss: 0.6620 - val_binary_accuracy: 0.6091\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6634 - binary_accuracy: 0.6051 - val_loss: 0.6598 - val_binary_accuracy: 0.6145\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6609 - binary_accuracy: 0.6112 - val_loss: 0.6577 - val_binary_accuracy: 0.6188\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6591 - binary_accuracy: 0.6155 - val_loss: 0.6556 - val_binary_accuracy: 0.6225\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6564 - binary_accuracy: 0.6189 - val_loss: 0.6536 - val_binary_accuracy: 0.6259\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6542 - binary_accuracy: 0.6253 - val_loss: 0.6516 - val_binary_accuracy: 0.6307\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6526 - binary_accuracy: 0.6271 - val_loss: 0.6496 - val_binary_accuracy: 0.6343\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6508 - binary_accuracy: 0.6330 - val_loss: 0.6476 - val_binary_accuracy: 0.6382\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6482 - binary_accuracy: 0.6362 - val_loss: 0.6457 - val_binary_accuracy: 0.6410\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6469 - binary_accuracy: 0.6405 - val_loss: 0.6438 - val_binary_accuracy: 0.6440\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6446 - binary_accuracy: 0.6441 - val_loss: 0.6419 - val_binary_accuracy: 0.6469\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6441 - binary_accuracy: 0.6439 - val_loss: 0.6401 - val_binary_accuracy: 0.6500\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6411 - binary_accuracy: 0.6496 - val_loss: 0.6383 - val_binary_accuracy: 0.6532\n","Epoch 30/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6390 - binary_accuracy: 0.6536 - val_loss: 0.6366 - val_binary_accuracy: 0.6563\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6377 - binary_accuracy: 0.6555 - val_loss: 0.6348 - val_binary_accuracy: 0.6588\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6358 - binary_accuracy: 0.6581 - val_loss: 0.6332 - val_binary_accuracy: 0.6617\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6335 - binary_accuracy: 0.6624 - val_loss: 0.6315 - val_binary_accuracy: 0.6638\n","Epoch 34/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6324 - binary_accuracy: 0.6642 - val_loss: 0.6299 - val_binary_accuracy: 0.6658\n","Epoch 35/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6308 - binary_accuracy: 0.6658 - val_loss: 0.6283 - val_binary_accuracy: 0.6675\n","Epoch 36/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6293 - binary_accuracy: 0.6675 - val_loss: 0.6267 - val_binary_accuracy: 0.6692\n","Epoch 37/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6274 - binary_accuracy: 0.6703 - val_loss: 0.6252 - val_binary_accuracy: 0.6707\n","Epoch 38/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6266 - binary_accuracy: 0.6695 - val_loss: 0.6237 - val_binary_accuracy: 0.6718\n","Epoch 39/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6245 - binary_accuracy: 0.6733 - val_loss: 0.6222 - val_binary_accuracy: 0.6740\n","Epoch 40/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6225 - binary_accuracy: 0.6759 - val_loss: 0.6207 - val_binary_accuracy: 0.6754\n","Epoch 41/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6224 - binary_accuracy: 0.6747 - val_loss: 0.6193 - val_binary_accuracy: 0.6771\n","Epoch 42/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6205 - binary_accuracy: 0.6765 - val_loss: 0.6179 - val_binary_accuracy: 0.6788\n","Epoch 43/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6190 - binary_accuracy: 0.6776 - val_loss: 0.6166 - val_binary_accuracy: 0.6798\n","Epoch 44/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6175 - binary_accuracy: 0.6801 - val_loss: 0.6152 - val_binary_accuracy: 0.6807\n","Epoch 45/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6160 - binary_accuracy: 0.6808 - val_loss: 0.6139 - val_binary_accuracy: 0.6815\n","Epoch 46/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6157 - binary_accuracy: 0.6803 - val_loss: 0.6126 - val_binary_accuracy: 0.6828\n","Epoch 47/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6130 - binary_accuracy: 0.6833 - val_loss: 0.6114 - val_binary_accuracy: 0.6844\n","Epoch 48/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6125 - binary_accuracy: 0.6837 - val_loss: 0.6101 - val_binary_accuracy: 0.6854\n","Epoch 49/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6113 - binary_accuracy: 0.6847 - val_loss: 0.6089 - val_binary_accuracy: 0.6860\n","Epoch 50/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6102 - binary_accuracy: 0.6856 - val_loss: 0.6077 - val_binary_accuracy: 0.6868\n","Epoch 51/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6091 - binary_accuracy: 0.6852 - val_loss: 0.6065 - val_binary_accuracy: 0.6879\n","Epoch 52/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6073 - binary_accuracy: 0.6877 - val_loss: 0.6054 - val_binary_accuracy: 0.6890\n","Epoch 53/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6061 - binary_accuracy: 0.6873 - val_loss: 0.6042 - val_binary_accuracy: 0.6898\n","Epoch 54/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6051 - binary_accuracy: 0.6895 - val_loss: 0.6031 - val_binary_accuracy: 0.6912\n","Epoch 55/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6038 - binary_accuracy: 0.6898 - val_loss: 0.6020 - val_binary_accuracy: 0.6917\n","Epoch 56/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6041 - binary_accuracy: 0.6882 - val_loss: 0.6010 - val_binary_accuracy: 0.6926\n","Epoch 57/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6026 - binary_accuracy: 0.6912 - val_loss: 0.5999 - val_binary_accuracy: 0.6932\n","Epoch 58/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6004 - binary_accuracy: 0.6931 - val_loss: 0.5989 - val_binary_accuracy: 0.6936\n","Epoch 59/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5995 - binary_accuracy: 0.6922 - val_loss: 0.5979 - val_binary_accuracy: 0.6939\n","Epoch 60/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5990 - binary_accuracy: 0.6923 - val_loss: 0.5969 - val_binary_accuracy: 0.6944\n","Epoch 61/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5983 - binary_accuracy: 0.6933 - val_loss: 0.5959 - val_binary_accuracy: 0.6946\n","Epoch 62/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5973 - binary_accuracy: 0.6940 - val_loss: 0.5949 - val_binary_accuracy: 0.6949\n","Epoch 63/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5966 - binary_accuracy: 0.6934 - val_loss: 0.5940 - val_binary_accuracy: 0.6954\n","Epoch 64/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5958 - binary_accuracy: 0.6935 - val_loss: 0.5930 - val_binary_accuracy: 0.6957\n","Epoch 65/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5937 - binary_accuracy: 0.6958 - val_loss: 0.5921 - val_binary_accuracy: 0.6961\n","Epoch 66/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5929 - binary_accuracy: 0.6949 - val_loss: 0.5912 - val_binary_accuracy: 0.6964\n","Epoch 67/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5935 - binary_accuracy: 0.6945 - val_loss: 0.5903 - val_binary_accuracy: 0.6966\n","Epoch 68/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5918 - binary_accuracy: 0.6962 - val_loss: 0.5894 - val_binary_accuracy: 0.6971\n","Epoch 69/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5908 - binary_accuracy: 0.6953 - val_loss: 0.5886 - val_binary_accuracy: 0.6974\n","Epoch 70/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5904 - binary_accuracy: 0.6961 - val_loss: 0.5877 - val_binary_accuracy: 0.6981\n","Epoch 71/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5891 - binary_accuracy: 0.6960 - val_loss: 0.5869 - val_binary_accuracy: 0.6984\n","Epoch 72/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5872 - binary_accuracy: 0.6996 - val_loss: 0.5860 - val_binary_accuracy: 0.6992\n","Epoch 73/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5876 - binary_accuracy: 0.6977 - val_loss: 0.5852 - val_binary_accuracy: 0.6993\n","Epoch 74/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5874 - binary_accuracy: 0.6970 - val_loss: 0.5844 - val_binary_accuracy: 0.6995\n","Epoch 75/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5851 - binary_accuracy: 0.6996 - val_loss: 0.5836 - val_binary_accuracy: 0.7000\n","Epoch 76/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5840 - binary_accuracy: 0.7013 - val_loss: 0.5828 - val_binary_accuracy: 0.7001\n","Epoch 77/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5838 - binary_accuracy: 0.7001 - val_loss: 0.5821 - val_binary_accuracy: 0.7005\n","Epoch 78/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5826 - binary_accuracy: 0.7012 - val_loss: 0.5813 - val_binary_accuracy: 0.7005\n","Epoch 79/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5828 - binary_accuracy: 0.7008 - val_loss: 0.5806 - val_binary_accuracy: 0.7008\n","Epoch 80/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5819 - binary_accuracy: 0.7011 - val_loss: 0.5798 - val_binary_accuracy: 0.7011\n","Epoch 81/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5807 - binary_accuracy: 0.7015 - val_loss: 0.5791 - val_binary_accuracy: 0.7016\n","Epoch 82/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5797 - binary_accuracy: 0.7013 - val_loss: 0.5784 - val_binary_accuracy: 0.7018\n","Epoch 83/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5796 - binary_accuracy: 0.7022 - val_loss: 0.5777 - val_binary_accuracy: 0.7020\n","Epoch 84/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5788 - binary_accuracy: 0.7022 - val_loss: 0.5770 - val_binary_accuracy: 0.7023\n","Epoch 85/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5784 - binary_accuracy: 0.7025 - val_loss: 0.5763 - val_binary_accuracy: 0.7025\n","Epoch 86/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5769 - binary_accuracy: 0.7039 - val_loss: 0.5756 - val_binary_accuracy: 0.7029\n","Epoch 87/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5767 - binary_accuracy: 0.7030 - val_loss: 0.5749 - val_binary_accuracy: 0.7030\n","Epoch 88/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5760 - binary_accuracy: 0.7028 - val_loss: 0.5743 - val_binary_accuracy: 0.7032\n","Epoch 89/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5768 - binary_accuracy: 0.7017 - val_loss: 0.5736 - val_binary_accuracy: 0.7034\n","Epoch 90/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5747 - binary_accuracy: 0.7044 - val_loss: 0.5729 - val_binary_accuracy: 0.7036\n","Epoch 91/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5741 - binary_accuracy: 0.7044 - val_loss: 0.5723 - val_binary_accuracy: 0.7038\n","Epoch 92/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5728 - binary_accuracy: 0.7054 - val_loss: 0.5717 - val_binary_accuracy: 0.7038\n","Epoch 93/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5716 - binary_accuracy: 0.7074 - val_loss: 0.5710 - val_binary_accuracy: 0.7042\n","Epoch 94/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5727 - binary_accuracy: 0.7053 - val_loss: 0.5704 - val_binary_accuracy: 0.7045\n","Epoch 95/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5716 - binary_accuracy: 0.7044 - val_loss: 0.5698 - val_binary_accuracy: 0.7046\n","Epoch 96/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5716 - binary_accuracy: 0.7048 - val_loss: 0.5692 - val_binary_accuracy: 0.7049\n","Epoch 97/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5690 - binary_accuracy: 0.7064 - val_loss: 0.5686 - val_binary_accuracy: 0.7053\n","Epoch 98/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5690 - binary_accuracy: 0.7069 - val_loss: 0.5680 - val_binary_accuracy: 0.7058\n","Epoch 99/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5683 - binary_accuracy: 0.7078 - val_loss: 0.5674 - val_binary_accuracy: 0.7058\n","Epoch 100/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5680 - binary_accuracy: 0.7084 - val_loss: 0.5668 - val_binary_accuracy: 0.7061\n"],"name":"stdout"},{"output_type":"stream","text":["\r  7%|         | 3/42 [03:55<50:54, 78.32s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adadelta', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6818 - binary_accuracy: 0.5836 - val_loss: 0.6818 - val_binary_accuracy: 0.5862\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6805 - binary_accuracy: 0.5874 - val_loss: 0.6807 - val_binary_accuracy: 0.5884\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6802 - binary_accuracy: 0.5876 - val_loss: 0.6795 - val_binary_accuracy: 0.5911\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6795 - binary_accuracy: 0.5885 - val_loss: 0.6783 - val_binary_accuracy: 0.5932\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6787 - binary_accuracy: 0.5902 - val_loss: 0.6770 - val_binary_accuracy: 0.5960\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6777 - binary_accuracy: 0.5909 - val_loss: 0.6757 - val_binary_accuracy: 0.5987\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6753 - binary_accuracy: 0.5968 - val_loss: 0.6743 - val_binary_accuracy: 0.6013\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6743 - binary_accuracy: 0.5971 - val_loss: 0.6729 - val_binary_accuracy: 0.6034\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6717 - binary_accuracy: 0.6016 - val_loss: 0.6716 - val_binary_accuracy: 0.6058\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6709 - binary_accuracy: 0.6025 - val_loss: 0.6702 - val_binary_accuracy: 0.6082\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6701 - binary_accuracy: 0.6038 - val_loss: 0.6688 - val_binary_accuracy: 0.6103\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6690 - binary_accuracy: 0.6061 - val_loss: 0.6673 - val_binary_accuracy: 0.6128\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6673 - binary_accuracy: 0.6078 - val_loss: 0.6659 - val_binary_accuracy: 0.6153\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6655 - binary_accuracy: 0.6110 - val_loss: 0.6645 - val_binary_accuracy: 0.6173\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6634 - binary_accuracy: 0.6132 - val_loss: 0.6631 - val_binary_accuracy: 0.6186\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6627 - binary_accuracy: 0.6143 - val_loss: 0.6617 - val_binary_accuracy: 0.6206\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6611 - binary_accuracy: 0.6158 - val_loss: 0.6603 - val_binary_accuracy: 0.6224\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6610 - binary_accuracy: 0.6164 - val_loss: 0.6589 - val_binary_accuracy: 0.6240\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6586 - binary_accuracy: 0.6198 - val_loss: 0.6575 - val_binary_accuracy: 0.6256\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6563 - binary_accuracy: 0.6231 - val_loss: 0.6562 - val_binary_accuracy: 0.6274\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6556 - binary_accuracy: 0.6231 - val_loss: 0.6548 - val_binary_accuracy: 0.6288\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6537 - binary_accuracy: 0.6255 - val_loss: 0.6534 - val_binary_accuracy: 0.6302\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6527 - binary_accuracy: 0.6272 - val_loss: 0.6521 - val_binary_accuracy: 0.6313\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6523 - binary_accuracy: 0.6258 - val_loss: 0.6508 - val_binary_accuracy: 0.6323\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6492 - binary_accuracy: 0.6308 - val_loss: 0.6495 - val_binary_accuracy: 0.6335\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6489 - binary_accuracy: 0.6292 - val_loss: 0.6481 - val_binary_accuracy: 0.6348\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6481 - binary_accuracy: 0.6315 - val_loss: 0.6468 - val_binary_accuracy: 0.6363\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6476 - binary_accuracy: 0.6306 - val_loss: 0.6456 - val_binary_accuracy: 0.6372\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6465 - binary_accuracy: 0.6315 - val_loss: 0.6443 - val_binary_accuracy: 0.6378\n","Epoch 30/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6441 - binary_accuracy: 0.6338 - val_loss: 0.6430 - val_binary_accuracy: 0.6387\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6433 - binary_accuracy: 0.6347 - val_loss: 0.6418 - val_binary_accuracy: 0.6394\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6416 - binary_accuracy: 0.6370 - val_loss: 0.6406 - val_binary_accuracy: 0.6407\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6404 - binary_accuracy: 0.6375 - val_loss: 0.6393 - val_binary_accuracy: 0.6418\n","Epoch 34/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6405 - binary_accuracy: 0.6356 - val_loss: 0.6381 - val_binary_accuracy: 0.6426\n","Epoch 35/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6385 - binary_accuracy: 0.6380 - val_loss: 0.6369 - val_binary_accuracy: 0.6437\n","Epoch 36/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6371 - binary_accuracy: 0.6399 - val_loss: 0.6357 - val_binary_accuracy: 0.6445\n","Epoch 37/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6361 - binary_accuracy: 0.6423 - val_loss: 0.6345 - val_binary_accuracy: 0.6455\n","Epoch 38/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6333 - binary_accuracy: 0.6435 - val_loss: 0.6334 - val_binary_accuracy: 0.6462\n","Epoch 39/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6328 - binary_accuracy: 0.6439 - val_loss: 0.6322 - val_binary_accuracy: 0.6473\n","Epoch 40/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6314 - binary_accuracy: 0.6449 - val_loss: 0.6310 - val_binary_accuracy: 0.6481\n","Epoch 41/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6310 - binary_accuracy: 0.6441 - val_loss: 0.6299 - val_binary_accuracy: 0.6492\n","Epoch 42/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6298 - binary_accuracy: 0.6441 - val_loss: 0.6288 - val_binary_accuracy: 0.6499\n","Epoch 43/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6293 - binary_accuracy: 0.6442 - val_loss: 0.6276 - val_binary_accuracy: 0.6509\n","Epoch 44/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6280 - binary_accuracy: 0.6459 - val_loss: 0.6265 - val_binary_accuracy: 0.6513\n","Epoch 45/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6263 - binary_accuracy: 0.6479 - val_loss: 0.6254 - val_binary_accuracy: 0.6519\n","Epoch 46/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6263 - binary_accuracy: 0.6473 - val_loss: 0.6243 - val_binary_accuracy: 0.6527\n","Epoch 47/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6248 - binary_accuracy: 0.6482 - val_loss: 0.6232 - val_binary_accuracy: 0.6532\n","Epoch 48/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6229 - binary_accuracy: 0.6512 - val_loss: 0.6222 - val_binary_accuracy: 0.6539\n","Epoch 49/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6226 - binary_accuracy: 0.6488 - val_loss: 0.6211 - val_binary_accuracy: 0.6546\n","Epoch 50/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6209 - binary_accuracy: 0.6518 - val_loss: 0.6200 - val_binary_accuracy: 0.6552\n","Epoch 51/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6211 - binary_accuracy: 0.6500 - val_loss: 0.6190 - val_binary_accuracy: 0.6561\n","Epoch 52/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6194 - binary_accuracy: 0.6511 - val_loss: 0.6180 - val_binary_accuracy: 0.6568\n","Epoch 53/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6186 - binary_accuracy: 0.6515 - val_loss: 0.6169 - val_binary_accuracy: 0.6575\n","Epoch 54/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6165 - binary_accuracy: 0.6543 - val_loss: 0.6159 - val_binary_accuracy: 0.6583\n","Epoch 55/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6156 - binary_accuracy: 0.6544 - val_loss: 0.6149 - val_binary_accuracy: 0.6590\n","Epoch 56/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6153 - binary_accuracy: 0.6541 - val_loss: 0.6139 - val_binary_accuracy: 0.6595\n","Epoch 57/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6147 - binary_accuracy: 0.6535 - val_loss: 0.6130 - val_binary_accuracy: 0.6603\n","Epoch 58/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6126 - binary_accuracy: 0.6572 - val_loss: 0.6120 - val_binary_accuracy: 0.6613\n","Epoch 59/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6123 - binary_accuracy: 0.6568 - val_loss: 0.6110 - val_binary_accuracy: 0.6617\n","Epoch 60/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6109 - binary_accuracy: 0.6572 - val_loss: 0.6101 - val_binary_accuracy: 0.6623\n","Epoch 61/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6112 - binary_accuracy: 0.6566 - val_loss: 0.6091 - val_binary_accuracy: 0.6630\n","Epoch 62/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6101 - binary_accuracy: 0.6574 - val_loss: 0.6082 - val_binary_accuracy: 0.6638\n","Epoch 63/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6090 - binary_accuracy: 0.6580 - val_loss: 0.6072 - val_binary_accuracy: 0.6644\n","Epoch 64/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6076 - binary_accuracy: 0.6599 - val_loss: 0.6063 - val_binary_accuracy: 0.6648\n","Epoch 65/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6062 - binary_accuracy: 0.6607 - val_loss: 0.6054 - val_binary_accuracy: 0.6654\n","Epoch 66/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6052 - binary_accuracy: 0.6619 - val_loss: 0.6045 - val_binary_accuracy: 0.6658\n","Epoch 67/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6057 - binary_accuracy: 0.6593 - val_loss: 0.6036 - val_binary_accuracy: 0.6663\n","Epoch 68/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6050 - binary_accuracy: 0.6608 - val_loss: 0.6027 - val_binary_accuracy: 0.6668\n","Epoch 69/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6029 - binary_accuracy: 0.6621 - val_loss: 0.6018 - val_binary_accuracy: 0.6674\n","Epoch 70/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6015 - binary_accuracy: 0.6646 - val_loss: 0.6009 - val_binary_accuracy: 0.6677\n","Epoch 71/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6014 - binary_accuracy: 0.6646 - val_loss: 0.6001 - val_binary_accuracy: 0.6685\n","Epoch 72/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5997 - binary_accuracy: 0.6661 - val_loss: 0.5992 - val_binary_accuracy: 0.6693\n","Epoch 73/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6002 - binary_accuracy: 0.6645 - val_loss: 0.5983 - val_binary_accuracy: 0.6695\n","Epoch 74/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5999 - binary_accuracy: 0.6641 - val_loss: 0.5975 - val_binary_accuracy: 0.6700\n","Epoch 75/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5977 - binary_accuracy: 0.6671 - val_loss: 0.5967 - val_binary_accuracy: 0.6705\n","Epoch 76/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5963 - binary_accuracy: 0.6675 - val_loss: 0.5958 - val_binary_accuracy: 0.6710\n","Epoch 77/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5957 - binary_accuracy: 0.6686 - val_loss: 0.5950 - val_binary_accuracy: 0.6714\n","Epoch 78/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5944 - binary_accuracy: 0.6700 - val_loss: 0.5942 - val_binary_accuracy: 0.6717\n","Epoch 79/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5946 - binary_accuracy: 0.6688 - val_loss: 0.5934 - val_binary_accuracy: 0.6727\n","Epoch 80/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5929 - binary_accuracy: 0.6708 - val_loss: 0.5926 - val_binary_accuracy: 0.6733\n","Epoch 81/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5941 - binary_accuracy: 0.6688 - val_loss: 0.5918 - val_binary_accuracy: 0.6737\n","Epoch 82/100\n","338/338 [==============================] - 2s 5ms/step - loss: 0.5921 - binary_accuracy: 0.6703 - val_loss: 0.5910 - val_binary_accuracy: 0.6746\n","Epoch 83/100\n","338/338 [==============================] - 1s 4ms/step - loss: 0.5912 - binary_accuracy: 0.6722 - val_loss: 0.5902 - val_binary_accuracy: 0.6750\n","Epoch 84/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5913 - binary_accuracy: 0.6700 - val_loss: 0.5895 - val_binary_accuracy: 0.6759\n","Epoch 85/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5908 - binary_accuracy: 0.6710 - val_loss: 0.5887 - val_binary_accuracy: 0.6763\n","Epoch 86/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5885 - binary_accuracy: 0.6733 - val_loss: 0.5879 - val_binary_accuracy: 0.6771\n","Epoch 87/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5888 - binary_accuracy: 0.6736 - val_loss: 0.5872 - val_binary_accuracy: 0.6775\n","Epoch 88/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5881 - binary_accuracy: 0.6738 - val_loss: 0.5864 - val_binary_accuracy: 0.6780\n","Epoch 89/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5875 - binary_accuracy: 0.6733 - val_loss: 0.5857 - val_binary_accuracy: 0.6785\n","Epoch 90/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5866 - binary_accuracy: 0.6751 - val_loss: 0.5850 - val_binary_accuracy: 0.6792\n","Epoch 91/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5856 - binary_accuracy: 0.6745 - val_loss: 0.5843 - val_binary_accuracy: 0.6799\n","Epoch 92/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5845 - binary_accuracy: 0.6760 - val_loss: 0.5835 - val_binary_accuracy: 0.6805\n","Epoch 93/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5828 - binary_accuracy: 0.6782 - val_loss: 0.5828 - val_binary_accuracy: 0.6811\n","Epoch 94/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5833 - binary_accuracy: 0.6775 - val_loss: 0.5821 - val_binary_accuracy: 0.6818\n","Epoch 95/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5824 - binary_accuracy: 0.6774 - val_loss: 0.5814 - val_binary_accuracy: 0.6822\n","Epoch 96/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5819 - binary_accuracy: 0.6778 - val_loss: 0.5807 - val_binary_accuracy: 0.6827\n","Epoch 97/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5803 - binary_accuracy: 0.6815 - val_loss: 0.5800 - val_binary_accuracy: 0.6832\n","Epoch 98/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5797 - binary_accuracy: 0.6812 - val_loss: 0.5793 - val_binary_accuracy: 0.6836\n","Epoch 99/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5792 - binary_accuracy: 0.6813 - val_loss: 0.5786 - val_binary_accuracy: 0.6841\n","Epoch 100/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5779 - binary_accuracy: 0.6815 - val_loss: 0.5780 - val_binary_accuracy: 0.6847\n"],"name":"stdout"},{"output_type":"stream","text":["\r 10%|         | 4/42 [05:17<50:23, 79.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adadelta', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6936 - binary_accuracy: 0.5180 - val_loss: 0.6911 - val_binary_accuracy: 0.5287\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6919 - binary_accuracy: 0.5224 - val_loss: 0.6896 - val_binary_accuracy: 0.5341\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6906 - binary_accuracy: 0.5299 - val_loss: 0.6878 - val_binary_accuracy: 0.5403\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6888 - binary_accuracy: 0.5372 - val_loss: 0.6860 - val_binary_accuracy: 0.5471\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6863 - binary_accuracy: 0.5435 - val_loss: 0.6840 - val_binary_accuracy: 0.5541\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6848 - binary_accuracy: 0.5494 - val_loss: 0.6820 - val_binary_accuracy: 0.5611\n","Epoch 7/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6826 - binary_accuracy: 0.5581 - val_loss: 0.6799 - val_binary_accuracy: 0.5674\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6806 - binary_accuracy: 0.5629 - val_loss: 0.6778 - val_binary_accuracy: 0.5738\n","Epoch 9/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6787 - binary_accuracy: 0.5717 - val_loss: 0.6757 - val_binary_accuracy: 0.5810\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6765 - binary_accuracy: 0.5773 - val_loss: 0.6735 - val_binary_accuracy: 0.5895\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6740 - binary_accuracy: 0.5876 - val_loss: 0.6714 - val_binary_accuracy: 0.5969\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6718 - binary_accuracy: 0.5936 - val_loss: 0.6692 - val_binary_accuracy: 0.6045\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6700 - binary_accuracy: 0.6004 - val_loss: 0.6671 - val_binary_accuracy: 0.6112\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6671 - binary_accuracy: 0.6094 - val_loss: 0.6649 - val_binary_accuracy: 0.6183\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6657 - binary_accuracy: 0.6131 - val_loss: 0.6628 - val_binary_accuracy: 0.6229\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6637 - binary_accuracy: 0.6206 - val_loss: 0.6607 - val_binary_accuracy: 0.6289\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6612 - binary_accuracy: 0.6264 - val_loss: 0.6586 - val_binary_accuracy: 0.6338\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6595 - binary_accuracy: 0.6307 - val_loss: 0.6565 - val_binary_accuracy: 0.6392\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6572 - binary_accuracy: 0.6377 - val_loss: 0.6545 - val_binary_accuracy: 0.6428\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6553 - binary_accuracy: 0.6425 - val_loss: 0.6525 - val_binary_accuracy: 0.6471\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6528 - binary_accuracy: 0.6467 - val_loss: 0.6505 - val_binary_accuracy: 0.6512\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6507 - binary_accuracy: 0.6526 - val_loss: 0.6485 - val_binary_accuracy: 0.6562\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6490 - binary_accuracy: 0.6553 - val_loss: 0.6466 - val_binary_accuracy: 0.6597\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6474 - binary_accuracy: 0.6591 - val_loss: 0.6447 - val_binary_accuracy: 0.6634\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6448 - binary_accuracy: 0.6627 - val_loss: 0.6428 - val_binary_accuracy: 0.6663\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6435 - binary_accuracy: 0.6668 - val_loss: 0.6410 - val_binary_accuracy: 0.6685\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6414 - binary_accuracy: 0.6686 - val_loss: 0.6392 - val_binary_accuracy: 0.6703\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6407 - binary_accuracy: 0.6687 - val_loss: 0.6374 - val_binary_accuracy: 0.6730\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6380 - binary_accuracy: 0.6722 - val_loss: 0.6357 - val_binary_accuracy: 0.6756\n","Epoch 30/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6361 - binary_accuracy: 0.6758 - val_loss: 0.6340 - val_binary_accuracy: 0.6771\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6347 - binary_accuracy: 0.6770 - val_loss: 0.6323 - val_binary_accuracy: 0.6791\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6328 - binary_accuracy: 0.6786 - val_loss: 0.6307 - val_binary_accuracy: 0.6809\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6307 - binary_accuracy: 0.6829 - val_loss: 0.6290 - val_binary_accuracy: 0.6831\n","Epoch 34/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6298 - binary_accuracy: 0.6835 - val_loss: 0.6275 - val_binary_accuracy: 0.6842\n","Epoch 35/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6280 - binary_accuracy: 0.6839 - val_loss: 0.6259 - val_binary_accuracy: 0.6856\n","Epoch 36/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6265 - binary_accuracy: 0.6852 - val_loss: 0.6244 - val_binary_accuracy: 0.6868\n","Epoch 37/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6248 - binary_accuracy: 0.6870 - val_loss: 0.6229 - val_binary_accuracy: 0.6883\n","Epoch 38/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6238 - binary_accuracy: 0.6863 - val_loss: 0.6214 - val_binary_accuracy: 0.6897\n","Epoch 39/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6219 - binary_accuracy: 0.6881 - val_loss: 0.6200 - val_binary_accuracy: 0.6911\n","Epoch 40/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6200 - binary_accuracy: 0.6912 - val_loss: 0.6186 - val_binary_accuracy: 0.6917\n","Epoch 41/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6198 - binary_accuracy: 0.6899 - val_loss: 0.6172 - val_binary_accuracy: 0.6919\n","Epoch 42/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6180 - binary_accuracy: 0.6904 - val_loss: 0.6158 - val_binary_accuracy: 0.6927\n","Epoch 43/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6165 - binary_accuracy: 0.6916 - val_loss: 0.6145 - val_binary_accuracy: 0.6934\n","Epoch 44/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6151 - binary_accuracy: 0.6930 - val_loss: 0.6132 - val_binary_accuracy: 0.6936\n","Epoch 45/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6135 - binary_accuracy: 0.6936 - val_loss: 0.6119 - val_binary_accuracy: 0.6940\n","Epoch 46/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6133 - binary_accuracy: 0.6922 - val_loss: 0.6107 - val_binary_accuracy: 0.6947\n","Epoch 47/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6108 - binary_accuracy: 0.6947 - val_loss: 0.6094 - val_binary_accuracy: 0.6956\n","Epoch 48/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6101 - binary_accuracy: 0.6949 - val_loss: 0.6082 - val_binary_accuracy: 0.6961\n","Epoch 49/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6090 - binary_accuracy: 0.6950 - val_loss: 0.6070 - val_binary_accuracy: 0.6964\n","Epoch 50/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6079 - binary_accuracy: 0.6943 - val_loss: 0.6058 - val_binary_accuracy: 0.6966\n","Epoch 51/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6068 - binary_accuracy: 0.6951 - val_loss: 0.6047 - val_binary_accuracy: 0.6967\n","Epoch 52/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6052 - binary_accuracy: 0.6970 - val_loss: 0.6035 - val_binary_accuracy: 0.6968\n","Epoch 53/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6040 - binary_accuracy: 0.6962 - val_loss: 0.6024 - val_binary_accuracy: 0.6973\n","Epoch 54/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6029 - binary_accuracy: 0.6979 - val_loss: 0.6013 - val_binary_accuracy: 0.6975\n","Epoch 55/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6017 - binary_accuracy: 0.6980 - val_loss: 0.6003 - val_binary_accuracy: 0.6978\n","Epoch 56/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6019 - binary_accuracy: 0.6954 - val_loss: 0.5992 - val_binary_accuracy: 0.6982\n","Epoch 57/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6005 - binary_accuracy: 0.6976 - val_loss: 0.5982 - val_binary_accuracy: 0.6987\n","Epoch 58/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5983 - binary_accuracy: 0.7001 - val_loss: 0.5972 - val_binary_accuracy: 0.6988\n","Epoch 59/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5976 - binary_accuracy: 0.6991 - val_loss: 0.5962 - val_binary_accuracy: 0.6989\n","Epoch 60/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5969 - binary_accuracy: 0.6994 - val_loss: 0.5952 - val_binary_accuracy: 0.6992\n","Epoch 61/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5963 - binary_accuracy: 0.6993 - val_loss: 0.5942 - val_binary_accuracy: 0.6996\n","Epoch 62/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5953 - binary_accuracy: 0.6994 - val_loss: 0.5933 - val_binary_accuracy: 0.7000\n","Epoch 63/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5946 - binary_accuracy: 0.6996 - val_loss: 0.5923 - val_binary_accuracy: 0.7003\n","Epoch 64/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5937 - binary_accuracy: 0.6994 - val_loss: 0.5914 - val_binary_accuracy: 0.7010\n","Epoch 65/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5917 - binary_accuracy: 0.7010 - val_loss: 0.5905 - val_binary_accuracy: 0.7008\n","Epoch 66/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5910 - binary_accuracy: 0.7006 - val_loss: 0.5896 - val_binary_accuracy: 0.7010\n","Epoch 67/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5915 - binary_accuracy: 0.7000 - val_loss: 0.5887 - val_binary_accuracy: 0.7013\n","Epoch 68/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5900 - binary_accuracy: 0.7015 - val_loss: 0.5878 - val_binary_accuracy: 0.7015\n","Epoch 69/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5889 - binary_accuracy: 0.7009 - val_loss: 0.5870 - val_binary_accuracy: 0.7018\n","Epoch 70/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5882 - binary_accuracy: 0.7012 - val_loss: 0.5861 - val_binary_accuracy: 0.7019\n","Epoch 71/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5872 - binary_accuracy: 0.7021 - val_loss: 0.5853 - val_binary_accuracy: 0.7021\n","Epoch 72/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5853 - binary_accuracy: 0.7048 - val_loss: 0.5844 - val_binary_accuracy: 0.7024\n","Epoch 73/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5857 - binary_accuracy: 0.7026 - val_loss: 0.5836 - val_binary_accuracy: 0.7027\n","Epoch 74/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5854 - binary_accuracy: 0.7018 - val_loss: 0.5828 - val_binary_accuracy: 0.7029\n","Epoch 75/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5834 - binary_accuracy: 0.7041 - val_loss: 0.5821 - val_binary_accuracy: 0.7034\n","Epoch 76/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5820 - binary_accuracy: 0.7065 - val_loss: 0.5813 - val_binary_accuracy: 0.7036\n","Epoch 77/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5820 - binary_accuracy: 0.7046 - val_loss: 0.5805 - val_binary_accuracy: 0.7039\n","Epoch 78/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5807 - binary_accuracy: 0.7060 - val_loss: 0.5798 - val_binary_accuracy: 0.7040\n","Epoch 79/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5807 - binary_accuracy: 0.7049 - val_loss: 0.5790 - val_binary_accuracy: 0.7044\n","Epoch 80/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5799 - binary_accuracy: 0.7048 - val_loss: 0.5783 - val_binary_accuracy: 0.7051\n","Epoch 81/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5790 - binary_accuracy: 0.7046 - val_loss: 0.5775 - val_binary_accuracy: 0.7054\n","Epoch 82/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5780 - binary_accuracy: 0.7048 - val_loss: 0.5768 - val_binary_accuracy: 0.7054\n","Epoch 83/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5776 - binary_accuracy: 0.7055 - val_loss: 0.5761 - val_binary_accuracy: 0.7058\n","Epoch 84/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5771 - binary_accuracy: 0.7051 - val_loss: 0.5754 - val_binary_accuracy: 0.7061\n","Epoch 85/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5766 - binary_accuracy: 0.7057 - val_loss: 0.5747 - val_binary_accuracy: 0.7063\n","Epoch 86/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5751 - binary_accuracy: 0.7076 - val_loss: 0.5740 - val_binary_accuracy: 0.7064\n","Epoch 87/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5749 - binary_accuracy: 0.7070 - val_loss: 0.5734 - val_binary_accuracy: 0.7065\n","Epoch 88/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5741 - binary_accuracy: 0.7069 - val_loss: 0.5727 - val_binary_accuracy: 0.7066\n","Epoch 89/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5748 - binary_accuracy: 0.7055 - val_loss: 0.5720 - val_binary_accuracy: 0.7070\n","Epoch 90/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5729 - binary_accuracy: 0.7083 - val_loss: 0.5714 - val_binary_accuracy: 0.7071\n","Epoch 91/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5723 - binary_accuracy: 0.7078 - val_loss: 0.5707 - val_binary_accuracy: 0.7074\n","Epoch 92/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5710 - binary_accuracy: 0.7083 - val_loss: 0.5701 - val_binary_accuracy: 0.7075\n","Epoch 93/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5698 - binary_accuracy: 0.7105 - val_loss: 0.5695 - val_binary_accuracy: 0.7078\n","Epoch 94/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5707 - binary_accuracy: 0.7090 - val_loss: 0.5688 - val_binary_accuracy: 0.7081\n","Epoch 95/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5698 - binary_accuracy: 0.7072 - val_loss: 0.5682 - val_binary_accuracy: 0.7083\n","Epoch 96/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5696 - binary_accuracy: 0.7078 - val_loss: 0.5676 - val_binary_accuracy: 0.7085\n","Epoch 97/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5673 - binary_accuracy: 0.7104 - val_loss: 0.5670 - val_binary_accuracy: 0.7088\n","Epoch 98/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5672 - binary_accuracy: 0.7106 - val_loss: 0.5664 - val_binary_accuracy: 0.7089\n","Epoch 99/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5666 - binary_accuracy: 0.7116 - val_loss: 0.5658 - val_binary_accuracy: 0.7093\n","Epoch 100/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5661 - binary_accuracy: 0.7112 - val_loss: 0.5652 - val_binary_accuracy: 0.7096\n"],"name":"stdout"},{"output_type":"stream","text":["\r 12%|        | 5/42 [06:38<49:16, 79.92s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adadelta', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6798 - binary_accuracy: 0.5839 - val_loss: 0.6796 - val_binary_accuracy: 0.5869\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6785 - binary_accuracy: 0.5884 - val_loss: 0.6784 - val_binary_accuracy: 0.5906\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6779 - binary_accuracy: 0.5898 - val_loss: 0.6771 - val_binary_accuracy: 0.5942\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6770 - binary_accuracy: 0.5917 - val_loss: 0.6757 - val_binary_accuracy: 0.5978\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6758 - binary_accuracy: 0.5941 - val_loss: 0.6743 - val_binary_accuracy: 0.6018\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6746 - binary_accuracy: 0.5960 - val_loss: 0.6728 - val_binary_accuracy: 0.6048\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6723 - binary_accuracy: 0.6024 - val_loss: 0.6713 - val_binary_accuracy: 0.6079\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6711 - binary_accuracy: 0.6037 - val_loss: 0.6698 - val_binary_accuracy: 0.6117\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6687 - binary_accuracy: 0.6088 - val_loss: 0.6682 - val_binary_accuracy: 0.6151\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6676 - binary_accuracy: 0.6103 - val_loss: 0.6667 - val_binary_accuracy: 0.6172\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6665 - binary_accuracy: 0.6124 - val_loss: 0.6652 - val_binary_accuracy: 0.6201\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6652 - binary_accuracy: 0.6158 - val_loss: 0.6636 - val_binary_accuracy: 0.6225\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6635 - binary_accuracy: 0.6181 - val_loss: 0.6621 - val_binary_accuracy: 0.6248\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6616 - binary_accuracy: 0.6217 - val_loss: 0.6605 - val_binary_accuracy: 0.6271\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6596 - binary_accuracy: 0.6236 - val_loss: 0.6590 - val_binary_accuracy: 0.6289\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6585 - binary_accuracy: 0.6252 - val_loss: 0.6575 - val_binary_accuracy: 0.6311\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6568 - binary_accuracy: 0.6273 - val_loss: 0.6559 - val_binary_accuracy: 0.6329\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6564 - binary_accuracy: 0.6276 - val_loss: 0.6544 - val_binary_accuracy: 0.6344\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6541 - binary_accuracy: 0.6317 - val_loss: 0.6529 - val_binary_accuracy: 0.6358\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6519 - binary_accuracy: 0.6344 - val_loss: 0.6514 - val_binary_accuracy: 0.6376\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6509 - binary_accuracy: 0.6345 - val_loss: 0.6500 - val_binary_accuracy: 0.6388\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6491 - binary_accuracy: 0.6375 - val_loss: 0.6485 - val_binary_accuracy: 0.6399\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6478 - binary_accuracy: 0.6388 - val_loss: 0.6471 - val_binary_accuracy: 0.6412\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6471 - binary_accuracy: 0.6375 - val_loss: 0.6457 - val_binary_accuracy: 0.6427\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6443 - binary_accuracy: 0.6430 - val_loss: 0.6443 - val_binary_accuracy: 0.6437\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6438 - binary_accuracy: 0.6409 - val_loss: 0.6429 - val_binary_accuracy: 0.6452\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6427 - binary_accuracy: 0.6421 - val_loss: 0.6415 - val_binary_accuracy: 0.6463\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6420 - binary_accuracy: 0.6418 - val_loss: 0.6402 - val_binary_accuracy: 0.6469\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6407 - binary_accuracy: 0.6423 - val_loss: 0.6388 - val_binary_accuracy: 0.6478\n","Epoch 30/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6387 - binary_accuracy: 0.6445 - val_loss: 0.6375 - val_binary_accuracy: 0.6484\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6377 - binary_accuracy: 0.6451 - val_loss: 0.6362 - val_binary_accuracy: 0.6492\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6359 - binary_accuracy: 0.6473 - val_loss: 0.6349 - val_binary_accuracy: 0.6503\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6346 - binary_accuracy: 0.6480 - val_loss: 0.6337 - val_binary_accuracy: 0.6512\n","Epoch 34/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6345 - binary_accuracy: 0.6454 - val_loss: 0.6324 - val_binary_accuracy: 0.6520\n","Epoch 35/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6326 - binary_accuracy: 0.6486 - val_loss: 0.6311 - val_binary_accuracy: 0.6527\n","Epoch 36/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6311 - binary_accuracy: 0.6507 - val_loss: 0.6299 - val_binary_accuracy: 0.6537\n","Epoch 37/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6301 - binary_accuracy: 0.6516 - val_loss: 0.6287 - val_binary_accuracy: 0.6545\n","Epoch 38/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6277 - binary_accuracy: 0.6531 - val_loss: 0.6275 - val_binary_accuracy: 0.6552\n","Epoch 39/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6269 - binary_accuracy: 0.6539 - val_loss: 0.6263 - val_binary_accuracy: 0.6560\n","Epoch 40/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6255 - binary_accuracy: 0.6546 - val_loss: 0.6251 - val_binary_accuracy: 0.6564\n","Epoch 41/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6251 - binary_accuracy: 0.6530 - val_loss: 0.6239 - val_binary_accuracy: 0.6572\n","Epoch 42/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6238 - binary_accuracy: 0.6544 - val_loss: 0.6228 - val_binary_accuracy: 0.6578\n","Epoch 43/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6231 - binary_accuracy: 0.6538 - val_loss: 0.6216 - val_binary_accuracy: 0.6582\n","Epoch 44/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6218 - binary_accuracy: 0.6552 - val_loss: 0.6205 - val_binary_accuracy: 0.6589\n","Epoch 45/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6201 - binary_accuracy: 0.6570 - val_loss: 0.6194 - val_binary_accuracy: 0.6597\n","Epoch 46/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6201 - binary_accuracy: 0.6552 - val_loss: 0.6183 - val_binary_accuracy: 0.6606\n","Epoch 47/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6185 - binary_accuracy: 0.6576 - val_loss: 0.6172 - val_binary_accuracy: 0.6611\n","Epoch 48/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6168 - binary_accuracy: 0.6602 - val_loss: 0.6161 - val_binary_accuracy: 0.6619\n","Epoch 49/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6163 - binary_accuracy: 0.6584 - val_loss: 0.6150 - val_binary_accuracy: 0.6624\n","Epoch 50/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6148 - binary_accuracy: 0.6599 - val_loss: 0.6140 - val_binary_accuracy: 0.6630\n","Epoch 51/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6146 - binary_accuracy: 0.6589 - val_loss: 0.6129 - val_binary_accuracy: 0.6637\n","Epoch 52/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6131 - binary_accuracy: 0.6602 - val_loss: 0.6119 - val_binary_accuracy: 0.6643\n","Epoch 53/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6122 - binary_accuracy: 0.6604 - val_loss: 0.6108 - val_binary_accuracy: 0.6650\n","Epoch 54/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6104 - binary_accuracy: 0.6632 - val_loss: 0.6098 - val_binary_accuracy: 0.6654\n","Epoch 55/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6095 - binary_accuracy: 0.6627 - val_loss: 0.6088 - val_binary_accuracy: 0.6660\n","Epoch 56/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6092 - binary_accuracy: 0.6627 - val_loss: 0.6078 - val_binary_accuracy: 0.6665\n","Epoch 57/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6085 - binary_accuracy: 0.6631 - val_loss: 0.6069 - val_binary_accuracy: 0.6670\n","Epoch 58/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6064 - binary_accuracy: 0.6661 - val_loss: 0.6059 - val_binary_accuracy: 0.6675\n","Epoch 59/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6060 - binary_accuracy: 0.6651 - val_loss: 0.6049 - val_binary_accuracy: 0.6680\n","Epoch 60/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6047 - binary_accuracy: 0.6643 - val_loss: 0.6040 - val_binary_accuracy: 0.6685\n","Epoch 61/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6049 - binary_accuracy: 0.6645 - val_loss: 0.6030 - val_binary_accuracy: 0.6690\n","Epoch 62/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6038 - binary_accuracy: 0.6650 - val_loss: 0.6021 - val_binary_accuracy: 0.6695\n","Epoch 63/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6029 - binary_accuracy: 0.6650 - val_loss: 0.6012 - val_binary_accuracy: 0.6699\n","Epoch 64/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6015 - binary_accuracy: 0.6670 - val_loss: 0.6003 - val_binary_accuracy: 0.6707\n","Epoch 65/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6001 - binary_accuracy: 0.6677 - val_loss: 0.5994 - val_binary_accuracy: 0.6715\n","Epoch 66/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5992 - binary_accuracy: 0.6683 - val_loss: 0.5985 - val_binary_accuracy: 0.6720\n","Epoch 67/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5996 - binary_accuracy: 0.6662 - val_loss: 0.5976 - val_binary_accuracy: 0.6727\n","Epoch 68/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5988 - binary_accuracy: 0.6677 - val_loss: 0.5967 - val_binary_accuracy: 0.6732\n","Epoch 69/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5969 - binary_accuracy: 0.6689 - val_loss: 0.5958 - val_binary_accuracy: 0.6740\n","Epoch 70/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5957 - binary_accuracy: 0.6716 - val_loss: 0.5950 - val_binary_accuracy: 0.6743\n","Epoch 71/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5954 - binary_accuracy: 0.6712 - val_loss: 0.5941 - val_binary_accuracy: 0.6749\n","Epoch 72/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5937 - binary_accuracy: 0.6731 - val_loss: 0.5933 - val_binary_accuracy: 0.6756\n","Epoch 73/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5941 - binary_accuracy: 0.6716 - val_loss: 0.5925 - val_binary_accuracy: 0.6761\n","Epoch 74/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5938 - binary_accuracy: 0.6706 - val_loss: 0.5916 - val_binary_accuracy: 0.6769\n","Epoch 75/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5918 - binary_accuracy: 0.6738 - val_loss: 0.5908 - val_binary_accuracy: 0.6775\n","Epoch 76/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5903 - binary_accuracy: 0.6744 - val_loss: 0.5900 - val_binary_accuracy: 0.6785\n","Epoch 77/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5899 - binary_accuracy: 0.6748 - val_loss: 0.5892 - val_binary_accuracy: 0.6788\n","Epoch 78/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5885 - binary_accuracy: 0.6765 - val_loss: 0.5884 - val_binary_accuracy: 0.6792\n","Epoch 79/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5886 - binary_accuracy: 0.6758 - val_loss: 0.5876 - val_binary_accuracy: 0.6798\n","Epoch 80/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5872 - binary_accuracy: 0.6769 - val_loss: 0.5869 - val_binary_accuracy: 0.6802\n","Epoch 81/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5881 - binary_accuracy: 0.6749 - val_loss: 0.5861 - val_binary_accuracy: 0.6810\n","Epoch 82/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5863 - binary_accuracy: 0.6770 - val_loss: 0.5853 - val_binary_accuracy: 0.6815\n","Epoch 83/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5854 - binary_accuracy: 0.6785 - val_loss: 0.5846 - val_binary_accuracy: 0.6819\n","Epoch 84/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5855 - binary_accuracy: 0.6764 - val_loss: 0.5838 - val_binary_accuracy: 0.6823\n","Epoch 85/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5850 - binary_accuracy: 0.6777 - val_loss: 0.5831 - val_binary_accuracy: 0.6830\n","Epoch 86/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5828 - binary_accuracy: 0.6796 - val_loss: 0.5824 - val_binary_accuracy: 0.6838\n","Epoch 87/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5830 - binary_accuracy: 0.6796 - val_loss: 0.5816 - val_binary_accuracy: 0.6842\n","Epoch 88/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5823 - binary_accuracy: 0.6808 - val_loss: 0.5809 - val_binary_accuracy: 0.6848\n","Epoch 89/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5820 - binary_accuracy: 0.6808 - val_loss: 0.5802 - val_binary_accuracy: 0.6855\n","Epoch 90/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5809 - binary_accuracy: 0.6817 - val_loss: 0.5795 - val_binary_accuracy: 0.6860\n","Epoch 91/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5800 - binary_accuracy: 0.6817 - val_loss: 0.5788 - val_binary_accuracy: 0.6865\n","Epoch 92/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5789 - binary_accuracy: 0.6827 - val_loss: 0.5781 - val_binary_accuracy: 0.6872\n","Epoch 93/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5773 - binary_accuracy: 0.6860 - val_loss: 0.5774 - val_binary_accuracy: 0.6879\n","Epoch 94/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5778 - binary_accuracy: 0.6853 - val_loss: 0.5767 - val_binary_accuracy: 0.6886\n","Epoch 95/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5770 - binary_accuracy: 0.6849 - val_loss: 0.5760 - val_binary_accuracy: 0.6891\n","Epoch 96/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5765 - binary_accuracy: 0.6849 - val_loss: 0.5754 - val_binary_accuracy: 0.6895\n","Epoch 97/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5749 - binary_accuracy: 0.6886 - val_loss: 0.5747 - val_binary_accuracy: 0.6901\n","Epoch 98/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5743 - binary_accuracy: 0.6884 - val_loss: 0.5740 - val_binary_accuracy: 0.6905\n","Epoch 99/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5739 - binary_accuracy: 0.6895 - val_loss: 0.5734 - val_binary_accuracy: 0.6913\n","Epoch 100/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5727 - binary_accuracy: 0.6897 - val_loss: 0.5727 - val_binary_accuracy: 0.6918\n"],"name":"stdout"},{"output_type":"stream","text":["\r 14%|        | 6/42 [07:57<47:46, 79.63s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5913 - binary_accuracy: 0.6792 - val_loss: 0.4863 - val_binary_accuracy: 0.7598\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4725 - binary_accuracy: 0.7700 - val_loss: 0.4511 - val_binary_accuracy: 0.7831\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4449 - binary_accuracy: 0.7849 - val_loss: 0.4383 - val_binary_accuracy: 0.7881\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4347 - binary_accuracy: 0.7895 - val_loss: 0.4333 - val_binary_accuracy: 0.7908\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4302 - binary_accuracy: 0.7911 - val_loss: 0.4308 - val_binary_accuracy: 0.7924\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7938 - val_loss: 0.4295 - val_binary_accuracy: 0.7926\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4277 - binary_accuracy: 0.7919 - val_loss: 0.4287 - val_binary_accuracy: 0.7929\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4271 - binary_accuracy: 0.7928 - val_loss: 0.4281 - val_binary_accuracy: 0.7929\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7937 - val_loss: 0.4278 - val_binary_accuracy: 0.7932\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7956 - val_loss: 0.4276 - val_binary_accuracy: 0.7933\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7954 - val_loss: 0.4273 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7955 - val_loss: 0.4273 - val_binary_accuracy: 0.7930\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7949 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4232 - binary_accuracy: 0.7946 - val_loss: 0.4270 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7953 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4250 - binary_accuracy: 0.7947 - val_loss: 0.4272 - val_binary_accuracy: 0.7939\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4191 - binary_accuracy: 0.7986 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4259 - binary_accuracy: 0.7941 - val_loss: 0.4270 - val_binary_accuracy: 0.7935\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7957 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4219 - binary_accuracy: 0.7959 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 17%|        | 7/42 [08:13<35:22, 60.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5986 - binary_accuracy: 0.6656 - val_loss: 0.4933 - val_binary_accuracy: 0.7568\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4779 - binary_accuracy: 0.7654 - val_loss: 0.4533 - val_binary_accuracy: 0.7815\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4468 - binary_accuracy: 0.7841 - val_loss: 0.4391 - val_binary_accuracy: 0.7875\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4354 - binary_accuracy: 0.7886 - val_loss: 0.4335 - val_binary_accuracy: 0.7907\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4306 - binary_accuracy: 0.7910 - val_loss: 0.4308 - val_binary_accuracy: 0.7925\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4268 - binary_accuracy: 0.7935 - val_loss: 0.4296 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7916 - val_loss: 0.4288 - val_binary_accuracy: 0.7930\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4272 - binary_accuracy: 0.7925 - val_loss: 0.4282 - val_binary_accuracy: 0.7933\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7934 - val_loss: 0.4278 - val_binary_accuracy: 0.7932\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4226 - binary_accuracy: 0.7957 - val_loss: 0.4277 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7955 - val_loss: 0.4274 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7958 - val_loss: 0.4274 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7950 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4234 - binary_accuracy: 0.7946 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7954 - val_loss: 0.4272 - val_binary_accuracy: 0.7938\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7947 - val_loss: 0.4272 - val_binary_accuracy: 0.7941\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4193 - binary_accuracy: 0.7984 - val_loss: 0.4272 - val_binary_accuracy: 0.7939\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7940 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7956 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7962 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7958 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7967 - val_loss: 0.4269 - val_binary_accuracy: 0.7938\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4212 - binary_accuracy: 0.7965 - val_loss: 0.4270 - val_binary_accuracy: 0.7943\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4221 - binary_accuracy: 0.7954 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7973 - val_loss: 0.4269 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7943\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7954 - val_loss: 0.4268 - val_binary_accuracy: 0.7945\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7961 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 30/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7950 - val_loss: 0.4269 - val_binary_accuracy: 0.7944\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7954 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4219 - binary_accuracy: 0.7964 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4203 - binary_accuracy: 0.7975 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n","Restoring model weights from the end of the best epoch.\n","Epoch 00033: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 19%|        | 8/42 [08:39<28:28, 50.24s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5914 - binary_accuracy: 0.6792 - val_loss: 0.4863 - val_binary_accuracy: 0.7598\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4725 - binary_accuracy: 0.7700 - val_loss: 0.4511 - val_binary_accuracy: 0.7831\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4449 - binary_accuracy: 0.7849 - val_loss: 0.4383 - val_binary_accuracy: 0.7880\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4347 - binary_accuracy: 0.7895 - val_loss: 0.4333 - val_binary_accuracy: 0.7908\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7911 - val_loss: 0.4308 - val_binary_accuracy: 0.7924\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7938 - val_loss: 0.4295 - val_binary_accuracy: 0.7926\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4277 - binary_accuracy: 0.7919 - val_loss: 0.4287 - val_binary_accuracy: 0.7929\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4271 - binary_accuracy: 0.7928 - val_loss: 0.4281 - val_binary_accuracy: 0.7929\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7936 - val_loss: 0.4278 - val_binary_accuracy: 0.7932\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7956 - val_loss: 0.4276 - val_binary_accuracy: 0.7933\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7954 - val_loss: 0.4273 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7956 - val_loss: 0.4273 - val_binary_accuracy: 0.7930\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7949 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4232 - binary_accuracy: 0.7946 - val_loss: 0.4270 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7953 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4250 - binary_accuracy: 0.7947 - val_loss: 0.4272 - val_binary_accuracy: 0.7939\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4191 - binary_accuracy: 0.7986 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4259 - binary_accuracy: 0.7941 - val_loss: 0.4270 - val_binary_accuracy: 0.7935\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7957 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4219 - binary_accuracy: 0.7959 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 21%|       | 9/42 [08:55<22:02, 40.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5986 - binary_accuracy: 0.6655 - val_loss: 0.4933 - val_binary_accuracy: 0.7568\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4779 - binary_accuracy: 0.7654 - val_loss: 0.4533 - val_binary_accuracy: 0.7815\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4468 - binary_accuracy: 0.7841 - val_loss: 0.4391 - val_binary_accuracy: 0.7875\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4354 - binary_accuracy: 0.7886 - val_loss: 0.4335 - val_binary_accuracy: 0.7907\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4306 - binary_accuracy: 0.7910 - val_loss: 0.4308 - val_binary_accuracy: 0.7925\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4268 - binary_accuracy: 0.7935 - val_loss: 0.4296 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7916 - val_loss: 0.4288 - val_binary_accuracy: 0.7930\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4272 - binary_accuracy: 0.7925 - val_loss: 0.4282 - val_binary_accuracy: 0.7933\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7934 - val_loss: 0.4278 - val_binary_accuracy: 0.7932\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4226 - binary_accuracy: 0.7957 - val_loss: 0.4277 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7955 - val_loss: 0.4274 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7958 - val_loss: 0.4274 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7950 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4234 - binary_accuracy: 0.7946 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7954 - val_loss: 0.4272 - val_binary_accuracy: 0.7938\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7947 - val_loss: 0.4272 - val_binary_accuracy: 0.7941\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4193 - binary_accuracy: 0.7984 - val_loss: 0.4272 - val_binary_accuracy: 0.7939\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7940 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7956 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7962 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7958 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7967 - val_loss: 0.4269 - val_binary_accuracy: 0.7938\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4212 - binary_accuracy: 0.7965 - val_loss: 0.4270 - val_binary_accuracy: 0.7943\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4221 - binary_accuracy: 0.7954 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7973 - val_loss: 0.4269 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7943\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7954 - val_loss: 0.4268 - val_binary_accuracy: 0.7945\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7961 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 30/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7950 - val_loss: 0.4269 - val_binary_accuracy: 0.7944\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7954 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4219 - binary_accuracy: 0.7964 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 33/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4203 - binary_accuracy: 0.7975 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n","Restoring model weights from the end of the best epoch.\n","Epoch 00033: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 24%|       | 10/42 [09:21<19:07, 35.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5896 - binary_accuracy: 0.6878 - val_loss: 0.4878 - val_binary_accuracy: 0.7602\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4738 - binary_accuracy: 0.7701 - val_loss: 0.4519 - val_binary_accuracy: 0.7822\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4456 - binary_accuracy: 0.7844 - val_loss: 0.4386 - val_binary_accuracy: 0.7880\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4348 - binary_accuracy: 0.7895 - val_loss: 0.4333 - val_binary_accuracy: 0.7907\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7911 - val_loss: 0.4307 - val_binary_accuracy: 0.7924\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7938 - val_loss: 0.4295 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4277 - binary_accuracy: 0.7918 - val_loss: 0.4287 - val_binary_accuracy: 0.7929\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4271 - binary_accuracy: 0.7927 - val_loss: 0.4281 - val_binary_accuracy: 0.7930\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7937 - val_loss: 0.4278 - val_binary_accuracy: 0.7931\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4224 - binary_accuracy: 0.7956 - val_loss: 0.4276 - val_binary_accuracy: 0.7933\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7955 - val_loss: 0.4273 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7956 - val_loss: 0.4273 - val_binary_accuracy: 0.7931\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7949 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4233 - binary_accuracy: 0.7946 - val_loss: 0.4270 - val_binary_accuracy: 0.7935\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7953 - val_loss: 0.4272 - val_binary_accuracy: 0.7940\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4250 - binary_accuracy: 0.7948 - val_loss: 0.4272 - val_binary_accuracy: 0.7938\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4192 - binary_accuracy: 0.7985 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4259 - binary_accuracy: 0.7941 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7956 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4219 - binary_accuracy: 0.7960 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 26%|       | 11/42 [09:38<15:33, 30.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5943 - binary_accuracy: 0.6723 - val_loss: 0.4928 - val_binary_accuracy: 0.7576\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4777 - binary_accuracy: 0.7669 - val_loss: 0.4536 - val_binary_accuracy: 0.7811\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4470 - binary_accuracy: 0.7847 - val_loss: 0.4392 - val_binary_accuracy: 0.7877\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4354 - binary_accuracy: 0.7890 - val_loss: 0.4335 - val_binary_accuracy: 0.7908\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4306 - binary_accuracy: 0.7910 - val_loss: 0.4308 - val_binary_accuracy: 0.7931\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4268 - binary_accuracy: 0.7935 - val_loss: 0.4296 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4278 - binary_accuracy: 0.7915 - val_loss: 0.4287 - val_binary_accuracy: 0.7928\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4272 - binary_accuracy: 0.7927 - val_loss: 0.4282 - val_binary_accuracy: 0.7934\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7934 - val_loss: 0.4278 - val_binary_accuracy: 0.7933\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4226 - binary_accuracy: 0.7957 - val_loss: 0.4277 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7954 - val_loss: 0.4274 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7958 - val_loss: 0.4274 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7950 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4234 - binary_accuracy: 0.7946 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7952 - val_loss: 0.4272 - val_binary_accuracy: 0.7937\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7947 - val_loss: 0.4272 - val_binary_accuracy: 0.7940\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4192 - binary_accuracy: 0.7983 - val_loss: 0.4272 - val_binary_accuracy: 0.7938\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7940 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7956 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7961 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7958 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7967 - val_loss: 0.4269 - val_binary_accuracy: 0.7939\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4212 - binary_accuracy: 0.7966 - val_loss: 0.4270 - val_binary_accuracy: 0.7944\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4221 - binary_accuracy: 0.7955 - val_loss: 0.4268 - val_binary_accuracy: 0.7942\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7972 - val_loss: 0.4269 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7965 - val_loss: 0.4271 - val_binary_accuracy: 0.7939\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 28/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4230 - binary_accuracy: 0.7955 - val_loss: 0.4268 - val_binary_accuracy: 0.7945\n","Epoch 29/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7961 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 30/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7950 - val_loss: 0.4269 - val_binary_accuracy: 0.7944\n","Epoch 31/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7954 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 32/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4219 - binary_accuracy: 0.7964 - val_loss: 0.4270 - val_binary_accuracy: 0.7941\n","Epoch 33/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4203 - binary_accuracy: 0.7975 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n"],"name":"stdout"},{"output_type":"stream","text":["\r 29%|       | 12/42 [10:06<14:42, 29.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00033: early stopping\n","{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5514 - binary_accuracy: 0.7087 - val_loss: 0.4512 - val_binary_accuracy: 0.7807\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4441 - binary_accuracy: 0.7828 - val_loss: 0.4377 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4334 - binary_accuracy: 0.7899 - val_loss: 0.4328 - val_binary_accuracy: 0.7909\n","Epoch 4/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4297 - binary_accuracy: 0.7916 - val_loss: 0.4307 - val_binary_accuracy: 0.7917\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4276 - binary_accuracy: 0.7928 - val_loss: 0.4293 - val_binary_accuracy: 0.7929\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4252 - binary_accuracy: 0.7946 - val_loss: 0.4286 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4267 - binary_accuracy: 0.7925 - val_loss: 0.4281 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4264 - binary_accuracy: 0.7936 - val_loss: 0.4277 - val_binary_accuracy: 0.7938\n","Epoch 9/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4247 - binary_accuracy: 0.7941 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7958 - val_loss: 0.4274 - val_binary_accuracy: 0.7934\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7958 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7959 - val_loss: 0.4272 - val_binary_accuracy: 0.7932\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7951 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4231 - binary_accuracy: 0.7949 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7957 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7950 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4190 - binary_accuracy: 0.7987 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7943 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 19/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4218 - binary_accuracy: 0.7961 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7960 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4212 - binary_accuracy: 0.7968 - val_loss: 0.4269 - val_binary_accuracy: 0.7942\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4211 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7946\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7955 - val_loss: 0.4267 - val_binary_accuracy: 0.7945\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4205 - binary_accuracy: 0.7973 - val_loss: 0.4268 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7967 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4215 - binary_accuracy: 0.7964 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7956 - val_loss: 0.4268 - val_binary_accuracy: 0.7943\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 31%|       | 13/42 [10:31<13:33, 28.03s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5534 - binary_accuracy: 0.7034 - val_loss: 0.4504 - val_binary_accuracy: 0.7809\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4434 - binary_accuracy: 0.7824 - val_loss: 0.4374 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4333 - binary_accuracy: 0.7898 - val_loss: 0.4327 - val_binary_accuracy: 0.7908\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7917 - val_loss: 0.4306 - val_binary_accuracy: 0.7915\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4278 - binary_accuracy: 0.7924 - val_loss: 0.4293 - val_binary_accuracy: 0.7926\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7943 - val_loss: 0.4287 - val_binary_accuracy: 0.7930\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4269 - binary_accuracy: 0.7922 - val_loss: 0.4282 - val_binary_accuracy: 0.7934\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7934 - val_loss: 0.4278 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4248 - binary_accuracy: 0.7935 - val_loss: 0.4276 - val_binary_accuracy: 0.7929\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7958 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7956 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7961 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7951 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Restoring model weights from the end of the best epoch.\n","Epoch 00013: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 33%|      | 14/42 [10:42<10:44, 23.02s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5514 - binary_accuracy: 0.7086 - val_loss: 0.4512 - val_binary_accuracy: 0.7807\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4441 - binary_accuracy: 0.7828 - val_loss: 0.4377 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4334 - binary_accuracy: 0.7899 - val_loss: 0.4328 - val_binary_accuracy: 0.7909\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7916 - val_loss: 0.4307 - val_binary_accuracy: 0.7917\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4276 - binary_accuracy: 0.7928 - val_loss: 0.4293 - val_binary_accuracy: 0.7929\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7946 - val_loss: 0.4286 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7925 - val_loss: 0.4281 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4264 - binary_accuracy: 0.7936 - val_loss: 0.4277 - val_binary_accuracy: 0.7938\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4247 - binary_accuracy: 0.7941 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7958 - val_loss: 0.4274 - val_binary_accuracy: 0.7934\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7958 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7959 - val_loss: 0.4272 - val_binary_accuracy: 0.7932\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7951 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7949 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7957 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7950 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4190 - binary_accuracy: 0.7987 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 18/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4258 - binary_accuracy: 0.7943 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4218 - binary_accuracy: 0.7961 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7960 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4212 - binary_accuracy: 0.7968 - val_loss: 0.4269 - val_binary_accuracy: 0.7942\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4211 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7946\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7955 - val_loss: 0.4267 - val_binary_accuracy: 0.7945\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4205 - binary_accuracy: 0.7973 - val_loss: 0.4268 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7967 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4215 - binary_accuracy: 0.7963 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7956 - val_loss: 0.4268 - val_binary_accuracy: 0.7943\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 36%|      | 15/42 [11:04<10:12, 22.67s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5534 - binary_accuracy: 0.7034 - val_loss: 0.4504 - val_binary_accuracy: 0.7809\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4434 - binary_accuracy: 0.7824 - val_loss: 0.4374 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4333 - binary_accuracy: 0.7897 - val_loss: 0.4327 - val_binary_accuracy: 0.7908\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7917 - val_loss: 0.4306 - val_binary_accuracy: 0.7915\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4278 - binary_accuracy: 0.7924 - val_loss: 0.4293 - val_binary_accuracy: 0.7926\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7943 - val_loss: 0.4287 - val_binary_accuracy: 0.7930\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4269 - binary_accuracy: 0.7922 - val_loss: 0.4282 - val_binary_accuracy: 0.7934\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7934 - val_loss: 0.4278 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4248 - binary_accuracy: 0.7935 - val_loss: 0.4276 - val_binary_accuracy: 0.7929\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7958 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7956 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7961 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7951 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Restoring model weights from the end of the best epoch.\n","Epoch 00013: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 38%|      | 16/42 [11:14<08:13, 19.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5495 - binary_accuracy: 0.7147 - val_loss: 0.4507 - val_binary_accuracy: 0.7815\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4437 - binary_accuracy: 0.7834 - val_loss: 0.4375 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4332 - binary_accuracy: 0.7901 - val_loss: 0.4327 - val_binary_accuracy: 0.7913\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4296 - binary_accuracy: 0.7914 - val_loss: 0.4306 - val_binary_accuracy: 0.7915\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4276 - binary_accuracy: 0.7929 - val_loss: 0.4293 - val_binary_accuracy: 0.7929\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7946 - val_loss: 0.4286 - val_binary_accuracy: 0.7926\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7924 - val_loss: 0.4282 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4264 - binary_accuracy: 0.7936 - val_loss: 0.4277 - val_binary_accuracy: 0.7938\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4247 - binary_accuracy: 0.7940 - val_loss: 0.4275 - val_binary_accuracy: 0.7930\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4221 - binary_accuracy: 0.7959 - val_loss: 0.4274 - val_binary_accuracy: 0.7933\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7957 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7960 - val_loss: 0.4272 - val_binary_accuracy: 0.7933\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7951 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7948 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7956 - val_loss: 0.4271 - val_binary_accuracy: 0.7942\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7951 - val_loss: 0.4272 - val_binary_accuracy: 0.7941\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4190 - binary_accuracy: 0.7988 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 18/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4259 - binary_accuracy: 0.7943 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7957 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4218 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7960 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4212 - binary_accuracy: 0.7968 - val_loss: 0.4269 - val_binary_accuracy: 0.7942\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4211 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7946\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7956 - val_loss: 0.4268 - val_binary_accuracy: 0.7945\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4205 - binary_accuracy: 0.7973 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7966 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4215 - binary_accuracy: 0.7964 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7956 - val_loss: 0.4268 - val_binary_accuracy: 0.7943\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 40%|      | 17/42 [11:36<08:12, 19.69s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5506 - binary_accuracy: 0.7079 - val_loss: 0.4502 - val_binary_accuracy: 0.7810\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4433 - binary_accuracy: 0.7830 - val_loss: 0.4373 - val_binary_accuracy: 0.7881\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4332 - binary_accuracy: 0.7900 - val_loss: 0.4327 - val_binary_accuracy: 0.7911\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7917 - val_loss: 0.4306 - val_binary_accuracy: 0.7919\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4278 - binary_accuracy: 0.7922 - val_loss: 0.4293 - val_binary_accuracy: 0.7925\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7945 - val_loss: 0.4287 - val_binary_accuracy: 0.7929\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4269 - binary_accuracy: 0.7923 - val_loss: 0.4282 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7935 - val_loss: 0.4278 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4248 - binary_accuracy: 0.7936 - val_loss: 0.4276 - val_binary_accuracy: 0.7929\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7958 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7957 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7960 - val_loss: 0.4272 - val_binary_accuracy: 0.7933\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7951 - val_loss: 0.4271 - val_binary_accuracy: 0.7934\n"],"name":"stdout"},{"output_type":"stream","text":["\r 43%|     | 18/42 [11:46<06:47, 16.96s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00013: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6017 - binary_accuracy: 0.6679 - val_loss: 0.4984 - val_binary_accuracy: 0.7540\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4851 - binary_accuracy: 0.7639 - val_loss: 0.4622 - val_binary_accuracy: 0.7780\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4556 - binary_accuracy: 0.7812 - val_loss: 0.4461 - val_binary_accuracy: 0.7857\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4420 - binary_accuracy: 0.7868 - val_loss: 0.4381 - val_binary_accuracy: 0.7892\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4346 - binary_accuracy: 0.7898 - val_loss: 0.4337 - val_binary_accuracy: 0.7909\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4294 - binary_accuracy: 0.7928 - val_loss: 0.4312 - val_binary_accuracy: 0.7924\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4293 - binary_accuracy: 0.7915 - val_loss: 0.4298 - val_binary_accuracy: 0.7934\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4281 - binary_accuracy: 0.7925 - val_loss: 0.4288 - val_binary_accuracy: 0.7934\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7934 - val_loss: 0.4282 - val_binary_accuracy: 0.7938\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7952 - val_loss: 0.4279 - val_binary_accuracy: 0.7929\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7951 - val_loss: 0.4276 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4232 - binary_accuracy: 0.7956 - val_loss: 0.4275 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7949 - val_loss: 0.4273 - val_binary_accuracy: 0.7931\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7942 - val_loss: 0.4271 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\r 45%|     | 19/42 [11:58<05:57, 15.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6070 - binary_accuracy: 0.6584 - val_loss: 0.5055 - val_binary_accuracy: 0.7502\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4909 - binary_accuracy: 0.7584 - val_loss: 0.4668 - val_binary_accuracy: 0.7762\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4596 - binary_accuracy: 0.7792 - val_loss: 0.4488 - val_binary_accuracy: 0.7841\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4445 - binary_accuracy: 0.7855 - val_loss: 0.4397 - val_binary_accuracy: 0.7878\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4363 - binary_accuracy: 0.7890 - val_loss: 0.4346 - val_binary_accuracy: 0.7901\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7927 - val_loss: 0.4317 - val_binary_accuracy: 0.7920\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4298 - binary_accuracy: 0.7916 - val_loss: 0.4300 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7925 - val_loss: 0.4289 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7936 - val_loss: 0.4282 - val_binary_accuracy: 0.7937\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7956 - val_loss: 0.4279 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7954 - val_loss: 0.4276 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7956 - val_loss: 0.4275 - val_binary_accuracy: 0.7928\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7948 - val_loss: 0.4273 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7940 - val_loss: 0.4272 - val_binary_accuracy: 0.7933\n"],"name":"stdout"},{"output_type":"stream","text":["\r 48%|     | 20/42 [12:11<05:19, 14.51s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6017 - binary_accuracy: 0.6679 - val_loss: 0.4984 - val_binary_accuracy: 0.7540\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4851 - binary_accuracy: 0.7639 - val_loss: 0.4622 - val_binary_accuracy: 0.7780\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4556 - binary_accuracy: 0.7812 - val_loss: 0.4461 - val_binary_accuracy: 0.7856\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4420 - binary_accuracy: 0.7868 - val_loss: 0.4381 - val_binary_accuracy: 0.7892\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4346 - binary_accuracy: 0.7898 - val_loss: 0.4337 - val_binary_accuracy: 0.7909\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4294 - binary_accuracy: 0.7928 - val_loss: 0.4312 - val_binary_accuracy: 0.7924\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4293 - binary_accuracy: 0.7915 - val_loss: 0.4298 - val_binary_accuracy: 0.7933\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4281 - binary_accuracy: 0.7925 - val_loss: 0.4288 - val_binary_accuracy: 0.7934\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7934 - val_loss: 0.4282 - val_binary_accuracy: 0.7938\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7952 - val_loss: 0.4279 - val_binary_accuracy: 0.7929\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7951 - val_loss: 0.4276 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7956 - val_loss: 0.4275 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7949 - val_loss: 0.4273 - val_binary_accuracy: 0.7931\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4234 - binary_accuracy: 0.7942 - val_loss: 0.4271 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|     | 21/42 [12:23<04:50, 13.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6070 - binary_accuracy: 0.6584 - val_loss: 0.5055 - val_binary_accuracy: 0.7501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4909 - binary_accuracy: 0.7584 - val_loss: 0.4668 - val_binary_accuracy: 0.7762\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4596 - binary_accuracy: 0.7792 - val_loss: 0.4488 - val_binary_accuracy: 0.7841\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4445 - binary_accuracy: 0.7855 - val_loss: 0.4397 - val_binary_accuracy: 0.7878\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4363 - binary_accuracy: 0.7890 - val_loss: 0.4346 - val_binary_accuracy: 0.7901\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7927 - val_loss: 0.4317 - val_binary_accuracy: 0.7920\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4298 - binary_accuracy: 0.7916 - val_loss: 0.4300 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7925 - val_loss: 0.4289 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7936 - val_loss: 0.4282 - val_binary_accuracy: 0.7937\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7956 - val_loss: 0.4279 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7954 - val_loss: 0.4276 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7956 - val_loss: 0.4275 - val_binary_accuracy: 0.7928\n","Epoch 13/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4234 - binary_accuracy: 0.7948 - val_loss: 0.4273 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7940 - val_loss: 0.4272 - val_binary_accuracy: 0.7933\n"],"name":"stdout"},{"output_type":"stream","text":["\r 52%|    | 22/42 [12:35<04:28, 13.41s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5996 - binary_accuracy: 0.6772 - val_loss: 0.4996 - val_binary_accuracy: 0.7534\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4863 - binary_accuracy: 0.7638 - val_loss: 0.4634 - val_binary_accuracy: 0.7781\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4567 - binary_accuracy: 0.7810 - val_loss: 0.4468 - val_binary_accuracy: 0.7854\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4426 - binary_accuracy: 0.7869 - val_loss: 0.4385 - val_binary_accuracy: 0.7892\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4351 - binary_accuracy: 0.7893 - val_loss: 0.4340 - val_binary_accuracy: 0.7911\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4296 - binary_accuracy: 0.7926 - val_loss: 0.4314 - val_binary_accuracy: 0.7926\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4294 - binary_accuracy: 0.7912 - val_loss: 0.4298 - val_binary_accuracy: 0.7933\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4281 - binary_accuracy: 0.7922 - val_loss: 0.4288 - val_binary_accuracy: 0.7935\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7934 - val_loss: 0.4282 - val_binary_accuracy: 0.7936\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7952 - val_loss: 0.4279 - val_binary_accuracy: 0.7930\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7952 - val_loss: 0.4276 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7955 - val_loss: 0.4275 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7949 - val_loss: 0.4273 - val_binary_accuracy: 0.7930\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7941 - val_loss: 0.4272 - val_binary_accuracy: 0.7931\n"],"name":"stdout"},{"output_type":"stream","text":["\r 55%|    | 23/42 [12:47<04:06, 13.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6028 - binary_accuracy: 0.6646 - val_loss: 0.5047 - val_binary_accuracy: 0.7506\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4905 - binary_accuracy: 0.7599 - val_loss: 0.4668 - val_binary_accuracy: 0.7761\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4596 - binary_accuracy: 0.7797 - val_loss: 0.4489 - val_binary_accuracy: 0.7841\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4445 - binary_accuracy: 0.7858 - val_loss: 0.4398 - val_binary_accuracy: 0.7882\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4364 - binary_accuracy: 0.7891 - val_loss: 0.4347 - val_binary_accuracy: 0.7902\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4303 - binary_accuracy: 0.7928 - val_loss: 0.4317 - val_binary_accuracy: 0.7918\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4298 - binary_accuracy: 0.7917 - val_loss: 0.4300 - val_binary_accuracy: 0.7930\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7926 - val_loss: 0.4289 - val_binary_accuracy: 0.7935\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7935 - val_loss: 0.4282 - val_binary_accuracy: 0.7936\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7955 - val_loss: 0.4279 - val_binary_accuracy: 0.7933\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7952 - val_loss: 0.4276 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7957 - val_loss: 0.4275 - val_binary_accuracy: 0.7927\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7948 - val_loss: 0.4273 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7941 - val_loss: 0.4272 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\r 57%|    | 24/42 [13:00<03:50, 12.78s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6017 - binary_accuracy: 0.6679 - val_loss: 0.4984 - val_binary_accuracy: 0.7541\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4852 - binary_accuracy: 0.7640 - val_loss: 0.4623 - val_binary_accuracy: 0.7780\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4559 - binary_accuracy: 0.7813 - val_loss: 0.4465 - val_binary_accuracy: 0.7855\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4425 - binary_accuracy: 0.7867 - val_loss: 0.4387 - val_binary_accuracy: 0.7890\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4353 - binary_accuracy: 0.7893 - val_loss: 0.4345 - val_binary_accuracy: 0.7907\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7922 - val_loss: 0.4320 - val_binary_accuracy: 0.7920\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4300 - binary_accuracy: 0.7912 - val_loss: 0.4304 - val_binary_accuracy: 0.7936\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4287 - binary_accuracy: 0.7921 - val_loss: 0.4294 - val_binary_accuracy: 0.7934\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4264 - binary_accuracy: 0.7938 - val_loss: 0.4287 - val_binary_accuracy: 0.7938\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7953 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7948 - val_loss: 0.4279 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7955 - val_loss: 0.4278 - val_binary_accuracy: 0.7930\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7947 - val_loss: 0.4275 - val_binary_accuracy: 0.7934\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7939 - val_loss: 0.4274 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\r 60%|    | 25/42 [13:11<03:29, 12.32s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6070 - binary_accuracy: 0.6584 - val_loss: 0.5055 - val_binary_accuracy: 0.7503\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4909 - binary_accuracy: 0.7584 - val_loss: 0.4669 - val_binary_accuracy: 0.7761\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4598 - binary_accuracy: 0.7790 - val_loss: 0.4493 - val_binary_accuracy: 0.7841\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4451 - binary_accuracy: 0.7857 - val_loss: 0.4405 - val_binary_accuracy: 0.7879\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4372 - binary_accuracy: 0.7889 - val_loss: 0.4355 - val_binary_accuracy: 0.7901\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4312 - binary_accuracy: 0.7925 - val_loss: 0.4326 - val_binary_accuracy: 0.7917\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7916 - val_loss: 0.4308 - val_binary_accuracy: 0.7922\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7924 - val_loss: 0.4295 - val_binary_accuracy: 0.7928\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7941 - val_loss: 0.4287 - val_binary_accuracy: 0.7935\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7952 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7953 - val_loss: 0.4279 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7956 - val_loss: 0.4277 - val_binary_accuracy: 0.7931\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7949 - val_loss: 0.4275 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7942 - val_loss: 0.4273 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4240 - binary_accuracy: 0.7952 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7944 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4195 - binary_accuracy: 0.7981 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4261 - binary_accuracy: 0.7932 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7952 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4214 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7965 - val_loss: 0.4269 - val_binary_accuracy: 0.7945\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7955 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7940\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7962 - val_loss: 0.4271 - val_binary_accuracy: 0.7939\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7944\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7951 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 62%|   | 26/42 [13:33<04:04, 15.28s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6017 - binary_accuracy: 0.6679 - val_loss: 0.4984 - val_binary_accuracy: 0.7540\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4852 - binary_accuracy: 0.7639 - val_loss: 0.4623 - val_binary_accuracy: 0.7780\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4558 - binary_accuracy: 0.7813 - val_loss: 0.4465 - val_binary_accuracy: 0.7855\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4425 - binary_accuracy: 0.7867 - val_loss: 0.4387 - val_binary_accuracy: 0.7890\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4353 - binary_accuracy: 0.7894 - val_loss: 0.4345 - val_binary_accuracy: 0.7907\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7922 - val_loss: 0.4320 - val_binary_accuracy: 0.7920\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4300 - binary_accuracy: 0.7912 - val_loss: 0.4304 - val_binary_accuracy: 0.7936\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4287 - binary_accuracy: 0.7921 - val_loss: 0.4294 - val_binary_accuracy: 0.7934\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4264 - binary_accuracy: 0.7938 - val_loss: 0.4287 - val_binary_accuracy: 0.7938\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7953 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7948 - val_loss: 0.4279 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7955 - val_loss: 0.4278 - val_binary_accuracy: 0.7930\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7947 - val_loss: 0.4275 - val_binary_accuracy: 0.7934\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7939 - val_loss: 0.4274 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\r 64%|   | 27/42 [13:45<03:32, 14.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6070 - binary_accuracy: 0.6584 - val_loss: 0.5055 - val_binary_accuracy: 0.7501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4909 - binary_accuracy: 0.7584 - val_loss: 0.4669 - val_binary_accuracy: 0.7761\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4598 - binary_accuracy: 0.7790 - val_loss: 0.4493 - val_binary_accuracy: 0.7840\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4451 - binary_accuracy: 0.7857 - val_loss: 0.4405 - val_binary_accuracy: 0.7879\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4372 - binary_accuracy: 0.7889 - val_loss: 0.4355 - val_binary_accuracy: 0.7901\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4312 - binary_accuracy: 0.7925 - val_loss: 0.4326 - val_binary_accuracy: 0.7917\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7916 - val_loss: 0.4308 - val_binary_accuracy: 0.7922\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7924 - val_loss: 0.4295 - val_binary_accuracy: 0.7928\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7941 - val_loss: 0.4287 - val_binary_accuracy: 0.7935\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7952 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7953 - val_loss: 0.4279 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7956 - val_loss: 0.4277 - val_binary_accuracy: 0.7931\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7949 - val_loss: 0.4275 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7942 - val_loss: 0.4273 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4240 - binary_accuracy: 0.7952 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7944 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4195 - binary_accuracy: 0.7981 - val_loss: 0.4272 - val_binary_accuracy: 0.7936\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4261 - binary_accuracy: 0.7932 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7952 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4214 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7965 - val_loss: 0.4269 - val_binary_accuracy: 0.7945\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7955 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7940\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7962 - val_loss: 0.4271 - val_binary_accuracy: 0.7939\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7944\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7951 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 67%|   | 28/42 [14:07<03:51, 16.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5996 - binary_accuracy: 0.6772 - val_loss: 0.4997 - val_binary_accuracy: 0.7534\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4864 - binary_accuracy: 0.7638 - val_loss: 0.4635 - val_binary_accuracy: 0.7782\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4569 - binary_accuracy: 0.7810 - val_loss: 0.4472 - val_binary_accuracy: 0.7851\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4431 - binary_accuracy: 0.7869 - val_loss: 0.4392 - val_binary_accuracy: 0.7889\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4358 - binary_accuracy: 0.7892 - val_loss: 0.4348 - val_binary_accuracy: 0.7909\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7921 - val_loss: 0.4321 - val_binary_accuracy: 0.7920\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7914 - val_loss: 0.4305 - val_binary_accuracy: 0.7933\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7921 - val_loss: 0.4294 - val_binary_accuracy: 0.7933\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7939 - val_loss: 0.4287 - val_binary_accuracy: 0.7936\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7954 - val_loss: 0.4283 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7949 - val_loss: 0.4279 - val_binary_accuracy: 0.7933\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7954 - val_loss: 0.4278 - val_binary_accuracy: 0.7933\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7948 - val_loss: 0.4275 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4238 - binary_accuracy: 0.7940 - val_loss: 0.4274 - val_binary_accuracy: 0.7933\n"],"name":"stdout"},{"output_type":"stream","text":["\r 69%|   | 29/42 [14:18<03:15, 15.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6028 - binary_accuracy: 0.6646 - val_loss: 0.5047 - val_binary_accuracy: 0.7506\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4905 - binary_accuracy: 0.7599 - val_loss: 0.4669 - val_binary_accuracy: 0.7761\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4599 - binary_accuracy: 0.7796 - val_loss: 0.4494 - val_binary_accuracy: 0.7837\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4451 - binary_accuracy: 0.7856 - val_loss: 0.4405 - val_binary_accuracy: 0.7877\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4372 - binary_accuracy: 0.7891 - val_loss: 0.4356 - val_binary_accuracy: 0.7899\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4312 - binary_accuracy: 0.7925 - val_loss: 0.4326 - val_binary_accuracy: 0.7918\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7914 - val_loss: 0.4308 - val_binary_accuracy: 0.7923\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7925 - val_loss: 0.4296 - val_binary_accuracy: 0.7929\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7940 - val_loss: 0.4288 - val_binary_accuracy: 0.7934\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7954 - val_loss: 0.4283 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7952 - val_loss: 0.4279 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7956 - val_loss: 0.4277 - val_binary_accuracy: 0.7932\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7949 - val_loss: 0.4275 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7942 - val_loss: 0.4273 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4240 - binary_accuracy: 0.7953 - val_loss: 0.4273 - val_binary_accuracy: 0.7934\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7943 - val_loss: 0.4273 - val_binary_accuracy: 0.7936\n","Epoch 17/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4195 - binary_accuracy: 0.7981 - val_loss: 0.4272 - val_binary_accuracy: 0.7936\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4261 - binary_accuracy: 0.7934 - val_loss: 0.4271 - val_binary_accuracy: 0.7938\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7952 - val_loss: 0.4271 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4214 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7941\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7965 - val_loss: 0.4269 - val_binary_accuracy: 0.7946\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4221 - binary_accuracy: 0.7955 - val_loss: 0.4268 - val_binary_accuracy: 0.7940\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7961 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7945\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7951 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 71%|  | 30/42 [14:40<03:25, 17.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adamax', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6189 - binary_accuracy: 0.6550 - val_loss: 0.5251 - val_binary_accuracy: 0.7322\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5119 - binary_accuracy: 0.7438 - val_loss: 0.4858 - val_binary_accuracy: 0.7603\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4798 - binary_accuracy: 0.7657 - val_loss: 0.4657 - val_binary_accuracy: 0.7722\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4617 - binary_accuracy: 0.7747 - val_loss: 0.4541 - val_binary_accuracy: 0.7795\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4506 - binary_accuracy: 0.7803 - val_loss: 0.4469 - val_binary_accuracy: 0.7841\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4426 - binary_accuracy: 0.7859 - val_loss: 0.4423 - val_binary_accuracy: 0.7867\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4405 - binary_accuracy: 0.7850 - val_loss: 0.4390 - val_binary_accuracy: 0.7877\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4377 - binary_accuracy: 0.7867 - val_loss: 0.4368 - val_binary_accuracy: 0.7890\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4341 - binary_accuracy: 0.7891 - val_loss: 0.4351 - val_binary_accuracy: 0.7897\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7907 - val_loss: 0.4339 - val_binary_accuracy: 0.7905\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4305 - binary_accuracy: 0.7912 - val_loss: 0.4329 - val_binary_accuracy: 0.7907\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4290 - binary_accuracy: 0.7916 - val_loss: 0.4320 - val_binary_accuracy: 0.7910\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7912 - val_loss: 0.4314 - val_binary_accuracy: 0.7918\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4283 - binary_accuracy: 0.7916 - val_loss: 0.4308 - val_binary_accuracy: 0.7923\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7929 - val_loss: 0.4304 - val_binary_accuracy: 0.7928\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7922 - val_loss: 0.4301 - val_binary_accuracy: 0.7925\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7960 - val_loss: 0.4298 - val_binary_accuracy: 0.7929\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4291 - binary_accuracy: 0.7909 - val_loss: 0.4294 - val_binary_accuracy: 0.7925\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7927 - val_loss: 0.4292 - val_binary_accuracy: 0.7926\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7935 - val_loss: 0.4290 - val_binary_accuracy: 0.7927\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7934 - val_loss: 0.4288 - val_binary_accuracy: 0.7929\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7948 - val_loss: 0.4286 - val_binary_accuracy: 0.7932\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7945 - val_loss: 0.4284 - val_binary_accuracy: 0.7932\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7940 - val_loss: 0.4283 - val_binary_accuracy: 0.7928\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7955 - val_loss: 0.4282 - val_binary_accuracy: 0.7928\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7953 - val_loss: 0.4282 - val_binary_accuracy: 0.7924\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7950 - val_loss: 0.4280 - val_binary_accuracy: 0.7928\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4250 - binary_accuracy: 0.7934 - val_loss: 0.4279 - val_binary_accuracy: 0.7929\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 74%|  | 31/42 [15:02<03:23, 18.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adamax', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6224 - binary_accuracy: 0.6461 - val_loss: 0.5320 - val_binary_accuracy: 0.7257\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5167 - binary_accuracy: 0.7392 - val_loss: 0.4880 - val_binary_accuracy: 0.7625\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4811 - binary_accuracy: 0.7656 - val_loss: 0.4663 - val_binary_accuracy: 0.7742\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4619 - binary_accuracy: 0.7751 - val_loss: 0.4539 - val_binary_accuracy: 0.7811\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4507 - binary_accuracy: 0.7803 - val_loss: 0.4466 - val_binary_accuracy: 0.7841\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4422 - binary_accuracy: 0.7858 - val_loss: 0.4419 - val_binary_accuracy: 0.7863\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4402 - binary_accuracy: 0.7852 - val_loss: 0.4386 - val_binary_accuracy: 0.7879\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4374 - binary_accuracy: 0.7860 - val_loss: 0.4365 - val_binary_accuracy: 0.7886\n","Epoch 9/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4339 - binary_accuracy: 0.7891 - val_loss: 0.4348 - val_binary_accuracy: 0.7901\n","Epoch 10/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4303 - binary_accuracy: 0.7903 - val_loss: 0.4336 - val_binary_accuracy: 0.7903\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7911 - val_loss: 0.4327 - val_binary_accuracy: 0.7907\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7914 - val_loss: 0.4319 - val_binary_accuracy: 0.7911\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7909 - val_loss: 0.4312 - val_binary_accuracy: 0.7913\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4283 - binary_accuracy: 0.7914 - val_loss: 0.4307 - val_binary_accuracy: 0.7921\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7926 - val_loss: 0.4303 - val_binary_accuracy: 0.7927\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4290 - binary_accuracy: 0.7921 - val_loss: 0.4300 - val_binary_accuracy: 0.7923\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7958 - val_loss: 0.4297 - val_binary_accuracy: 0.7926\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7910 - val_loss: 0.4294 - val_binary_accuracy: 0.7926\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4268 - binary_accuracy: 0.7926 - val_loss: 0.4292 - val_binary_accuracy: 0.7925\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7934 - val_loss: 0.4290 - val_binary_accuracy: 0.7925\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 76%|  | 32/42 [15:18<02:58, 17.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adamax', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6189 - binary_accuracy: 0.6549 - val_loss: 0.5251 - val_binary_accuracy: 0.7322\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5119 - binary_accuracy: 0.7438 - val_loss: 0.4858 - val_binary_accuracy: 0.7602\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4798 - binary_accuracy: 0.7657 - val_loss: 0.4657 - val_binary_accuracy: 0.7722\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4617 - binary_accuracy: 0.7747 - val_loss: 0.4541 - val_binary_accuracy: 0.7795\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4506 - binary_accuracy: 0.7803 - val_loss: 0.4469 - val_binary_accuracy: 0.7841\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4426 - binary_accuracy: 0.7860 - val_loss: 0.4423 - val_binary_accuracy: 0.7867\n","Epoch 7/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4405 - binary_accuracy: 0.7850 - val_loss: 0.4390 - val_binary_accuracy: 0.7877\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4377 - binary_accuracy: 0.7867 - val_loss: 0.4368 - val_binary_accuracy: 0.7890\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4341 - binary_accuracy: 0.7891 - val_loss: 0.4351 - val_binary_accuracy: 0.7897\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7907 - val_loss: 0.4339 - val_binary_accuracy: 0.7905\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4305 - binary_accuracy: 0.7912 - val_loss: 0.4329 - val_binary_accuracy: 0.7907\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4290 - binary_accuracy: 0.7916 - val_loss: 0.4320 - val_binary_accuracy: 0.7910\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7912 - val_loss: 0.4314 - val_binary_accuracy: 0.7918\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4283 - binary_accuracy: 0.7915 - val_loss: 0.4308 - val_binary_accuracy: 0.7923\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7929 - val_loss: 0.4304 - val_binary_accuracy: 0.7928\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7923 - val_loss: 0.4301 - val_binary_accuracy: 0.7925\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7960 - val_loss: 0.4298 - val_binary_accuracy: 0.7929\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4291 - binary_accuracy: 0.7909 - val_loss: 0.4294 - val_binary_accuracy: 0.7925\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7927 - val_loss: 0.4292 - val_binary_accuracy: 0.7926\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7935 - val_loss: 0.4290 - val_binary_accuracy: 0.7927\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7934 - val_loss: 0.4288 - val_binary_accuracy: 0.7929\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7948 - val_loss: 0.4286 - val_binary_accuracy: 0.7932\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7945 - val_loss: 0.4284 - val_binary_accuracy: 0.7932\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7940 - val_loss: 0.4283 - val_binary_accuracy: 0.7928\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7955 - val_loss: 0.4282 - val_binary_accuracy: 0.7928\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7953 - val_loss: 0.4282 - val_binary_accuracy: 0.7924\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7950 - val_loss: 0.4280 - val_binary_accuracy: 0.7928\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4250 - binary_accuracy: 0.7934 - val_loss: 0.4279 - val_binary_accuracy: 0.7929\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 79%|  | 33/42 [15:41<02:54, 19.42s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adamax', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6224 - binary_accuracy: 0.6460 - val_loss: 0.5320 - val_binary_accuracy: 0.7256\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5167 - binary_accuracy: 0.7392 - val_loss: 0.4880 - val_binary_accuracy: 0.7625\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4812 - binary_accuracy: 0.7656 - val_loss: 0.4663 - val_binary_accuracy: 0.7742\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4619 - binary_accuracy: 0.7751 - val_loss: 0.4539 - val_binary_accuracy: 0.7811\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4507 - binary_accuracy: 0.7803 - val_loss: 0.4466 - val_binary_accuracy: 0.7840\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4422 - binary_accuracy: 0.7858 - val_loss: 0.4419 - val_binary_accuracy: 0.7863\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4402 - binary_accuracy: 0.7852 - val_loss: 0.4386 - val_binary_accuracy: 0.7879\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4374 - binary_accuracy: 0.7860 - val_loss: 0.4365 - val_binary_accuracy: 0.7886\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4339 - binary_accuracy: 0.7891 - val_loss: 0.4348 - val_binary_accuracy: 0.7901\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4303 - binary_accuracy: 0.7903 - val_loss: 0.4336 - val_binary_accuracy: 0.7903\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7911 - val_loss: 0.4327 - val_binary_accuracy: 0.7907\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4289 - binary_accuracy: 0.7914 - val_loss: 0.4319 - val_binary_accuracy: 0.7911\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7909 - val_loss: 0.4312 - val_binary_accuracy: 0.7913\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4283 - binary_accuracy: 0.7914 - val_loss: 0.4307 - val_binary_accuracy: 0.7921\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7926 - val_loss: 0.4303 - val_binary_accuracy: 0.7927\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4290 - binary_accuracy: 0.7921 - val_loss: 0.4300 - val_binary_accuracy: 0.7923\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7958 - val_loss: 0.4297 - val_binary_accuracy: 0.7926\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7910 - val_loss: 0.4294 - val_binary_accuracy: 0.7926\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4268 - binary_accuracy: 0.7926 - val_loss: 0.4292 - val_binary_accuracy: 0.7925\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7934 - val_loss: 0.4290 - val_binary_accuracy: 0.7925\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 81%|  | 34/42 [15:58<02:29, 18.67s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adamax', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6161 - binary_accuracy: 0.6660 - val_loss: 0.5247 - val_binary_accuracy: 0.7339\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5117 - binary_accuracy: 0.7455 - val_loss: 0.4859 - val_binary_accuracy: 0.7614\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4799 - binary_accuracy: 0.7665 - val_loss: 0.4658 - val_binary_accuracy: 0.7725\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4617 - binary_accuracy: 0.7748 - val_loss: 0.4540 - val_binary_accuracy: 0.7799\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4505 - binary_accuracy: 0.7805 - val_loss: 0.4468 - val_binary_accuracy: 0.7842\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4424 - binary_accuracy: 0.7856 - val_loss: 0.4421 - val_binary_accuracy: 0.7863\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4403 - binary_accuracy: 0.7850 - val_loss: 0.4389 - val_binary_accuracy: 0.7881\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4375 - binary_accuracy: 0.7869 - val_loss: 0.4367 - val_binary_accuracy: 0.7890\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4339 - binary_accuracy: 0.7892 - val_loss: 0.4350 - val_binary_accuracy: 0.7898\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4303 - binary_accuracy: 0.7907 - val_loss: 0.4337 - val_binary_accuracy: 0.7904\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7911 - val_loss: 0.4328 - val_binary_accuracy: 0.7907\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7914 - val_loss: 0.4320 - val_binary_accuracy: 0.7915\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4288 - binary_accuracy: 0.7911 - val_loss: 0.4313 - val_binary_accuracy: 0.7919\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4282 - binary_accuracy: 0.7914 - val_loss: 0.4308 - val_binary_accuracy: 0.7924\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4278 - binary_accuracy: 0.7929 - val_loss: 0.4304 - val_binary_accuracy: 0.7930\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7922 - val_loss: 0.4301 - val_binary_accuracy: 0.7926\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7958 - val_loss: 0.4297 - val_binary_accuracy: 0.7930\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4291 - binary_accuracy: 0.7909 - val_loss: 0.4294 - val_binary_accuracy: 0.7925\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7928 - val_loss: 0.4292 - val_binary_accuracy: 0.7926\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7935 - val_loss: 0.4290 - val_binary_accuracy: 0.7926\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 83%| | 35/42 [16:14<02:05, 17.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adamax', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6177 - binary_accuracy: 0.6524 - val_loss: 0.5295 - val_binary_accuracy: 0.7313\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5149 - binary_accuracy: 0.7418 - val_loss: 0.4875 - val_binary_accuracy: 0.7640\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4808 - binary_accuracy: 0.7668 - val_loss: 0.4663 - val_binary_accuracy: 0.7747\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4618 - binary_accuracy: 0.7758 - val_loss: 0.4540 - val_binary_accuracy: 0.7809\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4507 - binary_accuracy: 0.7805 - val_loss: 0.4466 - val_binary_accuracy: 0.7844\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4422 - binary_accuracy: 0.7859 - val_loss: 0.4418 - val_binary_accuracy: 0.7863\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4401 - binary_accuracy: 0.7853 - val_loss: 0.4386 - val_binary_accuracy: 0.7880\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4374 - binary_accuracy: 0.7864 - val_loss: 0.4364 - val_binary_accuracy: 0.7886\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4338 - binary_accuracy: 0.7892 - val_loss: 0.4348 - val_binary_accuracy: 0.7898\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4303 - binary_accuracy: 0.7904 - val_loss: 0.4336 - val_binary_accuracy: 0.7906\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4303 - binary_accuracy: 0.7909 - val_loss: 0.4326 - val_binary_accuracy: 0.7910\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4288 - binary_accuracy: 0.7913 - val_loss: 0.4318 - val_binary_accuracy: 0.7915\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7908 - val_loss: 0.4312 - val_binary_accuracy: 0.7914\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4282 - binary_accuracy: 0.7914 - val_loss: 0.4307 - val_binary_accuracy: 0.7924\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4279 - binary_accuracy: 0.7924 - val_loss: 0.4303 - val_binary_accuracy: 0.7927\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4289 - binary_accuracy: 0.7922 - val_loss: 0.4300 - val_binary_accuracy: 0.7922\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7958 - val_loss: 0.4297 - val_binary_accuracy: 0.7928\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7910 - val_loss: 0.4294 - val_binary_accuracy: 0.7926\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4268 - binary_accuracy: 0.7926 - val_loss: 0.4292 - val_binary_accuracy: 0.7924\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7934 - val_loss: 0.4290 - val_binary_accuracy: 0.7926\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7932 - val_loss: 0.4288 - val_binary_accuracy: 0.7925\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7944 - val_loss: 0.4286 - val_binary_accuracy: 0.7930\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7941 - val_loss: 0.4285 - val_binary_accuracy: 0.7930\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7940 - val_loss: 0.4283 - val_binary_accuracy: 0.7930\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7952 - val_loss: 0.4282 - val_binary_accuracy: 0.7927\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4250 - binary_accuracy: 0.7952 - val_loss: 0.4282 - val_binary_accuracy: 0.7924\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7948 - val_loss: 0.4280 - val_binary_accuracy: 0.7930\n","Restoring model weights from the end of the best epoch.\n","Epoch 00027: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 86%| | 36/42 [16:35<01:53, 18.83s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Nadam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6058 - binary_accuracy: 0.6642 - val_loss: 0.4999 - val_binary_accuracy: 0.7526\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4864 - binary_accuracy: 0.7635 - val_loss: 0.4629 - val_binary_accuracy: 0.7776\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4563 - binary_accuracy: 0.7810 - val_loss: 0.4465 - val_binary_accuracy: 0.7857\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4424 - binary_accuracy: 0.7866 - val_loss: 0.4383 - val_binary_accuracy: 0.7889\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4349 - binary_accuracy: 0.7896 - val_loss: 0.4338 - val_binary_accuracy: 0.7909\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4295 - binary_accuracy: 0.7928 - val_loss: 0.4313 - val_binary_accuracy: 0.7924\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4294 - binary_accuracy: 0.7917 - val_loss: 0.4298 - val_binary_accuracy: 0.7936\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4281 - binary_accuracy: 0.7922 - val_loss: 0.4288 - val_binary_accuracy: 0.7935\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7934 - val_loss: 0.4282 - val_binary_accuracy: 0.7936\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7954 - val_loss: 0.4279 - val_binary_accuracy: 0.7926\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7954 - val_loss: 0.4276 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4232 - binary_accuracy: 0.7957 - val_loss: 0.4275 - val_binary_accuracy: 0.7927\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7947 - val_loss: 0.4273 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7942 - val_loss: 0.4271 - val_binary_accuracy: 0.7930\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 88%| | 37/42 [16:47<01:23, 16.79s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Nadam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6101 - binary_accuracy: 0.6559 - val_loss: 0.5070 - val_binary_accuracy: 0.7485\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4922 - binary_accuracy: 0.7575 - val_loss: 0.4674 - val_binary_accuracy: 0.7756\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4603 - binary_accuracy: 0.7790 - val_loss: 0.4493 - val_binary_accuracy: 0.7838\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4449 - binary_accuracy: 0.7856 - val_loss: 0.4400 - val_binary_accuracy: 0.7875\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4366 - binary_accuracy: 0.7892 - val_loss: 0.4347 - val_binary_accuracy: 0.7900\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7925 - val_loss: 0.4318 - val_binary_accuracy: 0.7918\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4299 - binary_accuracy: 0.7916 - val_loss: 0.4300 - val_binary_accuracy: 0.7926\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7922 - val_loss: 0.4289 - val_binary_accuracy: 0.7935\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7935 - val_loss: 0.4283 - val_binary_accuracy: 0.7937\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7955 - val_loss: 0.4279 - val_binary_accuracy: 0.7928\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7955 - val_loss: 0.4276 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7957 - val_loss: 0.4275 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7948 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7942 - val_loss: 0.4272 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\r 90%| | 38/42 [16:59<01:01, 15.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Nadam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6058 - binary_accuracy: 0.6641 - val_loss: 0.4999 - val_binary_accuracy: 0.7527\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4864 - binary_accuracy: 0.7635 - val_loss: 0.4629 - val_binary_accuracy: 0.7776\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4563 - binary_accuracy: 0.7810 - val_loss: 0.4465 - val_binary_accuracy: 0.7856\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4424 - binary_accuracy: 0.7865 - val_loss: 0.4383 - val_binary_accuracy: 0.7889\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4349 - binary_accuracy: 0.7896 - val_loss: 0.4338 - val_binary_accuracy: 0.7909\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4295 - binary_accuracy: 0.7928 - val_loss: 0.4313 - val_binary_accuracy: 0.7924\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4294 - binary_accuracy: 0.7917 - val_loss: 0.4298 - val_binary_accuracy: 0.7936\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4281 - binary_accuracy: 0.7922 - val_loss: 0.4288 - val_binary_accuracy: 0.7935\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7934 - val_loss: 0.4282 - val_binary_accuracy: 0.7936\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7954 - val_loss: 0.4279 - val_binary_accuracy: 0.7926\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7954 - val_loss: 0.4276 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4232 - binary_accuracy: 0.7957 - val_loss: 0.4275 - val_binary_accuracy: 0.7927\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7947 - val_loss: 0.4273 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7942 - val_loss: 0.4271 - val_binary_accuracy: 0.7930\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 93%|| 39/42 [17:11<00:43, 14.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Nadam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'he_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6101 - binary_accuracy: 0.6559 - val_loss: 0.5071 - val_binary_accuracy: 0.7485\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4922 - binary_accuracy: 0.7575 - val_loss: 0.4674 - val_binary_accuracy: 0.7756\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4603 - binary_accuracy: 0.7790 - val_loss: 0.4493 - val_binary_accuracy: 0.7839\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4449 - binary_accuracy: 0.7856 - val_loss: 0.4400 - val_binary_accuracy: 0.7875\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4366 - binary_accuracy: 0.7892 - val_loss: 0.4347 - val_binary_accuracy: 0.7900\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7925 - val_loss: 0.4318 - val_binary_accuracy: 0.7918\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4299 - binary_accuracy: 0.7916 - val_loss: 0.4300 - val_binary_accuracy: 0.7926\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7922 - val_loss: 0.4289 - val_binary_accuracy: 0.7935\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7935 - val_loss: 0.4283 - val_binary_accuracy: 0.7937\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7955 - val_loss: 0.4279 - val_binary_accuracy: 0.7928\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7955 - val_loss: 0.4276 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7957 - val_loss: 0.4275 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7948 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7942 - val_loss: 0.4272 - val_binary_accuracy: 0.7931\n"],"name":"stdout"},{"output_type":"stream","text":["\r 95%|| 40/42 [17:23<00:27, 13.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Nadam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6036 - binary_accuracy: 0.6736 - val_loss: 0.5011 - val_binary_accuracy: 0.7524\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4876 - binary_accuracy: 0.7634 - val_loss: 0.4641 - val_binary_accuracy: 0.7780\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4573 - binary_accuracy: 0.7808 - val_loss: 0.4472 - val_binary_accuracy: 0.7853\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4430 - binary_accuracy: 0.7867 - val_loss: 0.4388 - val_binary_accuracy: 0.7891\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4353 - binary_accuracy: 0.7892 - val_loss: 0.4341 - val_binary_accuracy: 0.7912\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4298 - binary_accuracy: 0.7924 - val_loss: 0.4315 - val_binary_accuracy: 0.7925\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4295 - binary_accuracy: 0.7914 - val_loss: 0.4299 - val_binary_accuracy: 0.7939\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4282 - binary_accuracy: 0.7921 - val_loss: 0.4289 - val_binary_accuracy: 0.7935\n","Epoch 9/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4259 - binary_accuracy: 0.7936 - val_loss: 0.4282 - val_binary_accuracy: 0.7935\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7953 - val_loss: 0.4279 - val_binary_accuracy: 0.7926\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7953 - val_loss: 0.4276 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4232 - binary_accuracy: 0.7956 - val_loss: 0.4275 - val_binary_accuracy: 0.7928\n","Restoring model weights from the end of the best epoch.\n","Epoch 00012: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 98%|| 41/42 [17:34<00:12, 12.72s/it]"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Nadam', 'reg_rate': 0.0, 'regularizer': 'L1', 'weight_initializer': 'lecun_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6060 - binary_accuracy: 0.6625 - val_loss: 0.5062 - val_binary_accuracy: 0.7499\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4917 - binary_accuracy: 0.7592 - val_loss: 0.4674 - val_binary_accuracy: 0.7759\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4603 - binary_accuracy: 0.7794 - val_loss: 0.4493 - val_binary_accuracy: 0.7834\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4449 - binary_accuracy: 0.7856 - val_loss: 0.4400 - val_binary_accuracy: 0.7875\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4366 - binary_accuracy: 0.7890 - val_loss: 0.4348 - val_binary_accuracy: 0.7904\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4304 - binary_accuracy: 0.7928 - val_loss: 0.4318 - val_binary_accuracy: 0.7922\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4299 - binary_accuracy: 0.7917 - val_loss: 0.4300 - val_binary_accuracy: 0.7927\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7924 - val_loss: 0.4289 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.7935 - val_loss: 0.4283 - val_binary_accuracy: 0.7937\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7955 - val_loss: 0.4279 - val_binary_accuracy: 0.7930\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4239 - binary_accuracy: 0.7954 - val_loss: 0.4276 - val_binary_accuracy: 0.7933\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7958 - val_loss: 0.4275 - val_binary_accuracy: 0.7926\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7948 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7942 - val_loss: 0.4272 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["100%|| 42/42 [17:46<00:00, 25.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"R7IkAuWksLjY","executionInfo":{"status":"ok","timestamp":1611303179011,"user_tz":-60,"elapsed":753,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"3b0e1621-afd8-42dc-89be-cba852f7635f"},"source":["# Get search results for all evaluated hyperparameter combinations sorted by val_accuracy\n","analyze_object = talos.Analyze(search_object)\n","analyze_object.data[[\"duration\", \"loss\", \"binary_accuracy\", \"val_loss\", \"val_binary_accuracy\", \"optimizer\", \"regularizer\", \"reg_rate\", \"weight_initializer\", \"batch_size\", \"round_epochs\"]] \\\n",".sort_values(by=\"val_binary_accuracy\", ascending=False).round(4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duration</th>\n","      <th>loss</th>\n","      <th>binary_accuracy</th>\n","      <th>val_loss</th>\n","      <th>val_binary_accuracy</th>\n","      <th>optimizer</th>\n","      <th>regularizer</th>\n","      <th>reg_rate</th>\n","      <th>weight_initializer</th>\n","      <th>batch_size</th>\n","      <th>round_epochs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12</th>\n","      <td>24.5887</td>\n","      <td>0.4226</td>\n","      <td>0.7962</td>\n","      <td>0.4268</td>\n","      <td>0.7943</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>21.6811</td>\n","      <td>0.4226</td>\n","      <td>0.7962</td>\n","      <td>0.4268</td>\n","      <td>0.7943</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_normal</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>21.1206</td>\n","      <td>0.4226</td>\n","      <td>0.7962</td>\n","      <td>0.4268</td>\n","      <td>0.7943</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_normal</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>21.9499</td>\n","      <td>0.4227</td>\n","      <td>0.7960</td>\n","      <td>0.4268</td>\n","      <td>0.7941</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_uniform</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>22.0006</td>\n","      <td>0.4227</td>\n","      <td>0.7960</td>\n","      <td>0.4268</td>\n","      <td>0.7941</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>21.9391</td>\n","      <td>0.4227</td>\n","      <td>0.7960</td>\n","      <td>0.4268</td>\n","      <td>0.7941</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_uniform</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>16.1833</td>\n","      <td>0.4228</td>\n","      <td>0.7957</td>\n","      <td>0.4270</td>\n","      <td>0.7940</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>16.5251</td>\n","      <td>0.4228</td>\n","      <td>0.7957</td>\n","      <td>0.4270</td>\n","      <td>0.7940</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_normal</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>16.1508</td>\n","      <td>0.4228</td>\n","      <td>0.7957</td>\n","      <td>0.4270</td>\n","      <td>0.7940</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_normal</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>11.1188</td>\n","      <td>0.4233</td>\n","      <td>0.7955</td>\n","      <td>0.4271</td>\n","      <td>0.7935</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>10.2316</td>\n","      <td>0.4233</td>\n","      <td>0.7955</td>\n","      <td>0.4271</td>\n","      <td>0.7935</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_uniform</td>\n","      <td>512</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>10.4208</td>\n","      <td>0.4233</td>\n","      <td>0.7954</td>\n","      <td>0.4271</td>\n","      <td>0.7934</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_uniform</td>\n","      <td>512</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>25.7271</td>\n","      <td>0.4226</td>\n","      <td>0.7958</td>\n","      <td>0.4270</td>\n","      <td>0.7934</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>25.7782</td>\n","      <td>0.4226</td>\n","      <td>0.7958</td>\n","      <td>0.4270</td>\n","      <td>0.7934</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_uniform</td>\n","      <td>512</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>27.6518</td>\n","      <td>0.4226</td>\n","      <td>0.7958</td>\n","      <td>0.4270</td>\n","      <td>0.7934</td>\n","      <td>RMSprop</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_uniform</td>\n","      <td>512</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>11.1884</td>\n","      <td>0.4237</td>\n","      <td>0.7950</td>\n","      <td>0.4274</td>\n","      <td>0.7933</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>11.9312</td>\n","      <td>0.4235</td>\n","      <td>0.7951</td>\n","      <td>0.4272</td>\n","      <td>0.7933</td>\n","      <td>Adam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>12.2603</td>\n","      <td>0.4235</td>\n","      <td>0.7951</td>\n","      <td>0.4272</td>\n","      <td>0.7933</td>\n","      <td>Adam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_uniform</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>12.0109</td>\n","      <td>0.4234</td>\n","      <td>0.7952</td>\n","      <td>0.4271</td>\n","      <td>0.7932</td>\n","      <td>Adam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11.0858</td>\n","      <td>0.4236</td>\n","      <td>0.7949</td>\n","      <td>0.4274</td>\n","      <td>0.7932</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>12.0807</td>\n","      <td>0.4234</td>\n","      <td>0.7952</td>\n","      <td>0.4271</td>\n","      <td>0.7932</td>\n","      <td>Adam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>12.0921</td>\n","      <td>0.4235</td>\n","      <td>0.7952</td>\n","      <td>0.4272</td>\n","      <td>0.7932</td>\n","      <td>Adam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_uniform</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>11.3519</td>\n","      <td>0.4236</td>\n","      <td>0.7949</td>\n","      <td>0.4274</td>\n","      <td>0.7932</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>11.8887</td>\n","      <td>0.4235</td>\n","      <td>0.7952</td>\n","      <td>0.4272</td>\n","      <td>0.7932</td>\n","      <td>Nadam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>11.6201</td>\n","      <td>0.4235</td>\n","      <td>0.7952</td>\n","      <td>0.4272</td>\n","      <td>0.7932</td>\n","      <td>Nadam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_uniform</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>11.6846</td>\n","      <td>0.4235</td>\n","      <td>0.7952</td>\n","      <td>0.4272</td>\n","      <td>0.7931</td>\n","      <td>Nadam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_uniform</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>11.8291</td>\n","      <td>0.4234</td>\n","      <td>0.7952</td>\n","      <td>0.4272</td>\n","      <td>0.7931</td>\n","      <td>Adam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>20.8400</td>\n","      <td>0.4244</td>\n","      <td>0.7944</td>\n","      <td>0.4280</td>\n","      <td>0.7930</td>\n","      <td>Adamax</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_uniform</td>\n","      <td>512</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>11.8321</td>\n","      <td>0.4234</td>\n","      <td>0.7952</td>\n","      <td>0.4271</td>\n","      <td>0.7930</td>\n","      <td>Nadam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>11.8107</td>\n","      <td>0.4234</td>\n","      <td>0.7952</td>\n","      <td>0.4271</td>\n","      <td>0.7930</td>\n","      <td>Nadam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>21.6280</td>\n","      <td>0.4242</td>\n","      <td>0.7946</td>\n","      <td>0.4279</td>\n","      <td>0.7929</td>\n","      <td>Adamax</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>22.8919</td>\n","      <td>0.4242</td>\n","      <td>0.7946</td>\n","      <td>0.4279</td>\n","      <td>0.7929</td>\n","      <td>Adamax</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_normal</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>10.4323</td>\n","      <td>0.4238</td>\n","      <td>0.7950</td>\n","      <td>0.4275</td>\n","      <td>0.7928</td>\n","      <td>Nadam</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_normal</td>\n","      <td>512</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>15.9163</td>\n","      <td>0.4256</td>\n","      <td>0.7935</td>\n","      <td>0.4290</td>\n","      <td>0.7926</td>\n","      <td>Adamax</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_normal</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>16.6978</td>\n","      <td>0.4257</td>\n","      <td>0.7935</td>\n","      <td>0.4290</td>\n","      <td>0.7925</td>\n","      <td>Adamax</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_uniform</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>16.0211</td>\n","      <td>0.4257</td>\n","      <td>0.7935</td>\n","      <td>0.4290</td>\n","      <td>0.7925</td>\n","      <td>Adamax</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>80.5656</td>\n","      <td>0.5661</td>\n","      <td>0.7108</td>\n","      <td>0.5652</td>\n","      <td>0.7096</td>\n","      <td>Adadelta</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_normal</td>\n","      <td>512</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>77.9224</td>\n","      <td>0.5679</td>\n","      <td>0.7076</td>\n","      <td>0.5668</td>\n","      <td>0.7062</td>\n","      <td>Adadelta</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>78.6440</td>\n","      <td>0.5679</td>\n","      <td>0.7076</td>\n","      <td>0.5668</td>\n","      <td>0.7061</td>\n","      <td>Adadelta</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_normal</td>\n","      <td>512</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>78.7650</td>\n","      <td>0.5733</td>\n","      <td>0.6890</td>\n","      <td>0.5727</td>\n","      <td>0.6918</td>\n","      <td>Adadelta</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>lecun_uniform</td>\n","      <td>512</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>77.9285</td>\n","      <td>0.5786</td>\n","      <td>0.6813</td>\n","      <td>0.5779</td>\n","      <td>0.6847</td>\n","      <td>Adadelta</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>82.2565</td>\n","      <td>0.5787</td>\n","      <td>0.6813</td>\n","      <td>0.5780</td>\n","      <td>0.6847</td>\n","      <td>Adadelta</td>\n","      <td>L1</td>\n","      <td>0.0</td>\n","      <td>he_uniform</td>\n","      <td>512</td>\n","      <td>100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    duration    loss  binary_accuracy  val_loss  val_binary_accuracy  \\\n","12   24.5887  0.4226           0.7962    0.4268               0.7943   \n","14   21.6811  0.4226           0.7962    0.4268               0.7943   \n","16   21.1206  0.4226           0.7962    0.4268               0.7943   \n","27   21.9499  0.4227           0.7960    0.4268               0.7941   \n","25   22.0006  0.4227           0.7960    0.4268               0.7941   \n","29   21.9391  0.4227           0.7960    0.4268               0.7941   \n","6    16.1833  0.4228           0.7957    0.4270               0.7940   \n","10   16.5251  0.4228           0.7957    0.4270               0.7940   \n","8    16.1508  0.4228           0.7957    0.4270               0.7940   \n","13   11.1188  0.4233           0.7955    0.4271               0.7935   \n","15   10.2316  0.4233           0.7955    0.4271               0.7935   \n","17   10.4208  0.4233           0.7954    0.4271               0.7934   \n","7    25.7271  0.4226           0.7958    0.4270               0.7934   \n","9    25.7782  0.4226           0.7958    0.4270               0.7934   \n","11   27.6518  0.4226           0.7958    0.4270               0.7934   \n","28   11.1884  0.4237           0.7950    0.4274               0.7933   \n","19   11.9312  0.4235           0.7951    0.4272               0.7933   \n","21   12.2603  0.4235           0.7951    0.4272               0.7933   \n","18   12.0109  0.4234           0.7952    0.4271               0.7932   \n","24   11.0858  0.4236           0.7949    0.4274               0.7932   \n","20   12.0807  0.4234           0.7952    0.4271               0.7932   \n","23   12.0921  0.4235           0.7952    0.4272               0.7932   \n","26   11.3519  0.4236           0.7949    0.4274               0.7932   \n","37   11.8887  0.4235           0.7952    0.4272               0.7932   \n","41   11.6201  0.4235           0.7952    0.4272               0.7932   \n","39   11.6846  0.4235           0.7952    0.4272               0.7931   \n","22   11.8291  0.4234           0.7952    0.4272               0.7931   \n","35   20.8400  0.4244           0.7944    0.4280               0.7930   \n","38   11.8321  0.4234           0.7952    0.4271               0.7930   \n","36   11.8107  0.4234           0.7952    0.4271               0.7930   \n","30   21.6280  0.4242           0.7946    0.4279               0.7929   \n","32   22.8919  0.4242           0.7946    0.4279               0.7929   \n","40   10.4323  0.4238           0.7950    0.4275               0.7928   \n","34   15.9163  0.4256           0.7935    0.4290               0.7926   \n","33   16.6978  0.4257           0.7935    0.4290               0.7925   \n","31   16.0211  0.4257           0.7935    0.4290               0.7925   \n","4    80.5656  0.5661           0.7108    0.5652               0.7096   \n","0    77.9224  0.5679           0.7076    0.5668               0.7062   \n","2    78.6440  0.5679           0.7076    0.5668               0.7061   \n","5    78.7650  0.5733           0.6890    0.5727               0.6918   \n","1    77.9285  0.5786           0.6813    0.5779               0.6847   \n","3    82.2565  0.5787           0.6813    0.5780               0.6847   \n","\n","           optimizer regularizer  reg_rate weight_initializer  batch_size  \\\n","12  RMSprop_centered          L1       0.0      glorot_normal         512   \n","14  RMSprop_centered          L1       0.0          he_normal         512   \n","16  RMSprop_centered          L1       0.0       lecun_normal         512   \n","27      Adam_amsgrad          L1       0.0         he_uniform         512   \n","25      Adam_amsgrad          L1       0.0     glorot_uniform         512   \n","29      Adam_amsgrad          L1       0.0      lecun_uniform         512   \n","6            RMSprop          L1       0.0      glorot_normal         512   \n","10           RMSprop          L1       0.0       lecun_normal         512   \n","8            RMSprop          L1       0.0          he_normal         512   \n","13  RMSprop_centered          L1       0.0     glorot_uniform         512   \n","15  RMSprop_centered          L1       0.0         he_uniform         512   \n","17  RMSprop_centered          L1       0.0      lecun_uniform         512   \n","7            RMSprop          L1       0.0     glorot_uniform         512   \n","9            RMSprop          L1       0.0         he_uniform         512   \n","11           RMSprop          L1       0.0      lecun_uniform         512   \n","28      Adam_amsgrad          L1       0.0       lecun_normal         512   \n","19              Adam          L1       0.0     glorot_uniform         512   \n","21              Adam          L1       0.0         he_uniform         512   \n","18              Adam          L1       0.0      glorot_normal         512   \n","24      Adam_amsgrad          L1       0.0      glorot_normal         512   \n","20              Adam          L1       0.0          he_normal         512   \n","23              Adam          L1       0.0      lecun_uniform         512   \n","26      Adam_amsgrad          L1       0.0          he_normal         512   \n","37             Nadam          L1       0.0     glorot_uniform         512   \n","41             Nadam          L1       0.0      lecun_uniform         512   \n","39             Nadam          L1       0.0         he_uniform         512   \n","22              Adam          L1       0.0       lecun_normal         512   \n","35            Adamax          L1       0.0      lecun_uniform         512   \n","38             Nadam          L1       0.0          he_normal         512   \n","36             Nadam          L1       0.0      glorot_normal         512   \n","30            Adamax          L1       0.0      glorot_normal         512   \n","32            Adamax          L1       0.0          he_normal         512   \n","40             Nadam          L1       0.0       lecun_normal         512   \n","34            Adamax          L1       0.0       lecun_normal         512   \n","33            Adamax          L1       0.0         he_uniform         512   \n","31            Adamax          L1       0.0     glorot_uniform         512   \n","4           Adadelta          L1       0.0       lecun_normal         512   \n","0           Adadelta          L1       0.0      glorot_normal         512   \n","2           Adadelta          L1       0.0          he_normal         512   \n","5           Adadelta          L1       0.0      lecun_uniform         512   \n","1           Adadelta          L1       0.0     glorot_uniform         512   \n","3           Adadelta          L1       0.0         he_uniform         512   \n","\n","    round_epochs  \n","12            28  \n","14            28  \n","16            28  \n","27            28  \n","25            28  \n","29            28  \n","6             20  \n","10            20  \n","8             20  \n","13            13  \n","15            13  \n","17            13  \n","7             33  \n","9             33  \n","11            33  \n","28            14  \n","19            14  \n","21            14  \n","18            14  \n","24            14  \n","20            14  \n","23            14  \n","26            14  \n","37            14  \n","41            14  \n","39            14  \n","22            14  \n","35            27  \n","38            14  \n","36            14  \n","30            28  \n","32            28  \n","40            12  \n","34            20  \n","33            20  \n","31            20  \n","4            100  \n","0            100  \n","2            100  \n","5            100  \n","1            100  \n","3            100  "]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"PFUAEnEYufMG"},"source":["Results:\n","- Adadelta perforemd notably inferior to the other optimizers -> i.e. drop\n","- also, Adamax and Nadam performed a little bit worse than the other optimizers in this setting -> i.e. drop\n","- RMSprop and Adam (plus their modified versions performed the best)\n","- no big difference between the truncated initializers (probably neglectible as long as the distribution is truncated\n","- in general: RMSprop works better with Normal and Adam better with Uniform distribution"]},{"cell_type":"markdown","metadata":{"id":"a3YY_dnKv8Rj"},"source":["##### Test if adding regularization can help to improve performance:"]},{"cell_type":"code","metadata":{"id":"AhA1vAFlwDyR"},"source":["# Define the logistic regression (i.e. NN without hidden layer and 1 sigmoid output neuron)\n","def logistic_regression(X_subtrain, y_subtrain, X_val, y_val, params):\n","  \n","  # Get hyperparameters from parameter_grid\n","  optimizer = params[\"optimizer\"]\n","  regularizer = params[\"regularizer\"]\n","  reg_rate = params[\"reg_rate\"]\n","  weight_initializer = params[\"weight_initializer\"]\n","  batch_size = params[\"batch_size\"]\n","  if regularizer==\"L1\": reg = L1(reg_rate)\n","  if regularizer==\"L2\": reg = L2(reg_rate)\n","  if weight_initializer==\"glorot_normal\" : init = GlorotNormal(seed=seed_value)\n","  if weight_initializer==\"glorot_uniform\" : init = GlorotUniform(seed=seed_value)\n","  if weight_initializer==\"he_normal\" : init = HeNormal(seed=seed_value)\n","  if weight_initializer==\"he_uniform\" : init = HeUniform(seed=seed_value)\n","  if weight_initializer==\"lecun_normal\" : init = LecunNormal(seed=seed_value)\n","  if weight_initializer==\"lecun_uniform\" : init = LecunUniform(seed=seed_value)\n","  if optimizer==\"Adadelta\": opt = Adadelta()\n","  if optimizer==\"RMSprop\": opt = RMSprop()\n","  if optimizer==\"RMSprop_centered\": opt = RMSprop(centered=True)\n","  if optimizer==\"Adam\": opt = Adam()\n","  if optimizer==\"Adam_amsgrad\": opt = Adam(amsgrad=True)\n","  if optimizer==\"Adamax\": opt = Adamax()\n","  if optimizer==\"Nadam\": opt = Nadam()\n","\n","  # Define Model\n","  model = Sequential()\n","  model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=reg, kernel_initializer=init))\n","\n","  # Compile Model\n","  model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n","  \n","  # Fit Model to Training Data\n","  history = model.fit(X_subtrain, y_subtrain, validation_data=(X_val, y_val), batch_size=batch_size, epochs=100, shuffle=True, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)])\n","  return history, model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6aMb1JUwDyT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611304607503,"user_tz":-60,"elapsed":656055,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"4dbd1896-fbfb-458e-e784-b415753a771f"},"source":["# Run Hyperparameter Search\n","params = {\n","    \"optimizer\" : [\"RMSprop_centered\", \"Adam_amsgrad\"],\n","    \"regularizer\" : [\"L1\", \"L2\"],\n","    \"reg_rate\" : [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100],\n","    \"weight_initializer\" : [\"glorot_normal\", \"glorot_uniform\"],\n","    \"batch_size\" : [512]\n","}\n","search_object = talos.Scan(x=X_subtrain, y=y_subtrain, x_val=X_val, y_val=y_val, model=logistic_regression, params=params, experiment_name=\"test1\", seed=seed_value, print_params=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0001, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5532 - binary_accuracy: 0.7086 - val_loss: 0.4544 - val_binary_accuracy: 0.7801\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4478 - binary_accuracy: 0.7821 - val_loss: 0.4423 - val_binary_accuracy: 0.7872\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4384 - binary_accuracy: 0.7889 - val_loss: 0.4383 - val_binary_accuracy: 0.7900\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4354 - binary_accuracy: 0.7901 - val_loss: 0.4367 - val_binary_accuracy: 0.7899\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4339 - binary_accuracy: 0.7915 - val_loss: 0.4356 - val_binary_accuracy: 0.7915\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4319 - binary_accuracy: 0.7932 - val_loss: 0.4352 - val_binary_accuracy: 0.7917\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4337 - binary_accuracy: 0.7904 - val_loss: 0.4349 - val_binary_accuracy: 0.7922\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4335 - binary_accuracy: 0.7914 - val_loss: 0.4346 - val_binary_accuracy: 0.7917\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4318 - binary_accuracy: 0.7923 - val_loss: 0.4345 - val_binary_accuracy: 0.7926\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4296 - binary_accuracy: 0.7939 - val_loss: 0.4345 - val_binary_accuracy: 0.7920\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7943 - val_loss: 0.4343 - val_binary_accuracy: 0.7922\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4303 - binary_accuracy: 0.7943 - val_loss: 0.4343 - val_binary_accuracy: 0.7926\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7937 - val_loss: 0.4342 - val_binary_accuracy: 0.7922\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4310 - binary_accuracy: 0.7930 - val_loss: 0.4341 - val_binary_accuracy: 0.7923\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4312 - binary_accuracy: 0.7941 - val_loss: 0.4343 - val_binary_accuracy: 0.7928\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4326 - binary_accuracy: 0.7932 - val_loss: 0.4343 - val_binary_accuracy: 0.7922\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4270 - binary_accuracy: 0.7970 - val_loss: 0.4343 - val_binary_accuracy: 0.7933\n","Epoch 18/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4336 - binary_accuracy: 0.7917 - val_loss: 0.4342 - val_binary_accuracy: 0.7925\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4315 - binary_accuracy: 0.7933 - val_loss: 0.4342 - val_binary_accuracy: 0.7925\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4300 - binary_accuracy: 0.7937 - val_loss: 0.4342 - val_binary_accuracy: 0.7928\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4310 - binary_accuracy: 0.7936 - val_loss: 0.4340 - val_binary_accuracy: 0.7926\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4290 - binary_accuracy: 0.7950 - val_loss: 0.4340 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  2%|         | 1/64 [00:18<19:26, 18.52s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00022: early stopping\n","{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0001, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5554 - binary_accuracy: 0.7034 - val_loss: 0.4536 - val_binary_accuracy: 0.7802\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4469 - binary_accuracy: 0.7822 - val_loss: 0.4418 - val_binary_accuracy: 0.7868\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4380 - binary_accuracy: 0.7888 - val_loss: 0.4380 - val_binary_accuracy: 0.7904\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4352 - binary_accuracy: 0.7903 - val_loss: 0.4365 - val_binary_accuracy: 0.7904\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4338 - binary_accuracy: 0.7913 - val_loss: 0.4355 - val_binary_accuracy: 0.7915\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4318 - binary_accuracy: 0.7931 - val_loss: 0.4351 - val_binary_accuracy: 0.7917\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4336 - binary_accuracy: 0.7905 - val_loss: 0.4349 - val_binary_accuracy: 0.7923\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4334 - binary_accuracy: 0.7914 - val_loss: 0.4346 - val_binary_accuracy: 0.7921\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4318 - binary_accuracy: 0.7924 - val_loss: 0.4345 - val_binary_accuracy: 0.7929\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4296 - binary_accuracy: 0.7938 - val_loss: 0.4345 - val_binary_accuracy: 0.7917\n","Epoch 11/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4307 - binary_accuracy: 0.7943 - val_loss: 0.4343 - val_binary_accuracy: 0.7924\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4302 - binary_accuracy: 0.7941 - val_loss: 0.4343 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4308 - binary_accuracy: 0.7936 - val_loss: 0.4342 - val_binary_accuracy: 0.7923\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4310 - binary_accuracy: 0.7926 - val_loss: 0.4341 - val_binary_accuracy: 0.7926\n","Epoch 15/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4312 - binary_accuracy: 0.7940 - val_loss: 0.4343 - val_binary_accuracy: 0.7928\n","Epoch 16/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4326 - binary_accuracy: 0.7931 - val_loss: 0.4343 - val_binary_accuracy: 0.7922\n","Epoch 17/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4269 - binary_accuracy: 0.7969 - val_loss: 0.4343 - val_binary_accuracy: 0.7932\n","Epoch 18/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4336 - binary_accuracy: 0.7917 - val_loss: 0.4342 - val_binary_accuracy: 0.7924\n","Epoch 19/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4315 - binary_accuracy: 0.7931 - val_loss: 0.4342 - val_binary_accuracy: 0.7925\n","Epoch 20/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4300 - binary_accuracy: 0.7939 - val_loss: 0.4342 - val_binary_accuracy: 0.7928\n","Epoch 21/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4310 - binary_accuracy: 0.7937 - val_loss: 0.4340 - val_binary_accuracy: 0.7928\n","Epoch 22/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4289 - binary_accuracy: 0.7950 - val_loss: 0.4340 - val_binary_accuracy: 0.7933\n","Epoch 23/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4291 - binary_accuracy: 0.7945 - val_loss: 0.4341 - val_binary_accuracy: 0.7932\n","Epoch 24/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4299 - binary_accuracy: 0.7934 - val_loss: 0.4339 - val_binary_accuracy: 0.7926\n","Epoch 25/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4285 - binary_accuracy: 0.7950 - val_loss: 0.4340 - val_binary_accuracy: 0.7928\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4309 - binary_accuracy: 0.7944 - val_loss: 0.4342 - val_binary_accuracy: 0.7926\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4293 - binary_accuracy: 0.7942 - val_loss: 0.4342 - val_binary_accuracy: 0.7932\n","Restoring model weights from the end of the best epoch.\n","Epoch 00027: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  3%|         | 2/64 [00:43<21:00, 20.34s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0001, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5517 - binary_accuracy: 0.7087 - val_loss: 0.4522 - val_binary_accuracy: 0.7807\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4454 - binary_accuracy: 0.7826 - val_loss: 0.4398 - val_binary_accuracy: 0.7878\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4357 - binary_accuracy: 0.7895 - val_loss: 0.4357 - val_binary_accuracy: 0.7903\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4328 - binary_accuracy: 0.7910 - val_loss: 0.4342 - val_binary_accuracy: 0.7907\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4314 - binary_accuracy: 0.7921 - val_loss: 0.4333 - val_binary_accuracy: 0.7924\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4295 - binary_accuracy: 0.7937 - val_loss: 0.4330 - val_binary_accuracy: 0.7923\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4314 - binary_accuracy: 0.7911 - val_loss: 0.4328 - val_binary_accuracy: 0.7926\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4313 - binary_accuracy: 0.7921 - val_loss: 0.4326 - val_binary_accuracy: 0.7924\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4298 - binary_accuracy: 0.7927 - val_loss: 0.4325 - val_binary_accuracy: 0.7933\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4274 - binary_accuracy: 0.7943 - val_loss: 0.4326 - val_binary_accuracy: 0.7925\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4286 - binary_accuracy: 0.7946 - val_loss: 0.4324 - val_binary_accuracy: 0.7925\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4283 - binary_accuracy: 0.7947 - val_loss: 0.4325 - val_binary_accuracy: 0.7928\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4287 - binary_accuracy: 0.7939 - val_loss: 0.4324 - val_binary_accuracy: 0.7933\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4290 - binary_accuracy: 0.7932 - val_loss: 0.4323 - val_binary_accuracy: 0.7932\n","Epoch 15/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4292 - binary_accuracy: 0.7944 - val_loss: 0.4325 - val_binary_accuracy: 0.7936\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4306 - binary_accuracy: 0.7934 - val_loss: 0.4325 - val_binary_accuracy: 0.7929\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4251 - binary_accuracy: 0.7972 - val_loss: 0.4325 - val_binary_accuracy: 0.7929\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4316 - binary_accuracy: 0.7924 - val_loss: 0.4324 - val_binary_accuracy: 0.7926\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4295 - binary_accuracy: 0.7937 - val_loss: 0.4324 - val_binary_accuracy: 0.7930\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4281 - binary_accuracy: 0.7943 - val_loss: 0.4324 - val_binary_accuracy: 0.7935\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  5%|         | 3/64 [00:59<19:31, 19.20s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.0001, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5537 - binary_accuracy: 0.7034 - val_loss: 0.4515 - val_binary_accuracy: 0.7805\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4447 - binary_accuracy: 0.7822 - val_loss: 0.4394 - val_binary_accuracy: 0.7877\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4355 - binary_accuracy: 0.7893 - val_loss: 0.4355 - val_binary_accuracy: 0.7907\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4328 - binary_accuracy: 0.7909 - val_loss: 0.4341 - val_binary_accuracy: 0.7909\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4315 - binary_accuracy: 0.7918 - val_loss: 0.4333 - val_binary_accuracy: 0.7921\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4294 - binary_accuracy: 0.7934 - val_loss: 0.4330 - val_binary_accuracy: 0.7923\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4314 - binary_accuracy: 0.7911 - val_loss: 0.4328 - val_binary_accuracy: 0.7924\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4313 - binary_accuracy: 0.7921 - val_loss: 0.4326 - val_binary_accuracy: 0.7925\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4298 - binary_accuracy: 0.7926 - val_loss: 0.4326 - val_binary_accuracy: 0.7931\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4275 - binary_accuracy: 0.7944 - val_loss: 0.4326 - val_binary_accuracy: 0.7922\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4286 - binary_accuracy: 0.7944 - val_loss: 0.4324 - val_binary_accuracy: 0.7926\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4283 - binary_accuracy: 0.7946 - val_loss: 0.4325 - val_binary_accuracy: 0.7929\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4288 - binary_accuracy: 0.7939 - val_loss: 0.4324 - val_binary_accuracy: 0.7933\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4290 - binary_accuracy: 0.7931 - val_loss: 0.4323 - val_binary_accuracy: 0.7930\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7944 - val_loss: 0.4325 - val_binary_accuracy: 0.7939\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7933 - val_loss: 0.4326 - val_binary_accuracy: 0.7928\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4251 - binary_accuracy: 0.7972 - val_loss: 0.4325 - val_binary_accuracy: 0.7931\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4316 - binary_accuracy: 0.7924 - val_loss: 0.4324 - val_binary_accuracy: 0.7926\n","Epoch 19/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4295 - binary_accuracy: 0.7937 - val_loss: 0.4324 - val_binary_accuracy: 0.7931\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4281 - binary_accuracy: 0.7943 - val_loss: 0.4324 - val_binary_accuracy: 0.7935\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  6%|         | 4/64 [01:16<18:22, 18.38s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.001, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5640 - binary_accuracy: 0.7073 - val_loss: 0.4694 - val_binary_accuracy: 0.7753\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4643 - binary_accuracy: 0.7769 - val_loss: 0.4615 - val_binary_accuracy: 0.7818\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4589 - binary_accuracy: 0.7825 - val_loss: 0.4591 - val_binary_accuracy: 0.7822\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4570 - binary_accuracy: 0.7833 - val_loss: 0.4583 - val_binary_accuracy: 0.7840\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4560 - binary_accuracy: 0.7833 - val_loss: 0.4576 - val_binary_accuracy: 0.7840\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4543 - binary_accuracy: 0.7859 - val_loss: 0.4574 - val_binary_accuracy: 0.7844\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4561 - binary_accuracy: 0.7830 - val_loss: 0.4573 - val_binary_accuracy: 0.7841\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4561 - binary_accuracy: 0.7835 - val_loss: 0.4571 - val_binary_accuracy: 0.7834\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4547 - binary_accuracy: 0.7845 - val_loss: 0.4569 - val_binary_accuracy: 0.7836\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4525 - binary_accuracy: 0.7853 - val_loss: 0.4569 - val_binary_accuracy: 0.7837\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4539 - binary_accuracy: 0.7858 - val_loss: 0.4568 - val_binary_accuracy: 0.7840\n","Restoring model weights from the end of the best epoch.\n","Epoch 00011: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  8%|         | 5/64 [01:25<15:27, 15.72s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.001, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5668 - binary_accuracy: 0.7021 - val_loss: 0.4689 - val_binary_accuracy: 0.7759\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4635 - binary_accuracy: 0.7776 - val_loss: 0.4611 - val_binary_accuracy: 0.7818\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4584 - binary_accuracy: 0.7824 - val_loss: 0.4589 - val_binary_accuracy: 0.7833\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4568 - binary_accuracy: 0.7831 - val_loss: 0.4582 - val_binary_accuracy: 0.7835\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4559 - binary_accuracy: 0.7833 - val_loss: 0.4577 - val_binary_accuracy: 0.7841\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4544 - binary_accuracy: 0.7858 - val_loss: 0.4575 - val_binary_accuracy: 0.7839\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4563 - binary_accuracy: 0.7830 - val_loss: 0.4574 - val_binary_accuracy: 0.7837\n","Epoch 8/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4563 - binary_accuracy: 0.7836 - val_loss: 0.4572 - val_binary_accuracy: 0.7836\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4548 - binary_accuracy: 0.7844 - val_loss: 0.4571 - val_binary_accuracy: 0.7839\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4527 - binary_accuracy: 0.7855 - val_loss: 0.4571 - val_binary_accuracy: 0.7836\n","Restoring model weights from the end of the best epoch.\n","Epoch 00010: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  9%|         | 6/64 [01:34<13:15, 13.72s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.001, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5541 - binary_accuracy: 0.7084 - val_loss: 0.4592 - val_binary_accuracy: 0.7786\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4538 - binary_accuracy: 0.7804 - val_loss: 0.4513 - val_binary_accuracy: 0.7857\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4485 - binary_accuracy: 0.7862 - val_loss: 0.4495 - val_binary_accuracy: 0.7870\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4475 - binary_accuracy: 0.7874 - val_loss: 0.4491 - val_binary_accuracy: 0.7872\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4470 - binary_accuracy: 0.7870 - val_loss: 0.4487 - val_binary_accuracy: 0.7877\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4455 - binary_accuracy: 0.7891 - val_loss: 0.4486 - val_binary_accuracy: 0.7878\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4476 - binary_accuracy: 0.7861 - val_loss: 0.4485 - val_binary_accuracy: 0.7874\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4475 - binary_accuracy: 0.7870 - val_loss: 0.4484 - val_binary_accuracy: 0.7873\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4462 - binary_accuracy: 0.7871 - val_loss: 0.4483 - val_binary_accuracy: 0.7878\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4441 - binary_accuracy: 0.7885 - val_loss: 0.4483 - val_binary_accuracy: 0.7872\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4454 - binary_accuracy: 0.7890 - val_loss: 0.4481 - val_binary_accuracy: 0.7878\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4451 - binary_accuracy: 0.7888 - val_loss: 0.4482 - val_binary_accuracy: 0.7875\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4458 - binary_accuracy: 0.7870 - val_loss: 0.4481 - val_binary_accuracy: 0.7877\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4459 - binary_accuracy: 0.7881 - val_loss: 0.4480 - val_binary_accuracy: 0.7876\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 11%|         | 7/64 [01:46<12:27, 13.11s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.001, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5562 - binary_accuracy: 0.7031 - val_loss: 0.4586 - val_binary_accuracy: 0.7795\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4532 - binary_accuracy: 0.7809 - val_loss: 0.4510 - val_binary_accuracy: 0.7858\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4481 - binary_accuracy: 0.7865 - val_loss: 0.4493 - val_binary_accuracy: 0.7874\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4473 - binary_accuracy: 0.7874 - val_loss: 0.4490 - val_binary_accuracy: 0.7869\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4469 - binary_accuracy: 0.7872 - val_loss: 0.4487 - val_binary_accuracy: 0.7877\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4455 - binary_accuracy: 0.7891 - val_loss: 0.4486 - val_binary_accuracy: 0.7878\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4476 - binary_accuracy: 0.7860 - val_loss: 0.4486 - val_binary_accuracy: 0.7874\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4475 - binary_accuracy: 0.7868 - val_loss: 0.4484 - val_binary_accuracy: 0.7874\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4463 - binary_accuracy: 0.7871 - val_loss: 0.4484 - val_binary_accuracy: 0.7876\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4442 - binary_accuracy: 0.7886 - val_loss: 0.4484 - val_binary_accuracy: 0.7872\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4454 - binary_accuracy: 0.7889 - val_loss: 0.4482 - val_binary_accuracy: 0.7878\n","Restoring model weights from the end of the best epoch.\n","Epoch 00011: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 12%|        | 8/64 [01:55<11:06, 11.91s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.01, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6210 - binary_accuracy: 0.7028 - val_loss: 0.5173 - val_binary_accuracy: 0.7627\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5141 - binary_accuracy: 0.7631 - val_loss: 0.5124 - val_binary_accuracy: 0.7664\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5119 - binary_accuracy: 0.7643 - val_loss: 0.5102 - val_binary_accuracy: 0.7661\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5096 - binary_accuracy: 0.7651 - val_loss: 0.5093 - val_binary_accuracy: 0.7652\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5087 - binary_accuracy: 0.7645 - val_loss: 0.5085 - val_binary_accuracy: 0.7658\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5066 - binary_accuracy: 0.7677 - val_loss: 0.5085 - val_binary_accuracy: 0.7656\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5081 - binary_accuracy: 0.7647 - val_loss: 0.5085 - val_binary_accuracy: 0.7662\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 14%|        | 9/64 [02:01<09:21, 10.22s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.01, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6269 - binary_accuracy: 0.6973 - val_loss: 0.5175 - val_binary_accuracy: 0.7622\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5144 - binary_accuracy: 0.7629 - val_loss: 0.5133 - val_binary_accuracy: 0.7663\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5128 - binary_accuracy: 0.7638 - val_loss: 0.5112 - val_binary_accuracy: 0.7662\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5105 - binary_accuracy: 0.7650 - val_loss: 0.5101 - val_binary_accuracy: 0.7651\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5095 - binary_accuracy: 0.7647 - val_loss: 0.5089 - val_binary_accuracy: 0.7658\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5068 - binary_accuracy: 0.7678 - val_loss: 0.5085 - val_binary_accuracy: 0.7652\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5082 - binary_accuracy: 0.7646 - val_loss: 0.5085 - val_binary_accuracy: 0.7664\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5088 - binary_accuracy: 0.7651 - val_loss: 0.5082 - val_binary_accuracy: 0.7657\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5077 - binary_accuracy: 0.7663 - val_loss: 0.5084 - val_binary_accuracy: 0.7661\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5062 - binary_accuracy: 0.7676 - val_loss: 0.5082 - val_binary_accuracy: 0.7666\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5073 - binary_accuracy: 0.7670 - val_loss: 0.5083 - val_binary_accuracy: 0.7664\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5068 - binary_accuracy: 0.7671 - val_loss: 0.5082 - val_binary_accuracy: 0.7671\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5081 - binary_accuracy: 0.7646 - val_loss: 0.5082 - val_binary_accuracy: 0.7673\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5074 - binary_accuracy: 0.7662 - val_loss: 0.5081 - val_binary_accuracy: 0.7672\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5070 - binary_accuracy: 0.7667 - val_loss: 0.5084 - val_binary_accuracy: 0.7674\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5082 - binary_accuracy: 0.7659 - val_loss: 0.5084 - val_binary_accuracy: 0.7663\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5060 - binary_accuracy: 0.7681 - val_loss: 0.5082 - val_binary_accuracy: 0.7675\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5094 - binary_accuracy: 0.7652 - val_loss: 0.5082 - val_binary_accuracy: 0.7666\n","Epoch 19/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5079 - binary_accuracy: 0.7655 - val_loss: 0.5082 - val_binary_accuracy: 0.7671\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5077 - binary_accuracy: 0.7657 - val_loss: 0.5083 - val_binary_accuracy: 0.7667\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5071 - binary_accuracy: 0.7671 - val_loss: 0.5082 - val_binary_accuracy: 0.7668\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5068 - binary_accuracy: 0.7662 - val_loss: 0.5083 - val_binary_accuracy: 0.7675\n","Restoring model weights from the end of the best epoch.\n","Epoch 00022: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 16%|        | 10/64 [02:19<11:15, 12.52s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.01, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5676 - binary_accuracy: 0.7066 - val_loss: 0.4889 - val_binary_accuracy: 0.7712\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4860 - binary_accuracy: 0.7712 - val_loss: 0.4863 - val_binary_accuracy: 0.7748\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4852 - binary_accuracy: 0.7730 - val_loss: 0.4855 - val_binary_accuracy: 0.7750\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4845 - binary_accuracy: 0.7735 - val_loss: 0.4852 - val_binary_accuracy: 0.7737\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4842 - binary_accuracy: 0.7730 - val_loss: 0.4848 - val_binary_accuracy: 0.7737\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4825 - binary_accuracy: 0.7749 - val_loss: 0.4847 - val_binary_accuracy: 0.7742\n","Epoch 7/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4842 - binary_accuracy: 0.7724 - val_loss: 0.4846 - val_binary_accuracy: 0.7745\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4843 - binary_accuracy: 0.7735 - val_loss: 0.4844 - val_binary_accuracy: 0.7734\n","Restoring model weights from the end of the best epoch.\n","Epoch 00008: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 17%|        | 11/64 [02:26<09:40, 10.96s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.01, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5700 - binary_accuracy: 0.7011 - val_loss: 0.4886 - val_binary_accuracy: 0.7716\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4857 - binary_accuracy: 0.7716 - val_loss: 0.4865 - val_binary_accuracy: 0.7752\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4853 - binary_accuracy: 0.7731 - val_loss: 0.4857 - val_binary_accuracy: 0.7747\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4847 - binary_accuracy: 0.7735 - val_loss: 0.4854 - val_binary_accuracy: 0.7736\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4844 - binary_accuracy: 0.7729 - val_loss: 0.4850 - val_binary_accuracy: 0.7738\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4826 - binary_accuracy: 0.7750 - val_loss: 0.4848 - val_binary_accuracy: 0.7742\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4843 - binary_accuracy: 0.7724 - val_loss: 0.4847 - val_binary_accuracy: 0.7745\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 19%|        | 12/64 [02:33<08:23,  9.69s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.1, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 1.0052 - binary_accuracy: 0.6733 - val_loss: 0.6426 - val_binary_accuracy: 0.6822\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6404 - binary_accuracy: 0.6845 - val_loss: 0.6429 - val_binary_accuracy: 0.6851\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6417 - binary_accuracy: 0.6844 - val_loss: 0.6427 - val_binary_accuracy: 0.6826\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6419 - binary_accuracy: 0.6830 - val_loss: 0.6428 - val_binary_accuracy: 0.6824\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6421 - binary_accuracy: 0.6824 - val_loss: 0.6431 - val_binary_accuracy: 0.6785\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6417 - binary_accuracy: 0.6823 - val_loss: 0.6424 - val_binary_accuracy: 0.6805\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6421 - binary_accuracy: 0.6819 - val_loss: 0.6424 - val_binary_accuracy: 0.6824\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 20%|        | 13/64 [02:40<07:26,  8.76s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.1, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 1.0504 - binary_accuracy: 0.6587 - val_loss: 0.6431 - val_binary_accuracy: 0.6816\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6404 - binary_accuracy: 0.6836 - val_loss: 0.6421 - val_binary_accuracy: 0.6852\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6417 - binary_accuracy: 0.6843 - val_loss: 0.6415 - val_binary_accuracy: 0.6827\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6419 - binary_accuracy: 0.6829 - val_loss: 0.6428 - val_binary_accuracy: 0.6828\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6421 - binary_accuracy: 0.6823 - val_loss: 0.6423 - val_binary_accuracy: 0.6788\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6417 - binary_accuracy: 0.6823 - val_loss: 0.6423 - val_binary_accuracy: 0.6802\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6421 - binary_accuracy: 0.6821 - val_loss: 0.6424 - val_binary_accuracy: 0.6822\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 22%|       | 14/64 [02:46<06:44,  8.09s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.1, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6357 - binary_accuracy: 0.6986 - val_loss: 0.5586 - val_binary_accuracy: 0.7410\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5564 - binary_accuracy: 0.7430 - val_loss: 0.5568 - val_binary_accuracy: 0.7430\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5566 - binary_accuracy: 0.7412 - val_loss: 0.5562 - val_binary_accuracy: 0.7412\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5561 - binary_accuracy: 0.7397 - val_loss: 0.5562 - val_binary_accuracy: 0.7356\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5561 - binary_accuracy: 0.7380 - val_loss: 0.5560 - val_binary_accuracy: 0.7362\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5548 - binary_accuracy: 0.7394 - val_loss: 0.5561 - val_binary_accuracy: 0.7361\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5558 - binary_accuracy: 0.7375 - val_loss: 0.5561 - val_binary_accuracy: 0.7367\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 23%|       | 15/64 [02:52<06:07,  7.50s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0.1, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6382 - binary_accuracy: 0.6920 - val_loss: 0.5589 - val_binary_accuracy: 0.7414\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5567 - binary_accuracy: 0.7432 - val_loss: 0.5569 - val_binary_accuracy: 0.7432\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5567 - binary_accuracy: 0.7414 - val_loss: 0.5562 - val_binary_accuracy: 0.7411\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5561 - binary_accuracy: 0.7397 - val_loss: 0.5562 - val_binary_accuracy: 0.7357\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5561 - binary_accuracy: 0.7381 - val_loss: 0.5560 - val_binary_accuracy: 0.7362\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5548 - binary_accuracy: 0.7394 - val_loss: 0.5561 - val_binary_accuracy: 0.7361\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5558 - binary_accuracy: 0.7376 - val_loss: 0.5561 - val_binary_accuracy: 0.7368\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 25%|       | 16/64 [02:59<05:45,  7.21s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5514 - binary_accuracy: 0.7087 - val_loss: 0.4512 - val_binary_accuracy: 0.7807\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4441 - binary_accuracy: 0.7828 - val_loss: 0.4377 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4334 - binary_accuracy: 0.7899 - val_loss: 0.4328 - val_binary_accuracy: 0.7909\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7916 - val_loss: 0.4307 - val_binary_accuracy: 0.7917\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4276 - binary_accuracy: 0.7928 - val_loss: 0.4293 - val_binary_accuracy: 0.7929\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7946 - val_loss: 0.4286 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7925 - val_loss: 0.4281 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4264 - binary_accuracy: 0.7936 - val_loss: 0.4277 - val_binary_accuracy: 0.7938\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4247 - binary_accuracy: 0.7941 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 10/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4220 - binary_accuracy: 0.7958 - val_loss: 0.4274 - val_binary_accuracy: 0.7934\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7958 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7959 - val_loss: 0.4272 - val_binary_accuracy: 0.7932\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7951 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7949 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7957 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7950 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4190 - binary_accuracy: 0.7987 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7943 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 19/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4218 - binary_accuracy: 0.7961 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7960 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4212 - binary_accuracy: 0.7968 - val_loss: 0.4269 - val_binary_accuracy: 0.7942\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4211 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7946\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7955 - val_loss: 0.4267 - val_binary_accuracy: 0.7945\n","Epoch 25/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4205 - binary_accuracy: 0.7973 - val_loss: 0.4268 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7967 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4215 - binary_accuracy: 0.7964 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7956 - val_loss: 0.4268 - val_binary_accuracy: 0.7943\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 27%|       | 17/64 [03:23<09:30, 12.13s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5534 - binary_accuracy: 0.7034 - val_loss: 0.4504 - val_binary_accuracy: 0.7809\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4434 - binary_accuracy: 0.7824 - val_loss: 0.4374 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4333 - binary_accuracy: 0.7898 - val_loss: 0.4327 - val_binary_accuracy: 0.7908\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7917 - val_loss: 0.4306 - val_binary_accuracy: 0.7915\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4278 - binary_accuracy: 0.7924 - val_loss: 0.4293 - val_binary_accuracy: 0.7926\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7943 - val_loss: 0.4287 - val_binary_accuracy: 0.7930\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4269 - binary_accuracy: 0.7922 - val_loss: 0.4282 - val_binary_accuracy: 0.7934\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7934 - val_loss: 0.4278 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4248 - binary_accuracy: 0.7935 - val_loss: 0.4276 - val_binary_accuracy: 0.7929\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7958 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7956 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7961 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 13/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4229 - binary_accuracy: 0.7951 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Restoring model weights from the end of the best epoch.\n","Epoch 00013: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 28%|       | 18/64 [03:34<09:10, 11.97s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5514 - binary_accuracy: 0.7087 - val_loss: 0.4512 - val_binary_accuracy: 0.7807\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4441 - binary_accuracy: 0.7828 - val_loss: 0.4377 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4334 - binary_accuracy: 0.7899 - val_loss: 0.4328 - val_binary_accuracy: 0.7909\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7916 - val_loss: 0.4307 - val_binary_accuracy: 0.7917\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4276 - binary_accuracy: 0.7928 - val_loss: 0.4293 - val_binary_accuracy: 0.7929\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7946 - val_loss: 0.4286 - val_binary_accuracy: 0.7928\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7925 - val_loss: 0.4281 - val_binary_accuracy: 0.7932\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4264 - binary_accuracy: 0.7936 - val_loss: 0.4277 - val_binary_accuracy: 0.7938\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4247 - binary_accuracy: 0.7941 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7958 - val_loss: 0.4274 - val_binary_accuracy: 0.7934\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7958 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4227 - binary_accuracy: 0.7959 - val_loss: 0.4272 - val_binary_accuracy: 0.7932\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7951 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7949 - val_loss: 0.4270 - val_binary_accuracy: 0.7934\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7957 - val_loss: 0.4271 - val_binary_accuracy: 0.7941\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4249 - binary_accuracy: 0.7950 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4190 - binary_accuracy: 0.7987 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4258 - binary_accuracy: 0.7943 - val_loss: 0.4270 - val_binary_accuracy: 0.7938\n","Epoch 19/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7939\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4218 - binary_accuracy: 0.7961 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7960 - val_loss: 0.4269 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4212 - binary_accuracy: 0.7968 - val_loss: 0.4269 - val_binary_accuracy: 0.7942\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4211 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7946\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4220 - binary_accuracy: 0.7955 - val_loss: 0.4267 - val_binary_accuracy: 0.7945\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4205 - binary_accuracy: 0.7973 - val_loss: 0.4268 - val_binary_accuracy: 0.7939\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7967 - val_loss: 0.4271 - val_binary_accuracy: 0.7936\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4215 - binary_accuracy: 0.7964 - val_loss: 0.4270 - val_binary_accuracy: 0.7942\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4230 - binary_accuracy: 0.7956 - val_loss: 0.4268 - val_binary_accuracy: 0.7943\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 30%|       | 19/64 [03:58<11:34, 15.44s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 0, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.5534 - binary_accuracy: 0.7034 - val_loss: 0.4504 - val_binary_accuracy: 0.7809\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4434 - binary_accuracy: 0.7824 - val_loss: 0.4374 - val_binary_accuracy: 0.7882\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4333 - binary_accuracy: 0.7898 - val_loss: 0.4327 - val_binary_accuracy: 0.7908\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4297 - binary_accuracy: 0.7917 - val_loss: 0.4306 - val_binary_accuracy: 0.7915\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4278 - binary_accuracy: 0.7924 - val_loss: 0.4293 - val_binary_accuracy: 0.7926\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7943 - val_loss: 0.4287 - val_binary_accuracy: 0.7930\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4269 - binary_accuracy: 0.7922 - val_loss: 0.4282 - val_binary_accuracy: 0.7934\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4265 - binary_accuracy: 0.7934 - val_loss: 0.4278 - val_binary_accuracy: 0.7936\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4248 - binary_accuracy: 0.7935 - val_loss: 0.4276 - val_binary_accuracy: 0.7929\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7958 - val_loss: 0.4275 - val_binary_accuracy: 0.7931\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4233 - binary_accuracy: 0.7956 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4228 - binary_accuracy: 0.7961 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4229 - binary_accuracy: 0.7951 - val_loss: 0.4271 - val_binary_accuracy: 0.7935\n","Restoring model weights from the end of the best epoch.\n","Epoch 00013: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 31%|      | 20/64 [04:09<10:23, 14.18s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 1, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 4.1226 - binary_accuracy: 0.6203 - val_loss: 0.7620 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.7620 - binary_accuracy: 0.6533 - val_loss: 0.7607 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.7633 - binary_accuracy: 0.6513 - val_loss: 0.7633 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.7639 - binary_accuracy: 0.6503 - val_loss: 0.7622 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.7643 - binary_accuracy: 0.6496 - val_loss: 0.7668 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.7646 - binary_accuracy: 0.6493 - val_loss: 0.7673 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 33%|      | 21/64 [04:15<08:30, 11.88s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 1, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 4.5702 - binary_accuracy: 0.6316 - val_loss: 0.7586 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.7620 - binary_accuracy: 0.6533 - val_loss: 0.7576 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.7633 - binary_accuracy: 0.6513 - val_loss: 0.7558 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.7638 - binary_accuracy: 0.6503 - val_loss: 0.7611 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.7643 - binary_accuracy: 0.6496 - val_loss: 0.7619 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.7645 - binary_accuracy: 0.6493 - val_loss: 0.7654 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 34%|      | 22/64 [04:21<07:04, 10.10s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 1, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.9949 - binary_accuracy: 0.6436 - val_loss: 0.6252 - val_binary_accuracy: 0.6503\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6233 - binary_accuracy: 0.6536 - val_loss: 0.6250 - val_binary_accuracy: 0.6503\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6247 - binary_accuracy: 0.6514 - val_loss: 0.6250 - val_binary_accuracy: 0.6503\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6251 - binary_accuracy: 0.6505 - val_loss: 0.6251 - val_binary_accuracy: 0.6503\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6253 - binary_accuracy: 0.6498 - val_loss: 0.6250 - val_binary_accuracy: 0.6503\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6250 - binary_accuracy: 0.6495 - val_loss: 0.6250 - val_binary_accuracy: 0.6502\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 36%|      | 23/64 [04:27<06:00,  8.80s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 1, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 1.0214 - binary_accuracy: 0.6410 - val_loss: 0.6252 - val_binary_accuracy: 0.6503\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6233 - binary_accuracy: 0.6536 - val_loss: 0.6250 - val_binary_accuracy: 0.6503\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6247 - binary_accuracy: 0.6514 - val_loss: 0.6250 - val_binary_accuracy: 0.6503\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6251 - binary_accuracy: 0.6505 - val_loss: 0.6251 - val_binary_accuracy: 0.6503\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6253 - binary_accuracy: 0.6498 - val_loss: 0.6250 - val_binary_accuracy: 0.6503\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6250 - binary_accuracy: 0.6495 - val_loss: 0.6250 - val_binary_accuracy: 0.6502\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 38%|      | 24/64 [04:33<05:16,  7.90s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 10, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 35.2163 - binary_accuracy: 0.6206 - val_loss: 1.7552 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 1.8115 - binary_accuracy: 0.6533 - val_loss: 1.7530 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 1.8127 - binary_accuracy: 0.6513 - val_loss: 1.7789 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 1.8136 - binary_accuracy: 0.6503 - val_loss: 1.7774 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 1.8140 - binary_accuracy: 0.6496 - val_loss: 1.8048 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 1.8146 - binary_accuracy: 0.6493 - val_loss: 1.7954 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 39%|      | 25/64 [04:39<04:43,  7.26s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 10, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 39.7048 - binary_accuracy: 0.6315 - val_loss: 1.7267 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 1.8113 - binary_accuracy: 0.6533 - val_loss: 1.7530 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 1.8128 - binary_accuracy: 0.6513 - val_loss: 1.7233 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 3ms/step - loss: 1.8130 - binary_accuracy: 0.6503 - val_loss: 1.7392 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 1.8136 - binary_accuracy: 0.6496 - val_loss: 1.7094 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 1.8135 - binary_accuracy: 0.6493 - val_loss: 1.6889 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 41%|      | 26/64 [04:45<04:23,  6.92s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 10, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 4.1134 - binary_accuracy: 0.6210 - val_loss: 0.6454 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6434 - binary_accuracy: 0.6533 - val_loss: 0.6454 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6447 - binary_accuracy: 0.6513 - val_loss: 0.6453 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6452 - binary_accuracy: 0.6503 - val_loss: 0.6455 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6456 - binary_accuracy: 0.6496 - val_loss: 0.6453 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6458 - binary_accuracy: 0.6493 - val_loss: 0.6454 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 42%|     | 27/64 [04:51<04:11,  6.80s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 10, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 4.3840 - binary_accuracy: 0.6310 - val_loss: 0.6454 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6434 - binary_accuracy: 0.6533 - val_loss: 0.6453 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6447 - binary_accuracy: 0.6513 - val_loss: 0.6453 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6452 - binary_accuracy: 0.6503 - val_loss: 0.6455 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6456 - binary_accuracy: 0.6496 - val_loss: 0.6453 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6458 - binary_accuracy: 0.6493 - val_loss: 0.6454 - val_binary_accuracy: 0.6501\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 44%|     | 28/64 [04:57<03:53,  6.50s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n","{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 100, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 346.1554 - binary_accuracy: 0.6205 - val_loss: 11.7239 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3063 - binary_accuracy: 0.6533 - val_loss: 11.6842 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3068 - binary_accuracy: 0.6513 - val_loss: 11.6443 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3069 - binary_accuracy: 0.6503 - val_loss: 11.6797 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3076 - binary_accuracy: 0.6496 - val_loss: 11.7891 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3090 - binary_accuracy: 0.6493 - val_loss: 11.7904 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 45%|     | 29/64 [05:03<03:39,  6.28s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 100, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 391.0528 - binary_accuracy: 0.6316 - val_loss: 10.9810 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.2987 - binary_accuracy: 0.6533 - val_loss: 11.1492 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3015 - binary_accuracy: 0.6513 - val_loss: 11.1476 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3021 - binary_accuracy: 0.6503 - val_loss: 11.0613 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3018 - binary_accuracy: 0.6496 - val_loss: 11.0517 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 12.3021 - binary_accuracy: 0.6493 - val_loss: 11.1047 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 47%|     | 30/64 [05:09<03:28,  6.13s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 100, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 35.1512 - binary_accuracy: 0.6185 - val_loss: 0.6529 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6510 - binary_accuracy: 0.6533 - val_loss: 0.6533 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6522 - binary_accuracy: 0.6513 - val_loss: 0.6527 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6528 - binary_accuracy: 0.6503 - val_loss: 0.6531 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6532 - binary_accuracy: 0.6496 - val_loss: 0.6529 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6534 - binary_accuracy: 0.6493 - val_loss: 0.6527 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 48%|     | 31/64 [05:14<03:17,  5.98s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'RMSprop_centered', 'reg_rate': 100, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 37.8678 - binary_accuracy: 0.6302 - val_loss: 0.6529 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6510 - binary_accuracy: 0.6533 - val_loss: 0.6532 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6522 - binary_accuracy: 0.6513 - val_loss: 0.6527 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6528 - binary_accuracy: 0.6503 - val_loss: 0.6532 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6532 - binary_accuracy: 0.6496 - val_loss: 0.6528 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6534 - binary_accuracy: 0.6493 - val_loss: 0.6527 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 50%|     | 32/64 [05:20<03:11,  5.98s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0001, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6034 - binary_accuracy: 0.6680 - val_loss: 0.5010 - val_binary_accuracy: 0.7534\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4881 - binary_accuracy: 0.7635 - val_loss: 0.4661 - val_binary_accuracy: 0.7776\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4600 - binary_accuracy: 0.7805 - val_loss: 0.4512 - val_binary_accuracy: 0.7851\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4475 - binary_accuracy: 0.7858 - val_loss: 0.4441 - val_binary_accuracy: 0.7871\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4410 - binary_accuracy: 0.7882 - val_loss: 0.4403 - val_binary_accuracy: 0.7894\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4362 - binary_accuracy: 0.7910 - val_loss: 0.4382 - val_binary_accuracy: 0.7910\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4365 - binary_accuracy: 0.7899 - val_loss: 0.4369 - val_binary_accuracy: 0.7914\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4355 - binary_accuracy: 0.7911 - val_loss: 0.4361 - val_binary_accuracy: 0.7918\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4332 - binary_accuracy: 0.7920 - val_loss: 0.4355 - val_binary_accuracy: 0.7926\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7930 - val_loss: 0.4353 - val_binary_accuracy: 0.7921\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4316 - binary_accuracy: 0.7935 - val_loss: 0.4350 - val_binary_accuracy: 0.7919\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4310 - binary_accuracy: 0.7935 - val_loss: 0.4349 - val_binary_accuracy: 0.7918\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4314 - binary_accuracy: 0.7930 - val_loss: 0.4347 - val_binary_accuracy: 0.7922\n","Epoch 14/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4315 - binary_accuracy: 0.7927 - val_loss: 0.4346 - val_binary_accuracy: 0.7924\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 52%|    | 33/64 [05:32<03:59,  7.72s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0001, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6088 - binary_accuracy: 0.6584 - val_loss: 0.5081 - val_binary_accuracy: 0.7499\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4938 - binary_accuracy: 0.7578 - val_loss: 0.4706 - val_binary_accuracy: 0.7757\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4638 - binary_accuracy: 0.7787 - val_loss: 0.4539 - val_binary_accuracy: 0.7833\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4498 - binary_accuracy: 0.7850 - val_loss: 0.4456 - val_binary_accuracy: 0.7868\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4425 - binary_accuracy: 0.7884 - val_loss: 0.4411 - val_binary_accuracy: 0.7887\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4370 - binary_accuracy: 0.7918 - val_loss: 0.4385 - val_binary_accuracy: 0.7902\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4368 - binary_accuracy: 0.7900 - val_loss: 0.4370 - val_binary_accuracy: 0.7915\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4356 - binary_accuracy: 0.7906 - val_loss: 0.4360 - val_binary_accuracy: 0.7920\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4332 - binary_accuracy: 0.7923 - val_loss: 0.4353 - val_binary_accuracy: 0.7923\n","Epoch 10/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4307 - binary_accuracy: 0.7934 - val_loss: 0.4350 - val_binary_accuracy: 0.7920\n","Epoch 11/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4314 - binary_accuracy: 0.7938 - val_loss: 0.4347 - val_binary_accuracy: 0.7925\n","Epoch 12/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4307 - binary_accuracy: 0.7940 - val_loss: 0.4346 - val_binary_accuracy: 0.7925\n","Epoch 13/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4312 - binary_accuracy: 0.7930 - val_loss: 0.4344 - val_binary_accuracy: 0.7927\n","Epoch 14/100\n","338/338 [==============================] - 1s 4ms/step - loss: 0.4312 - binary_accuracy: 0.7922 - val_loss: 0.4343 - val_binary_accuracy: 0.7925\n","Epoch 15/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4313 - binary_accuracy: 0.7939 - val_loss: 0.4343 - val_binary_accuracy: 0.7922\n","Epoch 16/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4327 - binary_accuracy: 0.7927 - val_loss: 0.4343 - val_binary_accuracy: 0.7925\n","Epoch 17/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4271 - binary_accuracy: 0.7970 - val_loss: 0.4343 - val_binary_accuracy: 0.7929\n","Epoch 18/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4336 - binary_accuracy: 0.7914 - val_loss: 0.4342 - val_binary_accuracy: 0.7928\n","Epoch 19/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4315 - binary_accuracy: 0.7929 - val_loss: 0.4342 - val_binary_accuracy: 0.7926\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4301 - binary_accuracy: 0.7937 - val_loss: 0.4342 - val_binary_accuracy: 0.7924\n","Epoch 21/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4309 - binary_accuracy: 0.7934 - val_loss: 0.4341 - val_binary_accuracy: 0.7927\n","Epoch 22/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4289 - binary_accuracy: 0.7948 - val_loss: 0.4341 - val_binary_accuracy: 0.7928\n","Restoring model weights from the end of the best epoch.\n","Epoch 00022: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 53%|    | 34/64 [05:53<05:46, 11.54s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0001, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6019 - binary_accuracy: 0.6680 - val_loss: 0.4989 - val_binary_accuracy: 0.7538\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4859 - binary_accuracy: 0.7639 - val_loss: 0.4635 - val_binary_accuracy: 0.7774\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4572 - binary_accuracy: 0.7810 - val_loss: 0.4484 - val_binary_accuracy: 0.7854\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4446 - binary_accuracy: 0.7863 - val_loss: 0.4412 - val_binary_accuracy: 0.7883\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4381 - binary_accuracy: 0.7888 - val_loss: 0.4376 - val_binary_accuracy: 0.7902\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4335 - binary_accuracy: 0.7920 - val_loss: 0.4355 - val_binary_accuracy: 0.7921\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4338 - binary_accuracy: 0.7905 - val_loss: 0.4344 - val_binary_accuracy: 0.7923\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4329 - binary_accuracy: 0.7915 - val_loss: 0.4337 - val_binary_accuracy: 0.7928\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4309 - binary_accuracy: 0.7923 - val_loss: 0.4332 - val_binary_accuracy: 0.7939\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4283 - binary_accuracy: 0.7942 - val_loss: 0.4330 - val_binary_accuracy: 0.7927\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4293 - binary_accuracy: 0.7938 - val_loss: 0.4328 - val_binary_accuracy: 0.7930\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4288 - binary_accuracy: 0.7944 - val_loss: 0.4328 - val_binary_accuracy: 0.7927\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4291 - binary_accuracy: 0.7934 - val_loss: 0.4326 - val_binary_accuracy: 0.7928\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4293 - binary_accuracy: 0.7929 - val_loss: 0.4325 - val_binary_accuracy: 0.7931\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 55%|    | 35/64 [06:05<05:43, 11.84s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.0001, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6072 - binary_accuracy: 0.6583 - val_loss: 0.5061 - val_binary_accuracy: 0.7500\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4916 - binary_accuracy: 0.7583 - val_loss: 0.4681 - val_binary_accuracy: 0.7761\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4612 - binary_accuracy: 0.7789 - val_loss: 0.4512 - val_binary_accuracy: 0.7834\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4471 - binary_accuracy: 0.7853 - val_loss: 0.4430 - val_binary_accuracy: 0.7879\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4399 - binary_accuracy: 0.7888 - val_loss: 0.4386 - val_binary_accuracy: 0.7891\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4344 - binary_accuracy: 0.7920 - val_loss: 0.4361 - val_binary_accuracy: 0.7909\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4344 - binary_accuracy: 0.7907 - val_loss: 0.4346 - val_binary_accuracy: 0.7918\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4332 - binary_accuracy: 0.7913 - val_loss: 0.4337 - val_binary_accuracy: 0.7924\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4310 - binary_accuracy: 0.7930 - val_loss: 0.4332 - val_binary_accuracy: 0.7931\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4284 - binary_accuracy: 0.7939 - val_loss: 0.4329 - val_binary_accuracy: 0.7930\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7942 - val_loss: 0.4327 - val_binary_accuracy: 0.7928\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4286 - binary_accuracy: 0.7945 - val_loss: 0.4326 - val_binary_accuracy: 0.7928\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4290 - binary_accuracy: 0.7932 - val_loss: 0.4325 - val_binary_accuracy: 0.7928\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4291 - binary_accuracy: 0.7931 - val_loss: 0.4323 - val_binary_accuracy: 0.7933\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4293 - binary_accuracy: 0.7944 - val_loss: 0.4324 - val_binary_accuracy: 0.7929\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7932 - val_loss: 0.4324 - val_binary_accuracy: 0.7929\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4252 - binary_accuracy: 0.7970 - val_loss: 0.4324 - val_binary_accuracy: 0.7932\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4316 - binary_accuracy: 0.7924 - val_loss: 0.4323 - val_binary_accuracy: 0.7926\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4295 - binary_accuracy: 0.7934 - val_loss: 0.4323 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 56%|    | 36/64 [06:21<06:06, 13.09s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00019: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.001, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6144 - binary_accuracy: 0.6675 - val_loss: 0.5120 - val_binary_accuracy: 0.7489\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5007 - binary_accuracy: 0.7602 - val_loss: 0.4818 - val_binary_accuracy: 0.7717\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4770 - binary_accuracy: 0.7753 - val_loss: 0.4697 - val_binary_accuracy: 0.7792\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4667 - binary_accuracy: 0.7792 - val_loss: 0.4642 - val_binary_accuracy: 0.7818\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4616 - binary_accuracy: 0.7814 - val_loss: 0.4614 - val_binary_accuracy: 0.7818\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4578 - binary_accuracy: 0.7848 - val_loss: 0.4599 - val_binary_accuracy: 0.7836\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4585 - binary_accuracy: 0.7820 - val_loss: 0.4590 - val_binary_accuracy: 0.7843\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4579 - binary_accuracy: 0.7827 - val_loss: 0.4585 - val_binary_accuracy: 0.7836\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4560 - binary_accuracy: 0.7845 - val_loss: 0.4580 - val_binary_accuracy: 0.7846\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4537 - binary_accuracy: 0.7852 - val_loss: 0.4578 - val_binary_accuracy: 0.7840\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4549 - binary_accuracy: 0.7856 - val_loss: 0.4576 - val_binary_accuracy: 0.7839\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4545 - binary_accuracy: 0.7853 - val_loss: 0.4575 - val_binary_accuracy: 0.7838\n","Epoch 13/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4553 - binary_accuracy: 0.7838 - val_loss: 0.4573 - val_binary_accuracy: 0.7838\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4552 - binary_accuracy: 0.7843 - val_loss: 0.4572 - val_binary_accuracy: 0.7839\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 58%|    | 37/64 [06:33<05:46, 12.83s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.001, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6207 - binary_accuracy: 0.6575 - val_loss: 0.5200 - val_binary_accuracy: 0.7461\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5070 - binary_accuracy: 0.7547 - val_loss: 0.4867 - val_binary_accuracy: 0.7714\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4809 - binary_accuracy: 0.7741 - val_loss: 0.4723 - val_binary_accuracy: 0.7782\n","Epoch 4/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4686 - binary_accuracy: 0.7794 - val_loss: 0.4655 - val_binary_accuracy: 0.7818\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4626 - binary_accuracy: 0.7813 - val_loss: 0.4619 - val_binary_accuracy: 0.7830\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4581 - binary_accuracy: 0.7852 - val_loss: 0.4599 - val_binary_accuracy: 0.7839\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4584 - binary_accuracy: 0.7830 - val_loss: 0.4587 - val_binary_accuracy: 0.7840\n","Epoch 8/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4576 - binary_accuracy: 0.7830 - val_loss: 0.4580 - val_binary_accuracy: 0.7834\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4556 - binary_accuracy: 0.7843 - val_loss: 0.4576 - val_binary_accuracy: 0.7844\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4534 - binary_accuracy: 0.7858 - val_loss: 0.4574 - val_binary_accuracy: 0.7841\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4544 - binary_accuracy: 0.7859 - val_loss: 0.4572 - val_binary_accuracy: 0.7841\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4540 - binary_accuracy: 0.7855 - val_loss: 0.4571 - val_binary_accuracy: 0.7837\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4548 - binary_accuracy: 0.7843 - val_loss: 0.4569 - val_binary_accuracy: 0.7836\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4548 - binary_accuracy: 0.7846 - val_loss: 0.4568 - val_binary_accuracy: 0.7836\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 59%|    | 38/64 [06:46<05:31, 12.76s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.001, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6036 - binary_accuracy: 0.6682 - val_loss: 0.5023 - val_binary_accuracy: 0.7526\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4903 - binary_accuracy: 0.7629 - val_loss: 0.4706 - val_binary_accuracy: 0.7763\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4654 - binary_accuracy: 0.7790 - val_loss: 0.4585 - val_binary_accuracy: 0.7831\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4556 - binary_accuracy: 0.7834 - val_loss: 0.4536 - val_binary_accuracy: 0.7859\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4511 - binary_accuracy: 0.7851 - val_loss: 0.4513 - val_binary_accuracy: 0.7860\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4478 - binary_accuracy: 0.7880 - val_loss: 0.4502 - val_binary_accuracy: 0.7867\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4490 - binary_accuracy: 0.7858 - val_loss: 0.4496 - val_binary_accuracy: 0.7883\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4486 - binary_accuracy: 0.7860 - val_loss: 0.4492 - val_binary_accuracy: 0.7878\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4470 - binary_accuracy: 0.7873 - val_loss: 0.4489 - val_binary_accuracy: 0.7884\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4448 - binary_accuracy: 0.7881 - val_loss: 0.4489 - val_binary_accuracy: 0.7882\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4460 - binary_accuracy: 0.7884 - val_loss: 0.4487 - val_binary_accuracy: 0.7877\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4456 - binary_accuracy: 0.7887 - val_loss: 0.4487 - val_binary_accuracy: 0.7877\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4463 - binary_accuracy: 0.7865 - val_loss: 0.4485 - val_binary_accuracy: 0.7877\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4463 - binary_accuracy: 0.7876 - val_loss: 0.4484 - val_binary_accuracy: 0.7879\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 61%|    | 39/64 [06:58<05:14, 12.56s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.001, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6090 - binary_accuracy: 0.6583 - val_loss: 0.5098 - val_binary_accuracy: 0.7494\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4963 - binary_accuracy: 0.7570 - val_loss: 0.4754 - val_binary_accuracy: 0.7753\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4694 - binary_accuracy: 0.7769 - val_loss: 0.4614 - val_binary_accuracy: 0.7823\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4579 - binary_accuracy: 0.7825 - val_loss: 0.4551 - val_binary_accuracy: 0.7851\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4524 - binary_accuracy: 0.7847 - val_loss: 0.4520 - val_binary_accuracy: 0.7867\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4484 - binary_accuracy: 0.7881 - val_loss: 0.4504 - val_binary_accuracy: 0.7871\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4491 - binary_accuracy: 0.7861 - val_loss: 0.4496 - val_binary_accuracy: 0.7873\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4485 - binary_accuracy: 0.7866 - val_loss: 0.4491 - val_binary_accuracy: 0.7880\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4468 - binary_accuracy: 0.7877 - val_loss: 0.4487 - val_binary_accuracy: 0.7877\n","Epoch 10/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4446 - binary_accuracy: 0.7883 - val_loss: 0.4486 - val_binary_accuracy: 0.7879\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4457 - binary_accuracy: 0.7887 - val_loss: 0.4484 - val_binary_accuracy: 0.7882\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4453 - binary_accuracy: 0.7889 - val_loss: 0.4484 - val_binary_accuracy: 0.7875\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4461 - binary_accuracy: 0.7869 - val_loss: 0.4483 - val_binary_accuracy: 0.7878\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4461 - binary_accuracy: 0.7877 - val_loss: 0.4482 - val_binary_accuracy: 0.7874\n","Epoch 15/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4459 - binary_accuracy: 0.7887 - val_loss: 0.4482 - val_binary_accuracy: 0.7880\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4472 - binary_accuracy: 0.7880 - val_loss: 0.4483 - val_binary_accuracy: 0.7876\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 62%|   | 40/64 [07:12<05:12, 13.04s/it]"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00016: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.01, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6907 - binary_accuracy: 0.6656 - val_loss: 0.5463 - val_binary_accuracy: 0.7453\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5379 - binary_accuracy: 0.7539 - val_loss: 0.5243 - val_binary_accuracy: 0.7609\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5221 - binary_accuracy: 0.7616 - val_loss: 0.5166 - val_binary_accuracy: 0.7642\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5153 - binary_accuracy: 0.7638 - val_loss: 0.5137 - val_binary_accuracy: 0.7646\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5128 - binary_accuracy: 0.7643 - val_loss: 0.5120 - val_binary_accuracy: 0.7655\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5096 - binary_accuracy: 0.7667 - val_loss: 0.5108 - val_binary_accuracy: 0.7658\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5103 - binary_accuracy: 0.7642 - val_loss: 0.5099 - val_binary_accuracy: 0.7664\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5102 - binary_accuracy: 0.7644 - val_loss: 0.5092 - val_binary_accuracy: 0.7648\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5084 - binary_accuracy: 0.7656 - val_loss: 0.5086 - val_binary_accuracy: 0.7663\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5064 - binary_accuracy: 0.7670 - val_loss: 0.5081 - val_binary_accuracy: 0.7663\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5070 - binary_accuracy: 0.7658 - val_loss: 0.5077 - val_binary_accuracy: 0.7657\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5063 - binary_accuracy: 0.7666 - val_loss: 0.5076 - val_binary_accuracy: 0.7665\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5075 - binary_accuracy: 0.7644 - val_loss: 0.5075 - val_binary_accuracy: 0.7662\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5067 - binary_accuracy: 0.7660 - val_loss: 0.5074 - val_binary_accuracy: 0.7667\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5062 - binary_accuracy: 0.7664 - val_loss: 0.5074 - val_binary_accuracy: 0.7668\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5074 - binary_accuracy: 0.7659 - val_loss: 0.5074 - val_binary_accuracy: 0.7661\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5052 - binary_accuracy: 0.7680 - val_loss: 0.5075 - val_binary_accuracy: 0.7667\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5086 - binary_accuracy: 0.7649 - val_loss: 0.5074 - val_binary_accuracy: 0.7668\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5071 - binary_accuracy: 0.7656 - val_loss: 0.5074 - val_binary_accuracy: 0.7667\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5069 - binary_accuracy: 0.7657 - val_loss: 0.5074 - val_binary_accuracy: 0.7666\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 64%|   | 41/64 [07:29<05:26, 14.20s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.01, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.7022 - binary_accuracy: 0.6535 - val_loss: 0.5529 - val_binary_accuracy: 0.7309\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5433 - binary_accuracy: 0.7409 - val_loss: 0.5283 - val_binary_accuracy: 0.7532\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5254 - binary_accuracy: 0.7557 - val_loss: 0.5182 - val_binary_accuracy: 0.7619\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5164 - binary_accuracy: 0.7606 - val_loss: 0.5135 - val_binary_accuracy: 0.7628\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5126 - binary_accuracy: 0.7627 - val_loss: 0.5112 - val_binary_accuracy: 0.7645\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5088 - binary_accuracy: 0.7659 - val_loss: 0.5098 - val_binary_accuracy: 0.7648\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5091 - binary_accuracy: 0.7640 - val_loss: 0.5086 - val_binary_accuracy: 0.7659\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5089 - binary_accuracy: 0.7643 - val_loss: 0.5079 - val_binary_accuracy: 0.7648\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5072 - binary_accuracy: 0.7656 - val_loss: 0.5077 - val_binary_accuracy: 0.7665\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5056 - binary_accuracy: 0.7668 - val_loss: 0.5076 - val_binary_accuracy: 0.7662\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5066 - binary_accuracy: 0.7660 - val_loss: 0.5074 - val_binary_accuracy: 0.7661\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5061 - binary_accuracy: 0.7669 - val_loss: 0.5074 - val_binary_accuracy: 0.7666\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5074 - binary_accuracy: 0.7644 - val_loss: 0.5074 - val_binary_accuracy: 0.7664\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.5066 - binary_accuracy: 0.7661 - val_loss: 0.5074 - val_binary_accuracy: 0.7670\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5062 - binary_accuracy: 0.7664 - val_loss: 0.5074 - val_binary_accuracy: 0.7673\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5074 - binary_accuracy: 0.7657 - val_loss: 0.5074 - val_binary_accuracy: 0.7662\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5052 - binary_accuracy: 0.7681 - val_loss: 0.5074 - val_binary_accuracy: 0.7672\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5086 - binary_accuracy: 0.7650 - val_loss: 0.5074 - val_binary_accuracy: 0.7671\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5071 - binary_accuracy: 0.7657 - val_loss: 0.5074 - val_binary_accuracy: 0.7667\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5069 - binary_accuracy: 0.7659 - val_loss: 0.5074 - val_binary_accuracy: 0.7668\n","Restoring model weights from the end of the best epoch.\n","Epoch 00020: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 66%|   | 42/64 [07:46<05:29, 14.99s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.01, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6143 - binary_accuracy: 0.6677 - val_loss: 0.5169 - val_binary_accuracy: 0.7474\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5079 - binary_accuracy: 0.7580 - val_loss: 0.4951 - val_binary_accuracy: 0.7677\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4925 - binary_accuracy: 0.7696 - val_loss: 0.4888 - val_binary_accuracy: 0.7725\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4875 - binary_accuracy: 0.7719 - val_loss: 0.4869 - val_binary_accuracy: 0.7729\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4857 - binary_accuracy: 0.7723 - val_loss: 0.4861 - val_binary_accuracy: 0.7743\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4837 - binary_accuracy: 0.7745 - val_loss: 0.4857 - val_binary_accuracy: 0.7743\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4852 - binary_accuracy: 0.7730 - val_loss: 0.4855 - val_binary_accuracy: 0.7746\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4852 - binary_accuracy: 0.7732 - val_loss: 0.4852 - val_binary_accuracy: 0.7735\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4841 - binary_accuracy: 0.7740 - val_loss: 0.4851 - val_binary_accuracy: 0.7746\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4824 - binary_accuracy: 0.7742 - val_loss: 0.4850 - val_binary_accuracy: 0.7742\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4833 - binary_accuracy: 0.7739 - val_loss: 0.4848 - val_binary_accuracy: 0.7740\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4829 - binary_accuracy: 0.7743 - val_loss: 0.4848 - val_binary_accuracy: 0.7729\n","Restoring model weights from the end of the best epoch.\n","Epoch 00012: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 67%|   | 43/64 [07:56<04:45, 13.59s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.01, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6202 - binary_accuracy: 0.6568 - val_loss: 0.5248 - val_binary_accuracy: 0.7433\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5139 - binary_accuracy: 0.7512 - val_loss: 0.4990 - val_binary_accuracy: 0.7663\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4954 - binary_accuracy: 0.7674 - val_loss: 0.4902 - val_binary_accuracy: 0.7715\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4884 - binary_accuracy: 0.7710 - val_loss: 0.4872 - val_binary_accuracy: 0.7722\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4859 - binary_accuracy: 0.7722 - val_loss: 0.4860 - val_binary_accuracy: 0.7736\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4835 - binary_accuracy: 0.7745 - val_loss: 0.4854 - val_binary_accuracy: 0.7746\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4848 - binary_accuracy: 0.7730 - val_loss: 0.4851 - val_binary_accuracy: 0.7746\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4848 - binary_accuracy: 0.7733 - val_loss: 0.4848 - val_binary_accuracy: 0.7735\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4836 - binary_accuracy: 0.7738 - val_loss: 0.4846 - val_binary_accuracy: 0.7749\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4820 - binary_accuracy: 0.7743 - val_loss: 0.4845 - val_binary_accuracy: 0.7737\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4828 - binary_accuracy: 0.7743 - val_loss: 0.4843 - val_binary_accuracy: 0.7737\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4824 - binary_accuracy: 0.7744 - val_loss: 0.4843 - val_binary_accuracy: 0.7729\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4834 - binary_accuracy: 0.7721 - val_loss: 0.4842 - val_binary_accuracy: 0.7734\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4829 - binary_accuracy: 0.7735 - val_loss: 0.4841 - val_binary_accuracy: 0.7735\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 69%|   | 44/64 [08:08<04:21, 13.09s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.1, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 1.3353 - binary_accuracy: 0.6546 - val_loss: 0.6484 - val_binary_accuracy: 0.7329\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6430 - binary_accuracy: 0.7290 - val_loss: 0.6369 - val_binary_accuracy: 0.7090\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6351 - binary_accuracy: 0.7042 - val_loss: 0.6344 - val_binary_accuracy: 0.6922\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6334 - binary_accuracy: 0.6904 - val_loss: 0.6338 - val_binary_accuracy: 0.6860\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6333 - binary_accuracy: 0.6848 - val_loss: 0.6340 - val_binary_accuracy: 0.6814\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6329 - binary_accuracy: 0.6828 - val_loss: 0.6340 - val_binary_accuracy: 0.6811\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 70%|   | 45/64 [08:14<03:26, 10.89s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.1, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 1.4202 - binary_accuracy: 0.6462 - val_loss: 0.6505 - val_binary_accuracy: 0.6854\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6449 - binary_accuracy: 0.6822 - val_loss: 0.6377 - val_binary_accuracy: 0.6707\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6362 - binary_accuracy: 0.6723 - val_loss: 0.6347 - val_binary_accuracy: 0.6710\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6341 - binary_accuracy: 0.6725 - val_loss: 0.6339 - val_binary_accuracy: 0.6751\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6336 - binary_accuracy: 0.6754 - val_loss: 0.6340 - val_binary_accuracy: 0.6775\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6331 - binary_accuracy: 0.6785 - val_loss: 0.6338 - val_binary_accuracy: 0.6791\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 72%|  | 46/64 [08:19<02:47,  9.30s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.1, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6925 - binary_accuracy: 0.6681 - val_loss: 0.5653 - val_binary_accuracy: 0.7395\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5619 - binary_accuracy: 0.7430 - val_loss: 0.5602 - val_binary_accuracy: 0.7457\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5598 - binary_accuracy: 0.7454 - val_loss: 0.5585 - val_binary_accuracy: 0.7442\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5582 - binary_accuracy: 0.7429 - val_loss: 0.5574 - val_binary_accuracy: 0.7412\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5573 - binary_accuracy: 0.7414 - val_loss: 0.5568 - val_binary_accuracy: 0.7394\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5555 - binary_accuracy: 0.7423 - val_loss: 0.5565 - val_binary_accuracy: 0.7390\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5562 - binary_accuracy: 0.7404 - val_loss: 0.5563 - val_binary_accuracy: 0.7407\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 73%|  | 47/64 [08:26<02:23,  8.44s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0.1, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6980 - binary_accuracy: 0.6540 - val_loss: 0.5684 - val_binary_accuracy: 0.7286\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5637 - binary_accuracy: 0.7342 - val_loss: 0.5597 - val_binary_accuracy: 0.7430\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5590 - binary_accuracy: 0.7415 - val_loss: 0.5573 - val_binary_accuracy: 0.7420\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5571 - binary_accuracy: 0.7411 - val_loss: 0.5565 - val_binary_accuracy: 0.7395\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5564 - binary_accuracy: 0.7399 - val_loss: 0.5562 - val_binary_accuracy: 0.7372\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5549 - binary_accuracy: 0.7406 - val_loss: 0.5562 - val_binary_accuracy: 0.7372\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.5558 - binary_accuracy: 0.7391 - val_loss: 0.5560 - val_binary_accuracy: 0.7387\n","Restoring model weights from the end of the best epoch.\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 75%|  | 48/64 [08:32<02:04,  7.78s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6017 - binary_accuracy: 0.6679 - val_loss: 0.4984 - val_binary_accuracy: 0.7541\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4852 - binary_accuracy: 0.7640 - val_loss: 0.4623 - val_binary_accuracy: 0.7780\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4559 - binary_accuracy: 0.7813 - val_loss: 0.4465 - val_binary_accuracy: 0.7855\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4425 - binary_accuracy: 0.7867 - val_loss: 0.4387 - val_binary_accuracy: 0.7890\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4353 - binary_accuracy: 0.7893 - val_loss: 0.4345 - val_binary_accuracy: 0.7907\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7922 - val_loss: 0.4320 - val_binary_accuracy: 0.7920\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4300 - binary_accuracy: 0.7912 - val_loss: 0.4304 - val_binary_accuracy: 0.7936\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4287 - binary_accuracy: 0.7921 - val_loss: 0.4294 - val_binary_accuracy: 0.7934\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4264 - binary_accuracy: 0.7938 - val_loss: 0.4287 - val_binary_accuracy: 0.7938\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7953 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4243 - binary_accuracy: 0.7948 - val_loss: 0.4279 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7955 - val_loss: 0.4278 - val_binary_accuracy: 0.7930\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7947 - val_loss: 0.4275 - val_binary_accuracy: 0.7934\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7939 - val_loss: 0.4274 - val_binary_accuracy: 0.7932\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 77%|  | 49/64 [08:45<02:18,  9.20s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6070 - binary_accuracy: 0.6584 - val_loss: 0.5055 - val_binary_accuracy: 0.7503\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4909 - binary_accuracy: 0.7584 - val_loss: 0.4669 - val_binary_accuracy: 0.7761\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4598 - binary_accuracy: 0.7790 - val_loss: 0.4493 - val_binary_accuracy: 0.7841\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4451 - binary_accuracy: 0.7857 - val_loss: 0.4405 - val_binary_accuracy: 0.7879\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4372 - binary_accuracy: 0.7889 - val_loss: 0.4355 - val_binary_accuracy: 0.7901\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4312 - binary_accuracy: 0.7925 - val_loss: 0.4326 - val_binary_accuracy: 0.7917\n","Epoch 7/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4307 - binary_accuracy: 0.7916 - val_loss: 0.4308 - val_binary_accuracy: 0.7922\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7924 - val_loss: 0.4295 - val_binary_accuracy: 0.7928\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7941 - val_loss: 0.4287 - val_binary_accuracy: 0.7935\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7952 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7953 - val_loss: 0.4279 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7956 - val_loss: 0.4277 - val_binary_accuracy: 0.7931\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7949 - val_loss: 0.4275 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7942 - val_loss: 0.4273 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4240 - binary_accuracy: 0.7952 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7944 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4195 - binary_accuracy: 0.7981 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4261 - binary_accuracy: 0.7932 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7952 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4214 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7965 - val_loss: 0.4269 - val_binary_accuracy: 0.7945\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7955 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7940\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7962 - val_loss: 0.4271 - val_binary_accuracy: 0.7939\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7944\n","Epoch 28/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4231 - binary_accuracy: 0.7951 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 78%|  | 50/64 [09:08<03:07, 13.36s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 0.6017 - binary_accuracy: 0.6679 - val_loss: 0.4984 - val_binary_accuracy: 0.7541\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4852 - binary_accuracy: 0.7640 - val_loss: 0.4623 - val_binary_accuracy: 0.7780\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4559 - binary_accuracy: 0.7813 - val_loss: 0.4465 - val_binary_accuracy: 0.7855\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4425 - binary_accuracy: 0.7867 - val_loss: 0.4387 - val_binary_accuracy: 0.7890\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4353 - binary_accuracy: 0.7893 - val_loss: 0.4345 - val_binary_accuracy: 0.7907\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7922 - val_loss: 0.4320 - val_binary_accuracy: 0.7920\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4300 - binary_accuracy: 0.7912 - val_loss: 0.4304 - val_binary_accuracy: 0.7936\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4287 - binary_accuracy: 0.7921 - val_loss: 0.4294 - val_binary_accuracy: 0.7934\n","Epoch 9/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4264 - binary_accuracy: 0.7938 - val_loss: 0.4287 - val_binary_accuracy: 0.7938\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4234 - binary_accuracy: 0.7953 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4243 - binary_accuracy: 0.7948 - val_loss: 0.4279 - val_binary_accuracy: 0.7932\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7955 - val_loss: 0.4278 - val_binary_accuracy: 0.7930\n","Epoch 13/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7947 - val_loss: 0.4275 - val_binary_accuracy: 0.7934\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7939 - val_loss: 0.4274 - val_binary_accuracy: 0.7932\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 80%|  | 51/64 [09:21<02:52, 13.31s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 0, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6070 - binary_accuracy: 0.6584 - val_loss: 0.5055 - val_binary_accuracy: 0.7503\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4909 - binary_accuracy: 0.7584 - val_loss: 0.4669 - val_binary_accuracy: 0.7761\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4598 - binary_accuracy: 0.7790 - val_loss: 0.4493 - val_binary_accuracy: 0.7841\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4451 - binary_accuracy: 0.7857 - val_loss: 0.4405 - val_binary_accuracy: 0.7879\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4372 - binary_accuracy: 0.7889 - val_loss: 0.4355 - val_binary_accuracy: 0.7901\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4312 - binary_accuracy: 0.7925 - val_loss: 0.4326 - val_binary_accuracy: 0.7917\n","Epoch 7/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4307 - binary_accuracy: 0.7916 - val_loss: 0.4308 - val_binary_accuracy: 0.7922\n","Epoch 8/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4292 - binary_accuracy: 0.7924 - val_loss: 0.4295 - val_binary_accuracy: 0.7928\n","Epoch 9/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4267 - binary_accuracy: 0.7941 - val_loss: 0.4287 - val_binary_accuracy: 0.7935\n","Epoch 10/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4237 - binary_accuracy: 0.7952 - val_loss: 0.4283 - val_binary_accuracy: 0.7932\n","Epoch 11/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4243 - binary_accuracy: 0.7953 - val_loss: 0.4279 - val_binary_accuracy: 0.7931\n","Epoch 12/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4236 - binary_accuracy: 0.7956 - val_loss: 0.4277 - val_binary_accuracy: 0.7931\n","Epoch 13/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7949 - val_loss: 0.4275 - val_binary_accuracy: 0.7932\n","Epoch 14/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4237 - binary_accuracy: 0.7942 - val_loss: 0.4273 - val_binary_accuracy: 0.7936\n","Epoch 15/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4240 - binary_accuracy: 0.7952 - val_loss: 0.4273 - val_binary_accuracy: 0.7933\n","Epoch 16/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4253 - binary_accuracy: 0.7944 - val_loss: 0.4273 - val_binary_accuracy: 0.7935\n","Epoch 17/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4195 - binary_accuracy: 0.7981 - val_loss: 0.4272 - val_binary_accuracy: 0.7935\n","Epoch 18/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4261 - binary_accuracy: 0.7932 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 19/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4238 - binary_accuracy: 0.7952 - val_loss: 0.4271 - val_binary_accuracy: 0.7940\n","Epoch 20/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7964 - val_loss: 0.4271 - val_binary_accuracy: 0.7937\n","Epoch 21/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4235 - binary_accuracy: 0.7958 - val_loss: 0.4270 - val_binary_accuracy: 0.7941\n","Epoch 22/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4214 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7940\n","Epoch 23/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4213 - binary_accuracy: 0.7965 - val_loss: 0.4269 - val_binary_accuracy: 0.7945\n","Epoch 24/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4222 - binary_accuracy: 0.7955 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Epoch 25/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4206 - binary_accuracy: 0.7969 - val_loss: 0.4269 - val_binary_accuracy: 0.7940\n","Epoch 26/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4232 - binary_accuracy: 0.7962 - val_loss: 0.4271 - val_binary_accuracy: 0.7939\n","Epoch 27/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.4216 - binary_accuracy: 0.7962 - val_loss: 0.4270 - val_binary_accuracy: 0.7944\n","Epoch 28/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.4231 - binary_accuracy: 0.7951 - val_loss: 0.4268 - val_binary_accuracy: 0.7941\n","Restoring model weights from the end of the best epoch.\n","Epoch 00028: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 81%| | 52/64 [09:44<03:15, 16.33s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 1, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 7.2948 - binary_accuracy: 0.5949 - val_loss: 0.6872 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6838 - binary_accuracy: 0.6533 - val_loss: 0.6758 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6749 - binary_accuracy: 0.6513 - val_loss: 0.6732 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6733 - binary_accuracy: 0.6503 - val_loss: 0.6730 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6741 - binary_accuracy: 0.6496 - val_loss: 0.6738 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6740 - binary_accuracy: 0.6493 - val_loss: 0.6715 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 83%| | 53/64 [09:50<02:24, 13.12s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 1, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 8.1368 - binary_accuracy: 0.6160 - val_loss: 0.6845 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6800 - binary_accuracy: 0.6533 - val_loss: 0.6755 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6738 - binary_accuracy: 0.6513 - val_loss: 0.6726 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6729 - binary_accuracy: 0.6503 - val_loss: 0.6744 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6730 - binary_accuracy: 0.6496 - val_loss: 0.6712 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6737 - binary_accuracy: 0.6493 - val_loss: 0.6727 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 84%| | 54/64 [09:56<01:50, 11.03s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 1, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 1.3067 - binary_accuracy: 0.6366 - val_loss: 0.6374 - val_binary_accuracy: 0.6608\n","Epoch 2/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6330 - binary_accuracy: 0.6603 - val_loss: 0.6285 - val_binary_accuracy: 0.6523\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6274 - binary_accuracy: 0.6524 - val_loss: 0.6257 - val_binary_accuracy: 0.6505\n","Epoch 4/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6256 - binary_accuracy: 0.6506 - val_loss: 0.6250 - val_binary_accuracy: 0.6504\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6253 - binary_accuracy: 0.6499 - val_loss: 0.6248 - val_binary_accuracy: 0.6503\n","Epoch 6/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6249 - binary_accuracy: 0.6495 - val_loss: 0.6248 - val_binary_accuracy: 0.6503\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 86%| | 55/64 [10:03<01:28,  9.78s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 1, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 1.3520 - binary_accuracy: 0.6278 - val_loss: 0.6340 - val_binary_accuracy: 0.6554\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6302 - binary_accuracy: 0.6573 - val_loss: 0.6268 - val_binary_accuracy: 0.6513\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6260 - binary_accuracy: 0.6518 - val_loss: 0.6251 - val_binary_accuracy: 0.6505\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6251 - binary_accuracy: 0.6506 - val_loss: 0.6249 - val_binary_accuracy: 0.6503\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6252 - binary_accuracy: 0.6498 - val_loss: 0.6248 - val_binary_accuracy: 0.6503\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6249 - binary_accuracy: 0.6494 - val_loss: 0.6248 - val_binary_accuracy: 0.6503\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 88%| | 56/64 [10:09<01:08,  8.60s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 10, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 66.8345 - binary_accuracy: 0.5949 - val_loss: 0.9608 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9458 - binary_accuracy: 0.6533 - val_loss: 0.9400 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9358 - binary_accuracy: 0.6513 - val_loss: 0.9503 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9364 - binary_accuracy: 0.6503 - val_loss: 0.9125 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9345 - binary_accuracy: 0.6496 - val_loss: 0.9353 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9341 - binary_accuracy: 0.6493 - val_loss: 0.9272 - val_binary_accuracy: 0.6501\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 89%| | 57/64 [10:15<00:54,  7.75s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 10, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 75.2664 - binary_accuracy: 0.6158 - val_loss: 0.9439 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9259 - binary_accuracy: 0.6533 - val_loss: 0.9243 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9144 - binary_accuracy: 0.6513 - val_loss: 0.8950 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9095 - binary_accuracy: 0.6503 - val_loss: 0.9079 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9086 - binary_accuracy: 0.6496 - val_loss: 0.8943 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.9085 - binary_accuracy: 0.6493 - val_loss: 0.9118 - val_binary_accuracy: 0.6501\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r 91%| | 58/64 [10:21<00:43,  7.21s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 10, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 7.0911 - binary_accuracy: 0.5973 - val_loss: 0.6700 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6573 - binary_accuracy: 0.6533 - val_loss: 0.6476 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6462 - binary_accuracy: 0.6513 - val_loss: 0.6451 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6449 - binary_accuracy: 0.6503 - val_loss: 0.6447 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6450 - binary_accuracy: 0.6496 - val_loss: 0.6447 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6451 - binary_accuracy: 0.6493 - val_loss: 0.6446 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 92%|| 59/64 [10:26<00:34,  6.82s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 10, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 7.5453 - binary_accuracy: 0.6165 - val_loss: 0.6559 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6507 - binary_accuracy: 0.6533 - val_loss: 0.6462 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6451 - binary_accuracy: 0.6513 - val_loss: 0.6448 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6447 - binary_accuracy: 0.6503 - val_loss: 0.6446 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 3ms/step - loss: 0.6449 - binary_accuracy: 0.6496 - val_loss: 0.6446 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6451 - binary_accuracy: 0.6493 - val_loss: 0.6446 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 94%|| 60/64 [10:32<00:25,  6.48s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 100, 'regularizer': 'L1', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 662.2466 - binary_accuracy: 0.5950 - val_loss: 3.5494 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.5269 - binary_accuracy: 0.6533 - val_loss: 3.5649 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.5189 - binary_accuracy: 0.6513 - val_loss: 3.5707 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.5004 - binary_accuracy: 0.6503 - val_loss: 3.3833 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.4850 - binary_accuracy: 0.6496 - val_loss: 3.4736 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.4838 - binary_accuracy: 0.6493 - val_loss: 3.7649 - val_binary_accuracy: 0.6501\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 95%|| 61/64 [10:38<00:18,  6.28s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n","{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 100, 'regularizer': 'L1', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 2s 3ms/step - loss: 746.5814 - binary_accuracy: 0.6159 - val_loss: 3.6139 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.4841 - binary_accuracy: 0.6533 - val_loss: 3.4903 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 3ms/step - loss: 3.4740 - binary_accuracy: 0.6513 - val_loss: 3.4764 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.4671 - binary_accuracy: 0.6503 - val_loss: 3.3688 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.4712 - binary_accuracy: 0.6496 - val_loss: 3.5172 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 3.4380 - binary_accuracy: 0.6493 - val_loss: 3.5090 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 97%|| 62/64 [10:44<00:12,  6.08s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 100, 'regularizer': 'L2', 'weight_initializer': 'glorot_normal'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 64.8165 - binary_accuracy: 0.5909 - val_loss: 0.7836 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6963 - binary_accuracy: 0.6533 - val_loss: 0.6499 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6485 - binary_accuracy: 0.6513 - val_loss: 0.6475 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6473 - binary_accuracy: 0.6503 - val_loss: 0.6472 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6474 - binary_accuracy: 0.6496 - val_loss: 0.6471 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6476 - binary_accuracy: 0.6493 - val_loss: 0.6471 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 98%|| 63/64 [10:49<00:05,  5.98s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["{'batch_size': 512, 'optimizer': 'Adam_amsgrad', 'reg_rate': 100, 'regularizer': 'L2', 'weight_initializer': 'glorot_uniform'}\n","Epoch 1/100\n","338/338 [==============================] - 1s 3ms/step - loss: 69.3714 - binary_accuracy: 0.6149 - val_loss: 0.6692 - val_binary_accuracy: 0.6501\n","Epoch 2/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6558 - binary_accuracy: 0.6533 - val_loss: 0.6486 - val_binary_accuracy: 0.6501\n","Epoch 3/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6475 - binary_accuracy: 0.6513 - val_loss: 0.6472 - val_binary_accuracy: 0.6501\n","Epoch 4/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6471 - binary_accuracy: 0.6503 - val_loss: 0.6471 - val_binary_accuracy: 0.6501\n","Epoch 5/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6474 - binary_accuracy: 0.6496 - val_loss: 0.6471 - val_binary_accuracy: 0.6501\n","Epoch 6/100\n","338/338 [==============================] - 1s 2ms/step - loss: 0.6476 - binary_accuracy: 0.6493 - val_loss: 0.6471 - val_binary_accuracy: 0.6501\n","Restoring model weights from the end of the best epoch.\n","Epoch 00006: early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|| 64/64 [10:55<00:00, 10.24s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"-QMIORfPwDyU","executionInfo":{"status":"ok","timestamp":1611304630932,"user_tz":-60,"elapsed":745,"user":{"displayName":"David George","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4s7QtDtwIj_fTOPO9qNQzFmkL4lFy0SvQ-yOlTw=s64","userId":"17656817416818487894"}},"outputId":"82648e8d-fd8d-48ed-9063-5797c804702f"},"source":["# Get search results for all evaluated hyperparameter combinations sorted by val_accuracy\n","analyze_object = talos.Analyze(search_object)\n","analyze_object.data[[\"duration\", \"loss\", \"binary_accuracy\", \"val_loss\", \"val_binary_accuracy\", \"optimizer\", \"regularizer\", \"reg_rate\", \"weight_initializer\", \"batch_size\", \"round_epochs\"]]\\\n",".sort_values(by=\"val_binary_accuracy\", ascending=False).round(4).head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>duration</th>\n","      <th>loss</th>\n","      <th>binary_accuracy</th>\n","      <th>val_loss</th>\n","      <th>val_binary_accuracy</th>\n","      <th>optimizer</th>\n","      <th>regularizer</th>\n","      <th>reg_rate</th>\n","      <th>weight_initializer</th>\n","      <th>batch_size</th>\n","      <th>round_epochs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>16</th>\n","      <td>23.4158</td>\n","      <td>0.4226</td>\n","      <td>0.7962</td>\n","      <td>0.4268</td>\n","      <td>0.7943</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0000</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>23.3299</td>\n","      <td>0.4226</td>\n","      <td>0.7962</td>\n","      <td>0.4268</td>\n","      <td>0.7943</td>\n","      <td>RMSprop_centered</td>\n","      <td>L2</td>\n","      <td>0.0000</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>23.1804</td>\n","      <td>0.4227</td>\n","      <td>0.7960</td>\n","      <td>0.4268</td>\n","      <td>0.7941</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L2</td>\n","      <td>0.0000</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>22.8586</td>\n","      <td>0.4227</td>\n","      <td>0.7960</td>\n","      <td>0.4268</td>\n","      <td>0.7941</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0000</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>11.0020</td>\n","      <td>0.4233</td>\n","      <td>0.7955</td>\n","      <td>0.4271</td>\n","      <td>0.7935</td>\n","      <td>RMSprop_centered</td>\n","      <td>L2</td>\n","      <td>0.0000</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>11.3855</td>\n","      <td>0.4233</td>\n","      <td>0.7955</td>\n","      <td>0.4271</td>\n","      <td>0.7935</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0000</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16.3342</td>\n","      <td>0.4287</td>\n","      <td>0.7944</td>\n","      <td>0.4324</td>\n","      <td>0.7935</td>\n","      <td>RMSprop_centered</td>\n","      <td>L2</td>\n","      <td>0.0001</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16.2596</td>\n","      <td>0.4287</td>\n","      <td>0.7943</td>\n","      <td>0.4324</td>\n","      <td>0.7935</td>\n","      <td>RMSprop_centered</td>\n","      <td>L2</td>\n","      <td>0.0001</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>24.3727</td>\n","      <td>0.4304</td>\n","      <td>0.7939</td>\n","      <td>0.4342</td>\n","      <td>0.7932</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0001</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>12.9613</td>\n","      <td>0.4236</td>\n","      <td>0.7949</td>\n","      <td>0.4274</td>\n","      <td>0.7932</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L2</td>\n","      <td>0.0000</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>12.3329</td>\n","      <td>0.4236</td>\n","      <td>0.7949</td>\n","      <td>0.4274</td>\n","      <td>0.7932</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0000</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>15.8072</td>\n","      <td>0.4287</td>\n","      <td>0.7943</td>\n","      <td>0.4323</td>\n","      <td>0.7932</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L2</td>\n","      <td>0.0001</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>18.3279</td>\n","      <td>0.4305</td>\n","      <td>0.7942</td>\n","      <td>0.4340</td>\n","      <td>0.7932</td>\n","      <td>RMSprop_centered</td>\n","      <td>L1</td>\n","      <td>0.0001</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>12.3301</td>\n","      <td>0.4291</td>\n","      <td>0.7939</td>\n","      <td>0.4325</td>\n","      <td>0.7931</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L2</td>\n","      <td>0.0001</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>20.2124</td>\n","      <td>0.4305</td>\n","      <td>0.7940</td>\n","      <td>0.4341</td>\n","      <td>0.7928</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0001</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>11.5936</td>\n","      <td>0.4313</td>\n","      <td>0.7935</td>\n","      <td>0.4346</td>\n","      <td>0.7924</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L1</td>\n","      <td>0.0001</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>11.9146</td>\n","      <td>0.4460</td>\n","      <td>0.7880</td>\n","      <td>0.4484</td>\n","      <td>0.7879</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L2</td>\n","      <td>0.0010</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8.9007</td>\n","      <td>0.4458</td>\n","      <td>0.7886</td>\n","      <td>0.4482</td>\n","      <td>0.7878</td>\n","      <td>RMSprop_centered</td>\n","      <td>L2</td>\n","      <td>0.0010</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>13.9396</td>\n","      <td>0.4457</td>\n","      <td>0.7886</td>\n","      <td>0.4483</td>\n","      <td>0.7876</td>\n","      <td>Adam_amsgrad</td>\n","      <td>L2</td>\n","      <td>0.0010</td>\n","      <td>glorot_uniform</td>\n","      <td>512</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>11.4995</td>\n","      <td>0.4456</td>\n","      <td>0.7886</td>\n","      <td>0.4480</td>\n","      <td>0.7876</td>\n","      <td>RMSprop_centered</td>\n","      <td>L2</td>\n","      <td>0.0010</td>\n","      <td>glorot_normal</td>\n","      <td>512</td>\n","      <td>14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    duration    loss  binary_accuracy  val_loss  val_binary_accuracy  \\\n","16   23.4158  0.4226           0.7962    0.4268               0.7943   \n","18   23.3299  0.4226           0.7962    0.4268               0.7943   \n","51   23.1804  0.4227           0.7960    0.4268               0.7941   \n","49   22.8586  0.4227           0.7960    0.4268               0.7941   \n","19   11.0020  0.4233           0.7955    0.4271               0.7935   \n","17   11.3855  0.4233           0.7955    0.4271               0.7935   \n","2    16.3342  0.4287           0.7944    0.4324               0.7935   \n","3    16.2596  0.4287           0.7943    0.4324               0.7935   \n","1    24.3727  0.4304           0.7939    0.4342               0.7932   \n","50   12.9613  0.4236           0.7949    0.4274               0.7932   \n","48   12.3329  0.4236           0.7949    0.4274               0.7932   \n","35   15.8072  0.4287           0.7943    0.4323               0.7932   \n","0    18.3279  0.4305           0.7942    0.4340               0.7932   \n","34   12.3301  0.4291           0.7939    0.4325               0.7931   \n","33   20.2124  0.4305           0.7940    0.4341               0.7928   \n","32   11.5936  0.4313           0.7935    0.4346               0.7924   \n","38   11.9146  0.4460           0.7880    0.4484               0.7879   \n","7     8.9007  0.4458           0.7886    0.4482               0.7878   \n","39   13.9396  0.4457           0.7886    0.4483               0.7876   \n","6    11.4995  0.4456           0.7886    0.4480               0.7876   \n","\n","           optimizer regularizer  reg_rate weight_initializer  batch_size  \\\n","16  RMSprop_centered          L1    0.0000      glorot_normal         512   \n","18  RMSprop_centered          L2    0.0000      glorot_normal         512   \n","51      Adam_amsgrad          L2    0.0000     glorot_uniform         512   \n","49      Adam_amsgrad          L1    0.0000     glorot_uniform         512   \n","19  RMSprop_centered          L2    0.0000     glorot_uniform         512   \n","17  RMSprop_centered          L1    0.0000     glorot_uniform         512   \n","2   RMSprop_centered          L2    0.0001      glorot_normal         512   \n","3   RMSprop_centered          L2    0.0001     glorot_uniform         512   \n","1   RMSprop_centered          L1    0.0001     glorot_uniform         512   \n","50      Adam_amsgrad          L2    0.0000      glorot_normal         512   \n","48      Adam_amsgrad          L1    0.0000      glorot_normal         512   \n","35      Adam_amsgrad          L2    0.0001     glorot_uniform         512   \n","0   RMSprop_centered          L1    0.0001      glorot_normal         512   \n","34      Adam_amsgrad          L2    0.0001      glorot_normal         512   \n","33      Adam_amsgrad          L1    0.0001     glorot_uniform         512   \n","32      Adam_amsgrad          L1    0.0001      glorot_normal         512   \n","38      Adam_amsgrad          L2    0.0010      glorot_normal         512   \n","7   RMSprop_centered          L2    0.0010     glorot_uniform         512   \n","39      Adam_amsgrad          L2    0.0010     glorot_uniform         512   \n","6   RMSprop_centered          L2    0.0010      glorot_normal         512   \n","\n","    round_epochs  \n","16            28  \n","18            28  \n","51            28  \n","49            28  \n","19            13  \n","17            13  \n","2             20  \n","3             20  \n","1             27  \n","50            14  \n","48            14  \n","35            19  \n","0             22  \n","34            14  \n","33            22  \n","32            14  \n","38            14  \n","7             11  \n","39            16  \n","6             14  "]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"3XJYKcTMyHB3"},"source":["Result:\n","- Logistic Regression with only structured features is shallow enough so that no overfitting happens\n","- thus, no regularization is needed\n","- L1 and L2 performed more or less the same (so doesn't matter which one is chosen)"]},{"cell_type":"markdown","metadata":{"id":"dFLoD2ymDL0v"},"source":["#### e) Best-Found LR Structured"]},{"cell_type":"markdown","metadata":{"id":"7tq3k25LFEKC"},"source":["- Preprocessing = Yeo-Johnson Transformation + Standardization for Numeric Features; Leave Binary Features as they are\n","- No regularization\n","- GlorotNormal initialization\n","- RMSProp (centered) optimizer\n","- Batch Size: 512"]}]}